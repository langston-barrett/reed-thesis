\documentclass[12pt,twoside,draft]{reedthesis}
\usepackage{graphicx} 
\usepackage{booktabs,setspace} 
% \usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new 
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the 
% bibliography is included. 
%\usepackage{biblatex-chicago}
%\bibliography{thesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% While drafting:
\usepackage{showlabels}

\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\definecolor{TODO}{HTML}{EE8800}
\newcommand{\TODO}[1]{\marginpar{\footnotesize\color{TODO}todo: #1}}

% \usepackage{showidx}
\let\oldindex\index
\definecolor{index}{HTML}{0088EE}
\renewcommand{\index}[1]{\oldindex{#1}\marginpar{\footnotesize\color{index}index: #1}}
\newcommand{\indeX}[1]{\oldindex{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% My stuff:
\input{../tex-preamble/math.tex}
\input{../tex-preamble/general.tex}
\input{../tex-preamble/amsthm.tex}
\input{../tex-preamble/problems.tex}
\input{../tex-preamble/logic.tex}
\input{../tex-preamble/hott.tex}
\usepackage{rotating}
\newcommand{\abbreviation}[1]{\textbf{#1}\indeX{#1@\textbf{#1}}} % abbreviations
\usepackage{makeidx}
\usepackage{prftree}\setlength{\prfinterspace}{1.2em}
\usepackage{tikz}
\usetikzlibrary{cd}
\tikzcdset{
  arrow style=tikz,
  arrows={line width=0.65pt},
  >={stealth}
}
\newcommand{\software}[1]{{\textsc{#1}}\indeX{#1}}
\newcommand{\Agda}{\software{Agda}}
\newcommand{\UniMath}{\software{UniMath}}
\newcommand{\Coq}{\software{Coq}}
\newcommand{\MTypes}{\software{HoTT/M-Types}}

% Gather environments with more interline spacing
\usepackage{environ}
\newlength{\oldjot}
\NewEnviron{gatherjot}{%
  \setlength{\oldjot}{\jot}\addtolength{\jot}{1em}
  \begin{gather*}
    \BODY
  \end{gather*}
  \setlength{\jot}{\oldjot}
}

\newcommand{\dual}[2]{
  \begin{itemize}\renewcommand{\labelitemi}{$∘$}
    \itemsep0em
    \item #1
    \item #2
   \end{itemize}
}

\newcommand{\define}[1]{\textbf{#1}} % term being defined
\newtheorem{notation}[theorem]{Notation}
\newtheorem{tt-rule}[theorem]{Rule}

\newcommand{\Coalgtype}{\ensuremath{\ttfun{Coalg}}}
\newcommand{\Final}{\ensuremath{\ttfun{Final}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% My stuff:

\title{Deriving Coinductive Types in Univalent Type Theory}
\author{Langston Barrett}
\date{May 2018}
\division{Mathematics and Natural Sciences}
\advisor{Safia Chettih}
\department{Mathematics}
\thedivisionof{The Established Interdisciplinary Committee for}

\setlength{\parskip}{0pt}
\begin{document}

\maketitle
\frontmatter % this stuff will be roman-numbered
\pagestyle{empty} % this removes page numbers from the frontmatter

% Acknowledgements (Acceptable American spelling) are optional
% So are Acknowledgments (proper English spelling)
\chapter*{Acknowledgments}

% The pcreface is optional
% To remove it, comment it out or delete it.
% \chapter*{Pcreface}
% This thesis delves into highly interdisciplinary territory. I'll

\chapter*{List of Abbreviations}

\begin{table}[h]
  \centering
  \begin{tabular}{ll}
    \textbf{HoTT}  	&  Homotopy type theory \\
    \textbf{UTT}  	&  Univalent type theory \\
    \textbf{ITT}  	&  Intensional type theory \\
    \textbf{FOL}  	&  Classical first-order logic \\
    \textbf{IPL}  	&  Intuitionistic propositional logic \\
    \textbf{LC}  	  &  Church's untyped λ-calculus
  \end{tabular}
\end{table}
	
% Depth to which to number and print sections in TOC
\setcounter{tocdepth}{4}
% \setcounter{secnumdepth}{2}
\tableofcontents
% if you want a list of tables, optional
% \listoftables
% if you want a list of figures, also optional
% \listoffigures

\chapter*{Abstract}

In this thesis, we explain Univalent type theory, a constructive and
computationally meaningful foundational system for mathematics inspired by
recent advances in the semantics of Per Martin-L\"of's intentional type theory.
We first develop the classical Curry-Howard correspondence between (the
constructive subset of) Gentzen's natural deducation and the λ-calculus.
We proceed to explicate Martin-L\"of's theory of dependent types and the central
modern development in type theory: Vladimir Voevodsky's Univalence principle. 
We go on to examine the nature of coinduction within this theory, presenting a
novel formalization of a recent result that M-types can be derived
\textit{internally} in Univalent type theory.

\chapter*{Dedication}

\mainmatter % here the regular arabic numbering starts
\pagestyle{fancyplain} % turns page numbering back on

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\chaptermark{Introduction}
\markboth{Introduction}{Introduction}

\section*{A short history of discomfort in mathematics}

% \subsection*{Trouble in Cantor's Paradise}

In the early 20\textsuperscript{th} century, mathematicians had a problem. Often
held up as the pinnacle of necessary, undoubtable, \textit{a priori} truth,
their discipline was suffering from an embarrassing lack of certainty. The
intricate arguments of advanced analysis left mathematicians unable to confirm
nor deny each other's proofs. While Cantor's na\"ive set theory had unleashed a
wave of paradoxes, the need for a rigorous logical foundation for higher
mathematics was so great that the enterprise of axiomatic set theory was pursued
headlong until some kind of consensus was reached. To make a long, rich story
brutally short, virtually every modern paper in mathematics and computer
science uses a combination of Gentzen's first-order logic (\abbreviation{FOL})
and an axiomatization of sets called
\abbreviation{ZFC}.\footnote{This ``consensus'' left out many
  prominent schools of thought, such as the Intuitionists and constructivists, a
  point we'll soon return to.} 

Despite the apparent rigor of \abbreviation{FOL}+\abbreviation{ZFC},
practitioners of the deductive sciences were still beset by issues of
verifiability. Complexity, specialization and sheer length made modern proofs
difficult to comprehend with the absolute certainty which is supposed to
characterize these disciplines. In the 1970s, two teams of topologists proved
contradictory results, and neither group could find the error in the other's
proof \cite{kolata}. Wiles's famous proof of Fermat's last theorem was utterly
unintelligible to the vast majority of mathematicians \cite{nyt}. The
classification of the simple finite groups, one of the crowning achievements of
modern mathematics, has a combined proof of over ten thousand pages. These
examples illustrate merely a few of the practical epistemological challenges
facing mathematicians, for a historical perspective see \cite{rigor-and-proof},
and for a general overview see \cite{fidelity}.

In 1990s, Fields medalist Vladimir Voevodsky grew concerned with the state
of mathematical knowledge. In 1998, Carlos Simpson released a pre-print arguing
that there was a major mistake in one of Voevodsky's papers. However, it was not
clear whether Voevodsky had errored, or whether there was a flaw in Simpson's
counterexample. In 1999, Pierre Deligne found a crucial mistake in Voevodsky's
``Cohomological Theory of Presheaves with Transfers'', upon which he had based
much of his work in the area of motivic cohomology. As he began to develop more
and more complex arguments, Voevodsky wondered: ``And who would ensure that I
did not forget something and did not make a mistake, if even the mistakes in
much more simple arguments take years to uncover?'' \cite{voevodsky-ias}. 

The problems facing Voevodsky and his peers seemed insurmountable.
As the requisite attention span, memory, and capacity for detail required to
understand new developments in higer mathematics reached inhuman proportions,
where was he to turn? He would not find a solution in the realm of pure
mathematics, but rather in one of the finest examples of collaboration between
mathematicians, computer scientists, and philosophers: the modern proof
assistant. \TODO{reword}

\section*{TODO}

While much of the mathematical discipline was simply relieved to have ``solved''
their foundational issues with axiomatic set theory, there remained a vocal
opposition to the newly-adopted methods. Most mathematicians are dimly aware
that there's some controversy about the Axiom of Choice (the ``C'' in
\abbreviation{ZFC}) \cite{martin-lof-100-years}, you don't have to look further
than \abbreviation{FOL} to find disagreements.

\section*{Our contribution}

(Co)inductive types and UniMath

\chapter{Propositions and Types}
\label[chapter]{chap:propositions-and-types}

In this chapter, we will present a logical and a computational framework.
We will begin with general remarks on discussing and defining formal systems in
\cref{sec:discussing-logic}. \Cref{sec:ipl} begins with an
intuitionistic, proof-relevant version of Gentzen's natural deduction
\cite{gentzen1935untersuchungen} called \abbreviation{IPL}. As most
mathematicians, computer scientists, and philosophers are familiar with
some version of first-order logic, this section will focus on the introduction
of notation, meta-logical concerns, and the key differences between
\abbreviation{IPL} and \abbreviation{FOL}. In \cref{sec:the-lambda-calculus}, we
will introduce Church's λ-calculus (typed and untyped). Finally, in
\cref{sec:the-curry-howard-correspondence}, we will discuss the fundamental and
harmonious relation between these systems known as the Curry-Howard
correspondence.

The vocabulary of \cref{sec:discussing-logic} is due to Martin-L\"of. He
emphasized the importance of the notion of judgment (\cref{def:judgment})
\cite{martin-lof-meanings} and justified the rules of his type theory
by meaning explanations \cite{martin-lof-constructive}, which we will imitate
in \cref{subsec:ipl-intro}. See \cite{modal-judgment} for a similar application
of Martin-L\"of vocabulary to predicate logic.

The presentation of \crefrange{sec:ipl}{sec:the-lambda-calculus} is influenced by
Philip Wadler \cite{wadler-propositions}, Frank Pfenning's lecture notes for
``Constructive Logic'', and Robert Harper's lectures on
\abbreviation{HoTT}.\TODO{citation}

Two recent Reed graduates wrote theses on the Curry-Howard correspondence, both
taking a different approach in presentation that may complement this work,
\cite{curry-howard-reed-thesis} and \cite{process-calculi-reed-thesis}.

\begin{notation}\label[notation]{notation:parens}
  Throughout this thesis, we will omit parentheses when applying function-like
  constructions wherever this results in no ambiguity. Function application is
  left-associative. For example, for function-like constructions $f$, $g$, $h$,
  and $i$ we have
  \begin{align*}
    \apply{f}{h}=f(h) &&
    f(\apply{g}{h})=f(g(h)) && \appply{f}{(\apply{g}{i})}{h}=f(g(i))(h)
  \end{align*}
  and so on.
\end{notation}

\section{Discussing logic}
\label{sec:discussing-logic}

The observant student of logic may notice a problem in the usual definition of
implication: one usually defines $P→ Q$ as ``if whenever $P$ is true,
$Q$ is true, then $P→ Q$ is true''. It seems that in order to understand
implication, one must first understand implication! More perniciously,
philosophers and mathematicians alike have argued that Skolem's
``paradox''\footnote{The L\"owenheim-Skolem theorem states that
  \abbreviation{ZFC} has a countable model. The paradox is that the statement
  ``there are uncountable sets'' is (famously) a theorem of \abbreviation{ZFC},
  Cantor's.} has deep and significant consequences for the philosophy of
mathematics \cite{skolem}. These misunderstandings both arise from a common
root: the failure to distinguish between object language and metalanguage. In
this section, we will attempt to mitigate some of the difficulties that arise
when discussing and defining formal systems for logic and computation.

\begin{definition}\label[definition]{def:object-meta-language}
  When discussing or defining a formal language (most commonly, a
  logico-deductive framework), we call that language the
  \define{object language}\index{object language}. Our discussion takes place in
  a \define{metalanguage}.
\end{definition}

\begin{example}
  \
  \begin{itemize}
    \itemsep0em
    \item In the statement ``\abbreviation{FOL} is complete'', \abbreviation{FOL}
      is the object language, and the metalanguage is English.
    \item G\"odel's second incompleteness theorem is a statement in the
      metalanguage of \abbreviation{FOL}+\abbreviation{ZFC}, about the object
      language of Peano arithmetic.
  \end{itemize}
\end{example}

\begin{definition}\label[definition]{def:metavariable}
	A \define{metavariable}\index{Metavariable} is a variable meant to stand for
  any well-formed expression of our object language.
\end{definition}

\begin{notation}
  In imitation of various proof assistants, we will prefix metavariables with
  question mark, e.g.\ $\mvar{a}$.
\end{notation}

See \cref{fig:structure}, \cref{ex:judgments}, and \cref{sec:ipl} for examples
of metavariables. 

\begin{definition}\label[definition]{def:judgment}
	A \define{judgment}\index{Judgment} is something that could be known. A
  judgment is \define{evident} if one does, in fact, know it.
  A \define{proof} evidence for a judgment.
  A \define{hypothetical judgment}\index{Judgment!Hypothetical} is one that
  holds under the assumption that some other judgments hold.
\end{definition}

\begin{example}\label[example]{ex:judgments}
  Here are some examples of common judgments in logic:
  \begin{itemize}
    \itemsep0em
    \item $\mvar{a}$ is a well-formed formula\footnote{The metavariables in
        these examples are to be understood as ranging over well-formed
        expressions of some unspecified object language.}
    \item $\mvar{a}$ is a proposition
    \item if $\mvar{a}$ and $\mvar{b}$ are propositions, then
      $\mvar{a}\land \mvar{b}$ is a proposition (this judgment is hypothetical)
    \item $\mvar{a}$ will happen in the future
    \item $\mvar{a}$ is a program with type $\mvar{t}$
    \item $\mvar{a}$ is true
    \item the variable $\mvar{v}$ is free (resp.\ bound) in $\mvar{a}$ 
  \end{itemize}
  If one is working in a formal metalanguage, judgments may be defined
  inductively in it.
\end{example}

\begin{figure}
  \centering
  \begin{equation*}
    \overbrace{\text{I know} \overbrace{\underbrace{\mvar{A}}_{\text{Expression}}\text{ is true}}^{\text{Judgment}}}^{\text{Evident judgment}}
  \end{equation*}
  \caption{\label{fig:structure}The structure of a judgment (transcribed
    from \cite{martin-lof-meanings})}
\end{figure}

\begin{figure}
  \centering
  % TODO
  % \begin{tikzpicture}
  % \end{tikzpicture}
  \caption{\label{fig:structure}Nested languages, formal and informal}
\end{figure}

\begin{notation}\label[notation]{notation:proof-tree}
  The premises and conclusions of proofs or rules of deduction are always
  judgments. If we can deduce a conclusion $B$ from premises $A_1,\ldots,A_n$
  via some rule R (again: $A_1,\ldots,A_n$ and $B$ are judgments, not terms of
  an object language), we will write
  \begin{align*}
    \prftree[r]{\footnotesize R}
      {A_1}{A_2}{\ldots}{A_n}
      {B}
    &&\text{or}&&
    \prftree[r]{\footnotesize R}
      {\prftree[r, noline]{}
        {A_1}
        {A_2}}
      {\prftree[r, noline]{}
        {A_3}
        {A_4}}
      {\ldots}{A_n}
      {B}.
  \end{align*}
  Iterating this notation, we can write long derivations as wellfounded trees
  with the conclusion as their root. 
\end{notation}

\begin{notation}\label[notation]{notation:sequent}
  If $B$ is a hypothetical judgment holding under assumptions $A_1,\ldots,A_n$,
  we denote this by $A_1,\ldots,A_n⊢ B$.\footnote{This is called Gentzen's
  \define{sequent notation}, $⊢$ can be read ``(syntactically) entails''.}
  A hypothetical judgment of this form is distinguished from the deduction of
  $B$ from premises $A_1,\ldots,A_n$ (shown in \cref{notation:proof-tree}).

  To denote some arbitrary sequence of hypotheses, we will use capital Greek
  letters $Γ$, $Δ$, and $Θ$. Our notions of entailment will
  always allow for \define{reflexivity}, \define{weakening}, \define{contraction},
  \define{substitution}, and \cref{sec:ipl} will allow for \define{exchange}, 
  which are the following meta-rules (where $J$, $K$, and $L$ stand for judgments):
  \begin{gatherjot}
    \prftree[r]{\footnotesize refl}{}
      {Γ,J,Δ ⊢ J}
    \qquad
    \prftree[r]{\footnotesize weak}
      {Γ ⊢ J}
      {Γ,K ⊢ J}
    \qquad
    \prftree[r]{\footnotesize contr}
      {Γ,K,Δ,K,Θ ⊢ J}
      {Γ,K,Δ,Θ ⊢ J} \\
    \prftree[r]{\footnotesize subst}
      {Γ,K,Δ ⊢ J}{Γ ⊢ K}
      {Γ,Δ ⊢ J}
    \qquad
    \prftree[r]{\footnotesize ex}
      {Γ,J,Δ,K,Θ ⊢ L}
      {Γ,K,Δ,J,Θ ⊢ L}
  \end{gatherjot}
\end{notation}

\begin{notation}\label[notation]{notation:substitution}
  We denote the substitution of a term $t$ for all free occurrences variable $v$
  in an expression $e$ by $e[v\coloneqq t]$. We will not deal directly with the
  issues of variable capture, scoping, and substitution here, for a rigorous
  treatment see any thorough textbook on logic.
\end{notation}

\section{Intuitionistic propositional logic}
\label{sec:ipl}

With these notations, we are prepared to define \abbreviation{IPL}. 

\subsection{Formation rules}
\label{subsec:ipl-form}

Since \abbreviation{IPL} is supposed to be a logic, we better have a notion of a
proposition.\footnote{Why do we judge that terms are propositions, instead of
  well-formed formulae? Propositions are strictly more general. With
  hypothetical judgments, we can have a judgment of the form
  $\true{\mvar{a}}⊢ \prop{\mvar{b}}$, expressing that $\mvar{b}$ is a
  proposition \textit{under the assumption that $\mvar{a}$ is true}. We will
  need such expressive power in \cref{chap:type-theory}. This is also why we
  refrain from specifying our syntax via formal BNF (or similar) grammars.}
Indeed, we denote the judgment that some term $\mvar{a}$ represents a
proposition by $\prop{\mvar{a}}$. The following are the \define{formation
rules}\index{Formation!In \abbreviation{IFOL}}:\footnote{For the less
  logically-versed reader: $⊤$ is read ``true'', $⊥$ is read ``false'',
  $\lnot$ is read ``not'', $\lor$ is read ``or'',
  $\land$ is read ``and'', $→$ is read ``implies'', $∀$ is read
  ``for all'', and $∃$ is read ``there exists''.
  The rules for deriving truth will justify these readings.}
\begin{gatherjot}
  \prftree[r]{}{}{Γ⊢\prop{⊤}}
  \qquad
  \prftree[r]{}{}{Γ⊢\prop{⊥}}
  \qquad
  \prftree[r]{}
    {\prop{Γ⊢\mvar{a}}}
    {\prop{Γ⊢\lnot \mvar{a}}} \\
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ⊢\prop{\mvar{a}→\mvar{b}}} 
  \qquad
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ⊢\prop{\mvar{a}\land \mvar{b}}} 
  \qquad
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ⊢\prop{\mvar{a}\lor\mvar{b}}} \\
  % \prftree[r]{}
  %    {\mvar{a}\prop}{v∉\FV(\mvar{a})}
  %    {∀ v.\mvar{a}}
  % \qquad
  % \prftree[r]{}
  %    {\mvar{a}\prop}{v∉\FV(\mvar{a})}
  %    {∃ v.\mvar{a}}
\end{gatherjot}
% The hypothesis $v∉\FV(\mvar{a})$ involves another kind of judgment: that $v$ is
% not a free variable of $\mvar{a}$. Intuitively, this just means that if $v$ appears at
% all in $\mvar{a}$, it is already ``bound'' by appearing just after a $∀$ or
% $∃$. Again, we won't deal with such issues with much care here.

\subsection{Introduction rules}
\label{subsec:ipl-intro}

Martin-L\"of said ``The meaning of a proposition is determined by [...] what
counts as a verification of it'' \cite{martin-lof-meanings}. Paraphrasing,
he follows the constructivist view and identifies (as we will identify) the
\textit{truth} of a proposition with having a \textit{proof} of it. Thus, to
understand the meanings of the formation rules, we must define the judgment
``$\mvar{p}:\mvar{a}$'', read ``$\mvar{p}$ is a proof of $\mvar{a}$''. The
ways of proving a proposition are called \define{introduction rules}.

The symbol $⊤$ represents the trivially true proposition. Accordingly, we
give it a trivial proof (with a weird name which will make sense later):
\begin{equation*}
  \prftree[r]{}{}{Γ ⊢ \unitelem:⊤}.
\end{equation*}

How do we know a conjunction? Intuitively, we know (have a proof of) $\mvar{a}$
and $\mvar{b}$ (written $\mvar{a}\land\mvar{b}$) just when we know (have a proof
of) both $\mvar{a}$ and $\mvar{b}$. In symbols,
\begin{equation*}
  \prftree[r]{}
    {Γ ⊢ \prop{\mvar{a}}}{Γ ⊢ \prop{\mvar{b}}}
    {Γ ⊢ \mvar{p}:\mvar{a}}{Γ ⊢ \mvar{q}:\mvar{b}}
    {Γ ⊢ (\mvar{p},\mvar{q}):\mvar{a}\land\mvar{b}}.
\end{equation*}

What about disjunction? There are two ways to know $\mvar{a}$ or
$\mvar{b}$ (written $\mvar{a}\lor\mvar{b}$). We can either know $\mvar{a}$ or
know $\mvar{b}$. Correspondingly, we have two introduction rules:
\begin{align*}
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{a}}
    {Γ ⊢ \apply{\inl}{\mvar{p}}:\mvar{a}\lor\mvar{b}}
  &&\text{and}&&
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{b}}
    {Γ ⊢ \apply{\inr}{\mvar{p}}:\mvar{a}\lor\mvar{b}}.
\end{align*}
The symbol $⊥$ represents falsehood. We say a logic is
\define{consistent}\index{Consistency} if it cannot prove falsehood (a highly
desirable property!). To this end, we have no introduction rule for $⊥$.

What does it mean to know a negation? We'll define
$\lnot\mvar{a}\defeq \mvar{a}→⊥$ and instead worry about when we
know an implication.\footnote{This definition suggests the following ``derived
  introduction rule'' for $⊥$:
  \begin{equation*}
    \prftree[r]{}
      {Γ ⊢ \prop{\mvar{a}}}{Γ ⊢ \mvar{p}:\mvar{a}}{Γ ⊢ \mvar{q}:\lnot\mvar{a}}
      {Γ ⊢ \ttfun{whoops!}(\mvar{p},\mvar{q}):⊥}.
  \end{equation*}
  The derivation of this rule from the elimination rule for $→$
  (\cref{subsec:ipl-elim}) is immediate.}
An implication is true just when we have a hypothetical judgment where the
hypothesis is the antecedent and the conclusion is the consequent. Implication
allows us to \textit{internalize}\index{Internalizing!Hypothetical judgments}
the meta-theoretical notion of hypothetical judgments by turning them into
non-hypothetical ones. To define the introduction rule for $→$, we
assume the presence of a countably-infinite set of \define{variables}
$v,w,\ldots$
\begin{equation*}
  \prftree[r]{}
    {Γ ⊢ \prop{\mvar{a}}}{Γ ⊢ \prop{\mvar{b}}}
    {Γ,\mvar{p}:\mvar{a},Δ⊢ \mvar{q}:\mvar{b}}
    {Γ,Δ⊢\λ{v}{\mvar{q}[\mvar{p}\coloneqq v]}:\mvar{a}→\mvar{b}}.
\end{equation*}

\begin{definition}\label[definition]{def:valid-theorem}
  If know that under no hypotheses $\mvar{a}$ is a proposition and has a proof,
  then we may judge that $\mvar{a}$ is \define{valid}\index{Valid!In
  \abbreviation{IPL}}:
  \begin{equation*}
    \prftree[r]{}
      {⊢\prop{\mvar{a}}}{⊢\mvar{p}:\mvar{a}}
      {⊢\valid{\mvar{a}}}.
  \end{equation*}
  In this case, we say that $\mvar{a}$ is a \define{theorem}\index{Theorem!Of
  \abbreviation{IPL}} of \abbreviation{IPL}.
\end{definition}

\subsection{Elimination rules}
\label{subsec:ipl-elim}

Once we know a judgment, how do we use it in a derivation?
We need \define{elimination rules}. These should be dual to our
introduction rules: we should be able to extract the same amount of information
from a proposition that went into its proof.\footnote{The feature that our
  elimination rules extract enough (resp.\ not too much) information is called
  \define{local completeness}\indeX{Completeness!Local} (resp \define{local
  soundness}\indeX{Soundness!Local}).}

For conjunction, we should be able to recover proofs of both conjuncts:
\begin{align*}
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{a}\land\mvar{b}}
    {Γ ⊢ \appr{1}\mvar{p}:\mvar{a}} 
  &&\text{and}&&
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{a}\land\mvar{b}}
    {Γ ⊢ \appr{2}\mvar{p}:\mvar{b}}.
\end{align*}
If both disjuncts imply a hypothesis, so does their disjunction:
\begin{align*}
  \prftree[r]{}
    {\prftree[r, noline]{}
      {\prftree[r, noline]{}
        {Γ ⊢ \prop{\mvar{a}}}
        {Γ ⊢ \prop{\mvar{b}}}}
      {Γ ⊢ \prop{\mvar{c}}}}
    {\prftree[r, noline]{}
      {Γ ⊢ \mvar{p}:\mvar{a}→\mvar{c}}
      {Γ ⊢ \mvar{q}:\mvar{b}→\mvar{c}}}
    {Γ ⊢ \mvar{r}:\mvar{a}\lor\mvar{b}}
    {Γ ⊢ \apppply{\case}{\mvar{p}}{\mvar{q}}{\mvar{r}}:\mvar{c}}.
\end{align*}
The elimination rule for implication has the common name
\define{modus ponens}\index{Modus ponens}, and expresses the idea that, if we
had a hypothetical judgment and then come to know all of the hypotheses, we can
deduce the consequence:
\begin{align*}
  \prftree[r]{}
    {Γ ⊢ \prop{\mvar{a}}}{Γ ⊢ \prop{\mvar{b}}}
    {Γ ⊢ \mvar{p}:\mvar{a}→ \mvar{b}}
    {Γ ⊢ \mvar{q}:\mvar{a}}
    {Γ ⊢ \apply{\mvar{p}}{\mvar{q}}:\mvar{b}}.
\end{align*}
The elimination rule for falsehood comes from the principle
\textit{ex falso quod libet}, ``from falsehood anything follows''. If we've been
able to prove $⊥$, our whole logic is bankrupt and we can derive anything we
please: 
\begin{align*}
  \prftree[r]{}
    {Γ ⊢ \prop{\mvar{a}}}
    {Γ ⊢ \mvar{p}:⊥}
    {Γ ⊢ \apply{\rec_{⊥}}{\mvar{p}}:\mvar{a}}.
\end{align*}

At this point, we'll begin to see some full proofs. Since these quickly become
unmanageably large, we'll implicitly hypothesize that all metavariables involved
represent propositions.

\begin{example}\label[example]{ex:ipl-and-comm}
  The following proves the classical tautology that $\land$ is commutative:
  $\mvar{a}\land\mvar{b}⊢ \mvar{b}\land\mvar{a}$.
  \begin{equation*}
    \prftree[r]{}
      {\prftree[r]{}
        {Γ ⊢ \mvar{p}:\mvar{a}\land\mvar{b}}
        {Γ ⊢ \appr{2}{\mvar{p}}:\mvar{b}}}
      {\prftree[r]{}
        {Γ ⊢ \mvar{p}:\mvar{a}\land\mvar{b}}
        {Γ ⊢ \appr{1}{\mvar{p}}:\mvar{a}}}
      {Γ ⊢ (\appr{2}{\mvar{p}},\appr{1}{\mvar{p}}):\mvar{b}\land\mvar{a}}
  \end{equation*}
  This derivation serves as a verification that the introduction and
  elimination rules for conjunction complement one another well.
\end{example}

\begin{definition}\label[definition]{def:lem-dne}
  The \define{law of the excluded middle}\index{Law of excluded middle} (from
  the syllogistic principle \textit{tertium non datur}, ``no third is given'')
  is the following rule of deduction:
  \begin{equation*}
    \prftree[r]{\footnotesize LEM}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \apply{\ttfun{lem}}{\mvar{a}}:\true{\mvar{a}\lor\lnot\mvar{a}}}.
  \end{equation*}
  It is inter-derivable with the rule of \define{double negation elimination}:
  \index{Double negation elimination}
  \begin{equation*}
    \prftree[r]{\footnotesize DNE}
      {Γ ⊢ \prop{\mvar{a}}}{\mvar{p}:\lnot\lnot\mvar{a}}
      {Γ ⊢ \apply{\ttfun{dne}}{\mvar{p}}:\mvar{a}}.
  \end{equation*}
\end{definition}

\begin{example}\label[example]{ex:ipl-not-not-lem}
  A crucial consequence of the definition of \abbreviation{IPL} is that the law
  of excluded middle isn't a theorem. However, we can demonstrate that
  its negation isn't either. While reading this proof, keep the following in
  mind:
  \begin{itemize}
    \itemsep0em
    \item for brevity, we abbreviate our hypothesis
      $\mvar{h}:\lnot(\mvar{a}\lor\lnot \mvar{a})$ as $\mvar{h}:H$
    \item $\lnot\mvar{a}\equiv \mvar{a}→⊥$
  \end{itemize}
  \begin{center}
  \noindent\makebox[\textwidth]{%
    \prftree[r]{}
      {\prftree[r]{}
        {\prftree[r]{}
          {\prftree[r]{}
            {\prftree[r]{}
              {\prftree[r]{\footnotesize weak}
                {\prftree[r]{}
                  {\prftree[r]{\footnotesize refl}
                    {\mvar{p}:\mvar{a}⊢\mvar{p}:\mvar{a}}}
                  {\mvar{p}:\mvar{a}⊢\apply{\inl}{\mvar{p}}:\mvar{a}\lor\lnot\mvar{a}}}
                {\mvar{p}:\mvar{a},\mvar{h}:H⊢\apply{\inl}{\mvar{p}}:\mvar{a}\lor\lnot\mvar{a}}}
              {\prftree[r]{\footnotesize ex}
                {\prftree[r]{\footnotesize weak}
                  {\prftree[r]{\footnotesize refl}
                    {\mvar{h}:H⊢\mvar{h}:H}}
                  {\mvar{h}:H,\mvar{p}:\mvar{a}⊢\mvar{h}:H}}
                {\mvar{p}:\mvar{a},\mvar{h}:H⊢\mvar{h}:H}}
              {\mvar{p}:\mvar{a},\mvar{h}:H⊢\apply{\mvar{h}}{(\apply{\inl}{\mvar{p}})}:⊥}}
            {\mvar{h}:H⊢\λ{v}\apply{\mvar{h}}{(\apply{\inl}{v})}:\lnot\mvar{a}}}
          {\mvar{h}:H⊢\apply{\inr}{(\λ{v}\apply{\mvar{h}}{(\apply{\inl}{v})})}:\mvar{a}\lor\lnot \mvar{a}}}
        {\prftree[r]{\footnotesize refl}
          {\mvar{h}:H⊢\mvar{h}:H}}
        {\mvar{h}:H⊢ \apply{\mvar{h}}{(\inr(\λ{v}\apply{\mvar{h}}{(\apply{\inl}{v})}))}:⊥}}
      {⊢\λ{w}\apply{w}{(\inr(\λ{v}\apply{w}{(\apply{\inl}{v})}))}:\lnot (\lnot(\mvar{a}\lor\lnot \mvar{a}))}
  }
  \end{center}
  The law of the excluded middle will be discussed further in \cref{subsec:on-lem}.
  \TODO{cite bob harper}
\end{example}

\subsection{Computation rules and judgmental equality}
\label{subsec:ipl-compute}

\begin{remark}\label[remark]{rmk:reverse-proof}
  As we can see in \cref{ex:ipl-and-comm}, and \cref{fig:lem-refute},
  derivations in \abbreviation{IPL} end with a \define{proof term} which
  summarizes the whole proof tree (modulo some actions on hypotheses, which we've
  excluded for brevity). To demonstrate this, let's attempt to reconstruct a
  proof tree based on a judgment including such a term. Suppose your friend
  tells you they apprehended the following hypothetical judgment
  in a dream, but upon waking, couldn't recall the derivation:
  \begin{equation*}
      {Γ ⊢ \apply{\mvar{r}}{(\appr{1}{(\apply{\mvar{p}}{\mvar{q}})})} : \mvar{d}}.
  \end{equation*}
  Well, it certainly had to end with an application of implication eliimination
  to hypotheses of the form $\mvar{b}$ and $\mvar{b}→ \mvar{d}$:
  \begin{equation*}
    \prftree[r]{}
      {Γ ⊢ \mvar{r}:\mvar{b}→\mvar{d}}
      {Γ ⊢ \prftree[r]{}
        {?}
        {Γ ⊢ \appr{1}{(\apply{\mvar{p}}{\mvar{q}})}:\mvar{b}}}
      {Γ ⊢ \apply{\mvar{r}}{(\appr{1}{(\apply{\mvar{p}}{\mvar{q}})})} : \mvar{d}}.
  \end{equation*}
  and if the term of type $\mvar{b}$ was produced from the application of
  $\pr{1}$, the term it was applied to had to be a proof of
  $\mvar{b}\land \mvar{c}$ for some $\mvar{c}$:
  \begin{equation*}
    \prftree[r]{}
      {Γ ⊢ \mvar{r}:\mvar{b}→\mvar{d}}
      {\prftree[r]{}
        {\prftree[r]{}
          {?}
          {Γ ⊢ \apply{\mvar{p}}{\mvar{q}}:\mvar{b}\land\mvar{c}}}
        {Γ ⊢ \appr{1}{(\apply{\mvar{p}}{\mvar{q}})}:\mvar{b}}}
      {Γ ⊢ \apply{\mvar{r}}{(\appr{1}{(\apply{\mvar{p}}{\mvar{q}})})} : \mvar{d}}.
  \end{equation*}
  which itself had to be applied to proofs of $\mvar{a}$ and
  $\mvar{a}→\mvar{b}\land\mvar{c}$ for some $\mvar{a}$:
  \begin{equation*}
    \prftree[r]{}
      {Γ ⊢ \mvar{r}:\mvar{b}→\mvar{d}}
      {\prftree[r]{}
        {\prftree[r]{}
          {Γ ⊢ \mvar{p}:\mvar{a}→(\mvar{b}\land\mvar{c})}
          {Γ ⊢ \mvar{q}:\mvar{a}}
          {Γ ⊢ \apply{\mvar{p}}{\mvar{q}}:\mvar{b}\land\mvar{c}}}
        {Γ ⊢ \appr{1}{(\apply{\mvar{p}}{\mvar{q}})}:\mvar{b}}}
      {Γ ⊢ \apply{\mvar{r}}{(\appr{1}{(\apply{\mvar{p}}{\mvar{q}})})} : \mvar{d}}.
  \end{equation*}
  At this point, we can (un)deduce no further. The form of the judgment allowed
  us no discretion; this is the only derivation (up to\footnote{For the
    non-mathematical reader: When a mathematician says ``X is true up to $R$''
    for some (equivalence) relation $R$, it means that all objects related by $R$
    are considered equivalent. In this case, ``unique up to isomorphism'' means that
    there may be other terminal objects, but all of them are mutually isomorphic.}
  renaming of metavariables) that could have produced that conclusion.
\end{remark}

The proof term in longer arguments is often cumbersome, and we might wonder if
there are any benefits to internalizing\index{Internalizing!Proofs} the notion
of proof. The answer to this question will play a critical role in
\cref{sec:curry-howard}, and it is essentially the following: when we
internalize proof, we can give formal rules for reducing our
proof terms.\footnote{On the philosophical side, Martin-L\"of made a fascinating
  and convincing argument that judgments involving proof terms are analytic,
  whereas proof-irrelevant mathematical arguments are synthetic: one must go
  beyond conceptual analysis and view the proof to be convinced of them
  \cite{martin-lof-analytic}.} By \cref{rmk:reverse-proof}, this is as good as
giving an algorithm that converts unnecessarily complex to some normal form.

What kind of proofs can be reduced? Consider the following example:
\begin{equation*}
  {\prftree[r]{}
    {\prftree[r]{}
      {Γ ⊢ \mvar{p}:\mvar{a}}
      {Γ ⊢ \mvar{q}:\mvar{b}}
      {Γ ⊢ (\mvar{p},\mvar{q}):\mvar{a}\land\mvar{b}}}
    {Γ ⊢ \appr{1}{(\mvar{p},\mvar{q})}:\mvar{a}}}.
\end{equation*}
This derivation could be eliminated entirely: we had a proof of $\mvar{a}$ to
begin with! This is an instance of a general pattern: whenever we have an
elimination rule for a connective just after its introduction, by the symmetry
between such rules, we can avoid the circumlocution and cut straight to the
conclusion.

Do such shortcuts result in \textit{the same} proof? To phrase this
question precisely, one needs a notion of equality for proof terms.
And Gentzen's notion was define it just so he could answer this question in the
affirmative.

\begin{definition}\label[definition]{def:jdeq-ipl}
	\define{Judgmental equality} of proof terms, denoted
  $\mvar{p}\jdeq\mvar{q}:mvar{a}$, is the least congruence closed under the
  following
  \define{computation rules}\index{Computation rules!In \abbreviation{IPL}}
  (also called \define{$β$-rules}):
  \begin{gatherjot}
    {\prftree[r]{\footnotesize $\landβ_{\text{l}}$}
      {Γ ⊢ \mvar{p}:\mvar{a}}{\mvar{q}:\mvar{b}}
      {Γ ⊢ \appr{1}{(\mvar{p},\mvar{q})}\jdeq \mvar{p}:\mvar{a}}}
    \qquad
    {\prftree[r]{\footnotesize $\landβ_{\text{r}}$}
      {Γ ⊢ \mvar{p}:\mvar{a}}{\mvar{q}:\mvar{b}}
      {Γ ⊢ \appr{2}{(\mvar{p},\mvar{q})}:\mvar{b}}} \\
    {\prftree[r]{\footnotesize $(→)β$}
      {Γ ⊢ \mvar{p}:\mvar{a}⊢\mvar{q}:\mvar{b}}
      {Γ ⊢ \mvar{r}:\mvar{a}}
      {Γ ⊢ \apply{(\λ{v}\mvar{q})}{\mvar{r}}\jdeq
        \mvar{q}[\mvar{p}\coloneqq\mvar{r}]:\mvar{b}}} \\
    {\prftree[r]{\footnotesize $\lorβ_{\text{l}}$}
      {\prftree[r, noline]{}
        {Γ ⊢ \mvar{p}:\mvar{a}→\mvar{c}}
        {Γ ⊢ \mvar{q}:\mvar{b}→\mvar{c}}}
      {Γ ⊢ \mvar{r}:\mvar{a}}
      {Γ ⊢ \apppply{\case}{\mvar{p}}{\mvar{q}}{(\apply{\inl}{\mvar{r}})}\jdeq
        \apply{\mvar{p}}{\mvar{r}}:\mvar{c}}} 
    \qquad
    {\prftree[r]{\footnotesize $\lorβ_{\text{r}}$}
      {\prftree[r, noline]{}
        {Γ ⊢ \mvar{p}:\mvar{a}→\mvar{c}}
        {Γ ⊢ \mvar{q}:\mvar{b}→\mvar{c}}}
      {Γ ⊢ \mvar{r}:\mvar{b}}
      {Γ ⊢ \apppply{\case}{\mvar{p}}{\mvar{q}}{(\apply{\inr}{\mvar{r}})}\jdeq
        \apply{\mvar{q}}{\mvar{r}}:\mvar{c}}}.
  \end{gatherjot}
  (Recall \cref{notation:substitution} for the meaning of
  $\mvar{q}[\mvar{p}\coloneqq\mvar{r}]$). Since the elimination rule for
  falsehood (\cref{subsec:ipl-elim}) gives back a proof of an arbitrary
  proposition (which is not in general one of the ``inputs'' to $\intro{⊥}$),
  there's no corresponding $β$-rule.
\end{definition}

% To define the rules for introducing quantifiers, we must deal with the issue of
% free and bound variables. Our second judgment expresses that $v$ is a free variable
% of $\mvar{a}$, denoted $v\in\FV(\mvar{a})$. For the sake of brevity, metavariables in the
% following are implicitly assumed to be propositions:
% \begin{gather*}
%   \prftree[r]{}
%      {v\in \FV(\mvar{a})}
%      {v\in \FV(\mvar{a}\land \mvar{b}\prop)}
%   \qquad
%   \prftree[r]{}
%      {v\in \FV(\mvar{b})}
%      {v\in \FV(\mvar{a}\land \mvar{b}\prop)} \\
%   \prftree[r]{}
%      {v\in \FV(\mvar{a})}
%      {v\in \FV(\mvar{a}\lor \mvar{b}\prop)}
%   \qquad
%   \prftree[r]{}
%      {v\in \FV(\mvar{b})}
%      {v\in \FV(\mvar{a}\lor \mvar{b}\prop)}
% \end{gather*}

\subsection{Unicity rules}
\label{subsec:ipl-uni}

Just because a logic doesn't give us an easy way to do something unexpected
doesn't mean it can't be done. The prime example is proving a contradiction.
History is filled with deductive systems that were thought to be sound, but
were shown to be useless by some clever argument. Russell's paradox dispensed
with Frege's \textit{Begriffsschrift}, the Burali-Forti antimony unraveled
na\"ive set theories, and Girard's paradox revealed a fatal flaw in
Martin-L\"of's early attempts at dependent type theory.

\section{The λ-calculus}
\label{sec:the-lambda-calculus}

\subsection{The untyped λ-calculus and computation}
\label{subsec:the-untyped-lambda-calculus}

The Church-Turing thesis

$β$-reduction

\subsection{The simply-typed λ-calculus}
\label{subsec:the-simply-typed-lambda-calculus}

Two judgments

The strong normalization property

\subsection{More complex types}
\label{subsec:more-complex-types}

\subsubsection{Product types}
\label{subsubsec:product-types}

\subsubsection{Coproduct types}
\label{subsubsec:coproduct-types}

\subsubsection{Natural numbers}
\label{subsubsec:natural-numbers}

\subsubsection{Lists}
\label{subsubsec:lists}

\section{The Curry-Howard correspondence}
\label{sec:curry-howard}

\cite{curry-howard}

\subsection{The BHK interpretation}
\label{subsec:the-bhk-interpretation}

\chapter{Type theory}
\label{chap:type-theory}

\section{The identity type}
\label{sec:the-identity-type}

\begin{tt-rule}\label[rule]{rule:id-elim}
  The rule of \define{identity elimination} (in the \abbreviation{HoTT}
  community, known as \define{path induction}) is as follows:\TODO{define id-elim}
\end{tt-rule}

\begin{lemma}[$\ttfun{ap}$]\label[lemma]{lemma:ap}
	\TODO{define ap}
\end{lemma}

\begin{lemma}[Transport]\label[lemma]{lemma:transport}
	\TODO{define transport}
\end{lemma}

\begin{notation}\label[notation]{notation:transport}
  We will often curry\index{Curry} \transportname. If we have a family
  $P:A\to\universe$ and a path $p:\propeq{A}{x}{y}$, we write $\transpor{P}{p}$
  for $\λ{x}\transport{P}{p}{x}$.
\end{notation}

\begin{lemma}\label[lemma]{lemma:transport-compose}
	For $f:A\to B$, $P:B\to\universe$, and $p:\propeq{A}{x}{y}$, 
  \begin{equation*}
    \propeq{}{
      \transpor{P∘ f}{p}
    }{
      \transpor{P}{\ap{f}{p}}
    }
  \end{equation*}
\end{lemma}

\section{Dependent types}
\label{sec:dependent-types}

Up to now, we have not considered the type of types. However, types are terms
like any other, and every term has a type.

\begin{definition}\label[definition]{def:universes}
  All the types we have considered up to this point are members of a universe
  \universei{0}.
\end{definition}

We now have the power to write an \textit{single} identity function that works
for \textit{all} types.

\begin{definition}\label[definition]{def:id-polymorphic}
  The identity function is
	\begin{align*}
    \id :\∏{A:\universe}{A\to A} &&
    \id \defeq \λ{A}{\λ{a}{a}}
  \end{align*}
  We will write its first argument as a subscript, as in $\id_{A}$.
\end{definition}

\begin{lemma}[Paths in $Σ$-types]\label[lemma]{lemma:path-sigma}
	If $x,y:\∑{a:A}{B(a)}$, then
  \begin{equation*}
    \weq{
      (\propeq{}{x}{y})
    }{
      \∑{
        p:\propeq{}{\appr{1}{x}}{\appr{1}{y}}
      }{
        \propeq{}{
          \transport{}{p}{\appr{2}{x}}
        }{
          \appr{2}{y}
        }
      }
    }.
  \end{equation*}
  \TODO{cite with page number}
\end{lemma}

\subsection{$Π$-types}
\label{subsec:pi-types}

\subsubsection{On the law of the excluded middle}
\label{subsec:on-lem}

\TODO{introduce the topic based on previous sections}

Consider functions with the following types:
\begin{itemize}
  \itemsep0em
  \item $\ttfun{dne}:\∏{A:\universe}{(A\to⊥\to⊥)\to A}$
  \item $\ttfun{lem}:\∏{A:\universe}{A+(A\to⊥)}$
\end{itemize}
What can we tell about these functions from their type signatures?
The term $\ttfun{dne}$ takes as argument a term $x:A\to⊥\to⊥$,
that is, a term showing that $A$ is not the empty type\footnote{More precisely,
  $x$ demonstrates that $A$ is not \textit{isomorphic} or \textit{equivalent} to
  $⊥$, in the sense of \cref{subsec:weak-equivalences}. If $A$ were isomorphic
  to $⊥$, then that isomorphism would be an inhabitant of $A\to⊥$.}, and
produces some element of $A$. This seems like a very tricky function to write:
how can we give a term of type $A$ just by knowing $A$ has terms? We don't
know what form data of type $A$ have! The function $\ttfun{lem}$ seems
similarly quagmired. Given a type $A$, this function either has to produce an
element of it, or demonstrate that it is uninhabited. 

As you may have already guessed, under the Curry-Howard correspondence, these
functions correspond to the rule of double negation elimination
($\ttfun{dne}$)\index{Double negation elimination}
and the law of excluded middle ($\ttfun{lem}$)\index{Law of excluded middle}
\cref{def:lem-dne}.

To demonstrate with finality that these are not definable terms in any
consistent type theory, consider the application of $\ttfun{lem}$ to a type
$\ttfun{pnp}$ that corresponds to the $\textsc{P}\neq\textsc{NP}$ conjecture
under the types-as-propositions interpretation. If $\ttfun{lem}$ existed, we
could trivially solve this problem and make a million dollars! We need no
recourse to philosophy to justify the constructive/intuitionistic logic of proof
assistants: nothing else computes.

\section{$h$-levels and truncation}
\label{sec:$h$-levels and truncation}

\section{Univalence}
\label{sec:univalence}

\subsection{Weak equivalences}
\label{subsec:weak-equivalences}

\begin{notation}\label[notation]{notation:weq-coerce}
  We may treat a weak equivalence as if it were a function and apply it to an
  argument. This amounts to implicitly appling $\pr{1}$.
\end{notation}

\subsection{The univalence axiom}
\label{subsec:the-univalence-axiom}

\begin{definition}\label[definition]{def:ua}
  \TODO{definintion}
  It has the following computation rule:

  Note that this rule holds up to \textit{propositional equality}. This is
  because in \abbreviation{UTT}, univalence is indeed an \textit{axiom}. It has
  no computational meaning. \TODO{mention CTT}
\end{definition}

\begin{theorem}[Function extensionality]\label[theorem]{thm:funext}
	Under the hypothesis of the univalence axiom, function extensionality holds,
  i.e.\ there is a term
  \begin{equation*}\label{eq:funext}
    \funext:\∏{A,B\:\universe}{(\homot{f}{g})\to \propeq{}{f}{g}}
  \end{equation*}
\end{theorem}

\chapter{Category theory}
\label{chap:category-theory}

Category theory presents a challange to set-theoretic mathematicians: the
canonical example of a category is $\Set$, the collection of all sets. In
\abbreviation{ZFC}+\abbreviation{FOL}, this category is undefinable\footnote{In
  particular, its formation is prevented by the axiom of regularity
  \cite{vonneumann}, which was included in \abbreviation{ZFC} to avoid the
  paradoxes of Burali-Forti and Russell. The discovery of said paradoxes
  motivated Bertrand Russell to invent something he called the ``theory of
  types'' \cite{russell}.}.
\Crefrange{sec:basics}{sec:functors-and-their-algebras}, 
are willfully imprecise, we work with an abstract and undefined notion of
``collection'', and adopt set-theoretic notation. See any textbook on category
theory for information on how problems of size are dealt with set-theoretically.
We will examine category theory within \abbreviation{UTT} in
\cref{sec:type-theoretic-category-theory}.

\section{Basics}
\label{sec:basics}


\TODO{Type theory and category theory are intimately connected via the
      discipline of \textit{topos theory}}%\index{Topos theory}.}

\begin{definition}
	A \define{category}\index{Category} $\bfC$ consists of the following data:
  \begin{itemize}
    \itemsep-0.2em
    \item a collection of \define{objects}, denoted $\Obj \bfC$,
    \item for each pair of objects $A,B\in\Obj \bfC$, a collection of
      \define{arrows} (or \define{morphisms}) between them, denoted
        $\Hom_{\bfC}(A,B)$,
    \item for each object $A\in\Obj \bfC$, a distinguished arrow
      $\id_A\in\Hom_{\bfC}(A,A)$ called the
      \define{identity}\index{Identity!Morphism}, and
    \item for each triple of objects $A,B,C\in\Obj\bfC$, an operation \\
      $∘:\Hom_{\bfC}(B,C)\times\Hom_{\bfC}(A,B)\to\Hom_{\bfC}(A,C)$ called
      \define{composition}\index{Composition!In a category}.
  \end{itemize}
  These data are subject to the following axioms:
  \begin{enumerate}%[label=\Alph*.]
    \itemsep-0.2em
    \item composition is associative, and
    \item the identity acts as a unit for composition.
  \end{enumerate}
  When the category in question is clear from context, one writes $f:A\to B$ for
  $f\in\Hom_{\bfC}(A,B)$. 
\end{definition}

\begin{definition}\label[definition]{def:domain-and-codomain}
  If $f\in\Hom_{\bfC}(A,B)$, then $A$ is the \define{domain}\index{Domain} or
  \define{source}\index{Source} of $f$ and $B$ is the
  \define{codomain}\index{Codomain} or \define{target}\index{Target} of $f$.
\end{definition}

\begin{example}
  A student of mathematics will be familiar with the following categories:
  \begin{itemize}
    \itemsep-0.2em
    \item $\Set$: The category with sets as objects, functions as morphisms, 
      the usual composition of functions, and identity functions.
    \item $\Grp$: The category of groups\index{Group} with group homomorphisms
      as morphisms. Note that the identity function of sets is the required
      identity morphism and that for any homomorphsisms $ϕ:G\to H$ and
      $ψ:H\to I$, the usual composition of functions defines a homomorphism
      $ψ∘ ϕ:G\to I$.
    \item $\AbGrp$: The category of abelian groups (this can be considered a
      \define{subcategory} of $\Grp$).
    \item $F\dVect$: The category of vector spaces\index{Vector space} over a
      field $F$ with linear transformations as morphisms.
    \item For any group $(G,∘,e)$, there is a corresponding category
      $\underline{G}$ with a single object (denoted $∗$) and
      $\Hom_{\underline{G}}(∗,∗)\coloneqq G$. Composition is given by the
      group operation.
  \end{itemize}
\end{example}

\begin{definition}\label[definition]{def:isomorphism}
	An \define{isomorphism}\index{Isomorphism!In a category} in $\bfC$ is an arrow
  $f\in\Hom_{\bfC}(A,B)$ such that there exists an arrow $g\in\Hom_{\bfC}(B,A)$
  such that $g∘ f=\id_A$ and $f∘ g=\id_B$. We call such a $g$ the
  \define{inverse}\index{Inverse!In a category} of $f$.
\end{definition}

Just as with functions, inverses (should they exist) are unique.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[scale=2]
    \node (A) [] {$∗$};

    \foreach \x in {1,2,3} 
      \draw (A) edge [in={50*\x},out={40+50*\x},loop] node[above] {$g_\x$} (A);

    \draw (A) edge [in=200,out=240,loop] node[below] {$\cdots$} (A);
    \draw (A) edge [in=-60,out=-20,loop] node[below] {$e$} (A);
  \end{tikzpicture}
  \caption{
    \label{fig:grp} A schematic of the category $\underline{G}$ for a
    group $G$ with identity $e$ and elements $g_1,g_2,\ldots$. Compositions and
    inverses not shown.
  }
\end{figure}

\begin{definition}\label[definition]{def:commutative-diagram}
  A commutative diagram is a way to visualize equations between arrows
  involving composition. Technically a diagram in $\bfC$ is a directed graph
  with vertices labeled by $\Obj\bfC$. If $e$ is an edge from $A$ to $B$, then it
  is labeled by an arrow in $\Hom_\bfC(A,B)$. A diagram \define{commutes}, or is
  commutative, if the composition of the arrows labeling the edges of any two
  directed paths with the same endpoints are equal.
  \TODO{better wording}
\end{definition}

\begin{example}\label[example]{ex:commutative-diagram}
	If $f\in\Hom_{\bfC}(A,B)$, $g\in\Hom_{\bfC}(B,C)$, $h\in\Hom_{\bfC}(A,C)$,
  and $g∘ f=h$, then we may draw the following
  \define{commutative triangle}:
  \begin{center}
    \begin{tikzcd}[column sep=large]
      A \arrow[r, "f"] \arrow[dr, "h"] & B \arrow[d, "g"] \\
      {}                               & C
    \end{tikzcd}
  \end{center}
	If $f\in\Hom_{\bfC}(A,B)$, $g\in\Hom_{\bfC}(B,D)$, $h\in\Hom_{\bfC}(A,C)$,
  $i\in\Hom_{\bfC}(C,D)$, and $g∘ f=i∘ h$, then we may draw the
  following \define{commutative square}:
  \begin{center}
    \begin{tikzcd}[column sep=large]
      A \arrow[r, "f"]  \arrow[d, "h"] & B \arrow[d, "g"] \\
      C \arrow[r, "i"]                 & D
    \end{tikzcd}
  \end{center}
\end{example}

\section{Duality}
\label{sec:duality}

From now on, dual concepts and statements will be introduced in pairs, and
typeset like so:

\dual{
  Concept
}{
  Co-concept
}

\section{Limits and colimits}
\label{sec:limits-and-colimits}

For this section, fix an arbitrary category $\bfC$.\TODO{elsewhere?}

\begin{definition}\label[definition]{def:terminal-and-initial}
  \
  \vspace{-0.3em}\dual{
    A \define{terminal object}\index{Terminal object} is an
    object $⊤\in\Obj\bfC$ such that for all $A\in\Obj\bfC$,
    there is exactly one arrow $f:A\to⊤$.
  }{
    An \define{initial object}\index{Initial object} is an
    object $⊥\in\Obj\bfC$ such that for all $A\in\Obj\bfC$,
    there is exactly one arrow $f:⊥\to A$.
  }
\end{definition}

\begin{remark}\label[remark]{remark:terminal-object-id-unique}
	In particular, for a terminal object $⊤$, $\id_⊤$ is the only arrow
  $⊤\to⊤$.\footnote{As noted in \cref{sec:duality}, this statement
    holds for initial objects as well. From this point on, we will leave it
    to the reader to construct the dual of a statement and infer its truth.}
\end{remark}

\begin{lemma}\label[lemma]{lemma:terminal-unique}
  Terminal objects are unique up to a specified isomorphism.
\end{lemma}
\begin{proof}
	Suppose $A$ and $B$ are terminal objects.
  There are unique arrows $f:A\to B$ and $g:B\to A$.
  Then $g∘ f:A\to A$ and $f∘ g:B\to B$, but as per
  \cref{remark:terminal-object-id-unique}\TODO{how to make this ``remark''?},
  $g∘ f=\id_A$ and $f∘ g=\id_B$.
\end{proof}

\begin{definition}\label[definition]{def:product-and-coproduct}
  Given two objects $A,B\in\Obj\bfC$,
  \vspace{-0.3em}\dual{
    a \define{product}\index{Product!In a category} of $A$ and $B$
    consists of an object $C\in\Obj\bfC$ together with arrows $p_1:C\to A$ and
    $p_2:C\to B$ satisfying the following universal property:

    For any other ``candidate product'' $D\in\Obj\bfC$ with arrows
    $q_1:D\to A$ and $q_2:D\to B$, there is a unique arrow $u:D\to C$ making the
    following diagram commute:
    \begin{center}
      \begin{tikzcd}[sep=large,ampersand replacement=\&]
        {} \& D\arrow[dl,swap,"q_1"]\arrow[dr,"q_2"]
              \arrow[d,dashed,"u"] \& {} \\
        A \& C \arrow[l, "p_1"]\arrow[r,swap, "p_2"] \& B
      \end{tikzcd}
    \end{center}
    If $C$ is a product of $A$ and $B$, we will denote it by $A\times B$, and
    the unque arrow $u$ as $\langle f,g \rangle$.
  }{
    a \define{coproduct}\index{Coproduct!In a category} of $A$ and $B$
    consists of an object $C\in\Obj\bfC$ together with arrows $i_1:A\to C$ and
    $i_2:B\to C$ satisfying the following universal property:

    For any other ``candidate coproduct'' $D\in\Obj\bfC$ with arrows
    $j_1:A\to D$ and $j_2:B\to D$, there is a unique arrow $u:C\to D$ making the
    following diagram commute:
    \begin{center}
      \begin{tikzcd}[sep=large,ampersand replacement=\&]
        A \arrow[r, "i_1"{name=I1}]\arrow[dr, swap, "j_1"{name=F}]
        \& C \arrow[d,dashed,"u"]
        \& B\arrow[l, "i_2"{name=I2},swap]\arrow[dl,"j_2"{name=G}] \\
        {} \& D \& {}
      \end{tikzcd}
    \end{center}
    If $C$ is a coproduct of $A$ and $B$, we will denote it by $A+B$, and
    the unque arrow $u$ as $[f,g]$.
  }
\end{definition}

\section{Functors and their algebras}
\label{sec:functors-and-their-algebras}

\begin{definition}\label[definition]{def:functor}
	A \define{functor}\index{Functor} $F$ between categories $\bfC$ and $\bfD$
  consists of the following data:
  \begin{itemize}
    \itemsep-0.2em
    \item a map $F_0:\Obj \bfC\to\Obj \bfD$ 
    \item for each pair of objects $A,B\in\Obj C$, a map \\
      $F_1:\Hom_{\bfC}(A,B) \to\Hom_{\bfD}(F_0(A),F_0(B))$ 
  \end{itemize}
  These data are subject to the following axioms:
  \begin{enumerate}%[label=\Alph*.]
    \itemsep-0.2em
    \item functors preserve composition
    \item $F(\id_A)=\id_{F(A)}$ for all $A\in\Obj C$.
  \end{enumerate}
\end{definition}

We generally leave off the subscripts and parentheses when possible, denoting
the application by simply $FA$ or $Ff$. A functor $F$ from $\bfC$ to
$\bfD$ may be denoted $F:\bfC\to\bfD$. We may define functors without names
using the following notation:
\begin{align*}
  \bfC &⟶ \bfD \\
  A    &⟼_0 \ldots \\
  f    &⟼_1 \ldots
\end{align*}

\begin{example}\label[example]{ex:identity-functor-cat}
  For any category $\bfC$, there is an \define{identity
  functor}\index{Identity!Functor} $\id_{\bfC}\bfC\to\bfC$ which acts as the
  identity on objects and morphisms. The composition of functors is associative,
  and there is a category $\Cat$ of ``small'' categories (it doesn't include
  itself, for instance).
\end{example}

\begin{example}\label[example]{ex:forget}
  For each category of algebraic objects where the morphisms are the
  corresponding type of homomorphism, there is a \define{forgetful functor},
  generally denoted $U$, which takes sets with some structure to their
  underlying sets and homomorphisms to the corresponding maps of sets. For
  instance, there is a forgetful functor $U:\Grp\to\Set$.
\end{example}

\begin{example}\label[example]{ex:coproduct-functoriality}
	If $\bfC$ has distinguished binary coproducts, then 
  for any fixed $A,B\in\Obj\bfC$, one can define the following
  functors:\footnote{Really, this is a consequence of the fact that $+$ is
    what's called a \define{bifunctor}\indeX{Bifunctor}, but we don't need the
    full power of that statement for our purposes.}
  \begin{align*}
    \begin{split}
      \bfC &⟶ \bfC \\
      X    &⟼_0 A + X \\
      f    &⟼_1 \id_A+f = [i_1, i_2∘ f]
    \end{split}
    \begin{split}
      \bfC &⟶ \bfC \\
      Y    &⟼_0 Y + B \\
      g    &⟼_1 g+\id_B = [i_1∘ g, i_2]
    \end{split}
  \end{align*}
\end{example}

Since functors preserve sources, targets, and composition, they preserve
commutative diagrams. If $f,g,h$ form a commutative triangle in $\bfC$, then
their images under $F:\bfC\to\bfD$ do in $\bfD$:
\begin{center}
  \begin{tikzcd}[column sep=large]
    A\arrow[dr, "f"]\arrow[dd, "h"]\arrow[rr,mapsto,"F"] & {} & F A
    \arrow[dr, "F f"] \arrow[dd, near start, "F h", crossing over] & {} \\
    {} & B\arrow[dl, "g"]\arrow[rr,near start, mapsto, "F", crossing over] & {}
    & F B \arrow[dl, "F g"]\\
    C \arrow[rr,mapsto,"F"]& {} & F C &  {}
  \end{tikzcd}
\end{center}
Note however that it is possible that $FA=FB=FC$, as in a functor to a category
with a single object. However, the equalties between composites still hold.
One consequence is that functors preserve isomorphism.

\begin{definition}\label[definition]{def:endofunctor}
	An \define{endofunctor}\index{Functor!Endofunctor} is a functor with identical
  domain and codomain.
\end{definition}

\begin{definition}\label[definition]{def:f-coalgebra}
  \
  \vspace{-0.3em}\dual{
    An \define{algebra}\index{(Co)algebra for a functor} for an endofunctor
    $F:\bfC\to\bfC$ (also called an $F$\define{-algebra}) is a pair
    $(A,α)$ of an object $A\in\Obj\bfC$ and an arrow $α:FA\to A$.
  }{
    A \define{coalgebra} for an endofunctor
    $F:\bfC\to\bfC$ (also called an $F$\define{-coalgebra}) is a pair
    $(A,α)$ of an object $A\in\Obj\bfC$ and an arrow $α:A\to FA$
    \cite{category-theory-for-computing-science}.
  }
\end{definition}

For now, we will concentrate on properties of functor coalgebras, though all
constructions in this section hold dually for algebras.

\begin{definition}\label[definition]{def:coalgebra-morphism}
  A \define{coalgebra morphism}\index{(Co)algebra morphism} from
  $(A,α)$ to $(B,β)$ is an arrow $f:A\to B$ such that the following
  diagram commutes:
  \begin{center}
    \begin{tikzcd}
      A  \arrow[d, "α"] \arrow[r, "f"] & B \arrow[d, "β"] \\
      FA \arrow[r, "Ff"] & FB
    \end{tikzcd}
  \end{center}
\end{definition}

Since $F$ is a functor, the composition of coalgebra morphisms is again a
coalgebra morphism. In fact, $F$-coalgebras have all of the structure of a
category, which we will call $F\Coalg$.\TODO{Proof?}

\begin{definition}
  \
  \vspace{-0.3em}\dual{
    An \define{initial $F$-algebra}\index{Initial algebra} is an initial
    object of $F\Alg$.
  }{
    An \define{final $F$-coalgebra}\index{Final coalgebra} is a terminal
    object of $F\Coalg$.
  }
\end{definition}

The following example is crucial to
\cref{chap:coinductive-types-in-univalent-type-theory}.

\begin{example}\label[example]{ex:nat}
  Let $\bfC$ be a category with distinguished binary coproducts and a terminal
  object $1$. Consider the functor
  \begin{align*}
    F:\bfC &⟶ \bfC  \\
    A &⟼_0 1+A \\
    f &⟼_1 \id_1+f
  \end{align*}
  where 1 is a terminal object\index{Terminal object} and $+$ is the coproduct
  bifunctor as in \cref{ex:coproduct-functoriality}. Let's examine
  what it \textit{means} for some $F$-algebra $(N,η)$ to be initial. By
  composing with the coproduct\index{Coproduct!In category theory} injections,
  we can define
  \begin{align*}
    \begin{split}
      z &: 1 ⟶ N \\
      z &= η∘ i_1
    \end{split}
    \begin{split}
      s &: 1 ⟶ N \\
      s &= η∘ i_2
    \end{split}
  \end{align*}
  so that $η=[z,s]$:
  \begin{center}
    \begin{tikzcd}[sep=large,ampersand replacement=\&]
      1 \arrow[r, "i_1"{name=I1}]\arrow[dr, swap, "z"{name=F}]
      \& 1+N \arrow[d,"η"]
      \& N\arrow[l, "i_2"{name=I2},swap]\arrow[dl,"s"{name=G}] \\
      {} \& N \& {}
    \end{tikzcd}
  \end{center}
  Suppose we have another $F$-algebra $(A,α)$, and we define $f,g$ by
  composition as above so that $α=[f,g]$. By initiality of $(N,η)$,
  there is a unique arrow $u$ making the following diagram commute:
  \newcommand{\eeeeeta}{[z,s]}
  \newcommand{\aaaaalpha}{[f,g]}
  \begin{center}
    \begin{tikzcd}[sep=large]
      1+N \arrow[r, dashed, "\id_1+u"]\arrow[d, "\eeeeeta"] & 1+A\arrow[d, "\aaaaalpha"]  \\
      N \arrow[r, dashed, "u"] & A
    \end{tikzcd}
  \end{center}
  By functoriality of the coproduct (\ref{example:coproduct-functoriality}), we can
  compose along either the left- or right-hand paths in the above diagram.
  The above square states $u∘ [z,s]= [f,g]∘ (\id_1+u)$. Precomposing
  with $i_1:1\to 1+N$ yields
  \begin{align*}
    u∘ z = u∘ [z,s]∘ i_1
    &= [f,g]∘ (\id_1+u) ∘ i_1 
    && \text{Above diagram} \\
    &= [f,g]∘ [i_1,i_2∘ u] ∘ i_1 
    && \text{Definition of }+ \\
    &= [f, g]∘ i_1 \\
    &= f
  \end{align*}
  By similar reasoning, precomposing with $i_2$ yields
  \begin{equation*}
    u∘ s = u∘ [z,s] ∘ i_2
    = [f,g]∘(\id_1+u)∘ i_2
    = g∘ u.
  \end{equation*}
  Combining the above two equations, we have the following universal property
  for $(N,η)$. For any object $A$ with arrows $f:1\to A$ and $g:A\to A$,
  there is a unique arrow $u:N\to A$ making the following diagram commute:
  \begin{center}
    \begin{tikzcd}[sep=large]
      1
        \arrow[r, "z"]
        \arrow[dr, swap, "f"]
        & N \arrow[r, "s"] \arrow[d, "u", dashed]
        & N \arrow[d, "u", dashed] \\
      {}
      & A \arrow[r, "g"]
      & A
    \end{tikzcd}
  \end{center}
  Well, we've successfully rephrased the property of initiality, but it seems
  just as cryptic now as it was then. Let's see if we can figure out what an
  type with this property would look like in our favorite category \universe, the
  universe of small types in \abbreviation{ITT}!

  Let \List{\N} be the type of lists of natural numbers
  as in \cref{subsubsec:lists}. To utilize the above property, we need
  to choose $A$, $f$, and $g$. Pick the following:
  \begin{itemize}
    \itemsep0em
    \item $A:\equiv \List{\N}$
    \item $f:\equiv \λ{x}{\nil}$
    \item $g:\equiv \λ{l}{\cons(5,l)}$
  \end{itemize}
  From the universal property of $(N,[z,s])$, we get a function
  $u:N\to \List{A}$ such that
  \begin{align*}
    u(z) = \nil
    &&\text{and}&&
    u(s(n)) = \cons(5, u(n))
  \end{align*}
  Look familiar yet? Indeed, one type with such a property is $\N$! In that
  case, $u$ is the function that, when given a number $n$, outputs a list of $5$s
  with length $n$.\footnote{In $\Set$, $\N$ would be an initial algebra.
    It makes sense to ask if this functor has an initial algebra in any
    category with a terminal object and chosen binary coproducts, and in
    general, such algebras are called \define{natural number objects} (NNOs) 
    \cite{sketches}.
  }
\end{example}

\section{Type theoretic category theory}
\label{sec:type-theoretic-category-theory}

Unfortunately, terminology varies between the three predominant sources
on category theory in univalent type theory \cite{book} \cite{unimath}
\cite{hott-lib}. 

\chapter{Coinductive types in univalent type theory}
\label{chap:coinductive-types-in-univalent-type-theory}

\begin{notation}
  Following \cite{non-wellfounded}, we will adopt the notation
  $(X,α)⇒ (Y,β)$ or $X⇒ Y$ to denote the type of
  coalgebra morphisms from $(X,α)$ to $(Y,β)$.
\end{notation}

\begin{definition}\label[definition]{def:polynomial-functor}
  Given a signature $S\defeq (A,B)$, its associated \define{polynomial functor}
  is the function
  \begin{gather*}
    P:\universe\to \universe \\
    \apply{P_{A,B}}{X}\defeq\∑{a:A}{B(a)\to X}
  \end{gather*}
  We will regularly leave off the subscript for $P$.
  The \define{action} of $P$ on functions is
  \begin{gather*}
    P^* : (X\to Y)\to P_{A,B}(X)\to P_{A,B}(Y) \\
    \apply{P^*f}{(a,g)}\defeq (a,g∘ f).
  \end{gather*}
  \TODO{this isn't a functor}
\end{definition}

Fix a signature $S\defeq (A,B)$ and let $P$ be the associated polynomial functor.
We will prove a few auxiliary lemmas on the way to the following result.
This proof appeared in the \Agda{} formalization of \cite{non-wellfounded}, this
is its first appearance ``de-formalized''.

\begin{lemma}\label[lemma]{lemma:final-colagebra-unique}
  Any two final $P$-coalgebras are equal. In other words, the following type is
  a proposition:
  \begin{equation*}
    \Final(S)\defeq
    \∑{(X,α):\Coalgtype(S)}{
      \∏{(Y,β):\Coalgtype(S)}{\isContr((Y,β)⇒ (X,α))}
    }
  \end{equation*}
\end{lemma}

First, we'll show that their carriers are equivalent:

\begin{lemma}\label[lemma]{lemma:algebra-iso-equiv}
	If $(X,α)$ and $(Y,β)$ are final $P$-coalgebras,
  then the first projections of the unique coalgebra morphisms
  $f:X⇒ Y$ and $g:Y⇒ X$ induce
  an equivalence of types $\weq{X}{Y}$.
\end{lemma}
\begin{proof}
  This proof is a standard categorical technique, much reminiscent of
  \cref{lemma:terminal-unique}.
	\TODO{proof}
\end{proof}

We can then invoke the characterization of paths in $Σ$-types,
\cref{lemma:path-sigma}. We'll need to demonstrate that the coalgebra map
$α:X\to FX$ is equal to $β:Y\to FY$ when transported along the path
constructed in \cref{lemma:algebra-iso-equiv}.

\begin{lemma}\label[lemma]{lemma:polynomial-functor-transport}
  For all $X,Y:\universe$, $F:\universe\to\universe$,
  $f:X\to FX$, $g:Y\to FY$, and $p:\propeq{}{X}{Y}$, 
  if for all $x:X$ we have
  \begin{equation*}
    \propeq{}{
      \transport{F}{p}{\apply{f}{x}}
    }{
      g({\transport{\id_{\universe}}{p}{x}})
    }
  \end{equation*}
  then $\propeq{}{\transport{Z↦ (Z\to FZ)}{p}{f}}{g}$.
  That is, $f$ is equal to $g$ after being transported just when
  applying $f$ and transporting the result is the same as transporting the 
  input and applying $g$.
\end{lemma}
\begin{proof}
  Using identity elimination (\cref{rule:id-elim}), it suffices to assume
  $X\jdeq Y$ and $p\jdeq\refl{X}$. Then by the definition of transport
  (\cref{lemma:transport}), our hypothesis becomes
  \begin{align*}
    &\propeq{}{
      \transport{F}{p}{\apply{f}{x}}
    }{
      g({\transport{\id_{\universe}}{p}{x}})
    } \\
    &\implies
    \propeq{}{
      \transport{F}{\refl{X}}{\apply{f}{x}}
    }{
      g({\transport{\id_{\universe}}{\refl{X}}{x}})
    } \\
    &\implies
    \propeq{}{\apply{f}{x}}{\apply{g}{x}}
  \end{align*}
  so by function extensionality $f=g$. Again by definition of transport,
  $\transport{Z↦ (Z\to FZ)}{\refl{X}}{f} \jdeq f=g$.
\end{proof}

To complete the proof that the transported coalgebra maps are equal, we'll need
the following auxiliary result:

\begin{lemma}\label[lemma]{lemma:polynomial-functor-transport}
  For all $X,Y:\universe$ and $p:\propeq{}{X}{Y}$, 
  \begin{equation*}
    \propeq{}{
      P^*\paren{\transpor{\id_{\universe}}{p}}
    }{
      \transpor{P}{p}
    }
  \end{equation*}
  Note that $P^*$ is applied to the function $\transpor{\id_{\universe}}{p}$
  before it gets applied to its second argument.
\end{lemma}
\begin{proof}
  Note that we're using \cref{notation:transport}. By the elimination rule for
  the identity type, (\cref{rule:id-elim}), it suffices to assume that $X\jdeq
  Y$ and that $p\jdeq\refl{X}$. Then
  \begin{align*}
    P^*\paren*{\transpor{\id_{\universe}}{p}}
    &\jdeq P^*\paren*{\transpor{\id_{\universe}}{\refl{X}}}
    && \text{Identity elim.} \\
    &\jdeq P^*\paren{\id_X}
    && \text{\Cref{lemma:transport}} \\
    &\jdeq \λ{(a,f)}{(a,f∘ \id_X)}
    && \text{\Cref{def:polynomial-functor}} \\
    &\jdeq \λ{(a,f)}{(a,f)} \\
    &\jdeq \id_{PX}
    && \text{\Cref{def:id-polymorphic}} \\
    &\jdeq \transpor{\id_{\universe}}{\refl{PX}}
    && \text{\Cref{lemma:transport}} \\
    &\jdeq \transpor{\id_{\universe}}{\ap{P}{\refl{X}}}
    && \text{\Cref{lemma:ap}} \\
    &\jdeq \transpor{P}{\refl{X}}
    && \text{\Cref{lemma:transport-compose}} \\
    &\jdeq \transpor{P}{p}
    && \text{Identity elim.}
  \end{align*}
\end{proof}

\begin{proof}[Proof of \cref{lemma:final-colagebra-unique}]
  Let:
  \begin{itemize}
    \itemsep0em
    \item $(X,α)$, $(Y,β)$ be final $P$-coalgebras,
    \item $f:X⇒ Y$ and $g:Y⇒ X$ be the unique $P$-coalgebra
      morphisms between them,
    \item $q:\weq{X}{Y}$ the equivalence of types induced by $f$ and $g$
      (\cref{lemma:algebra-iso-equiv}).
  \end{itemize}
  By univalence, there is a path
    $\apply{\ua}{q}:\propeq{\universe}{X}{Y}$.\footnote{This
    is our first (but not nearly our last) crucial use of univalence. Without an
    equality, we couldn't \transportname{} $α$ to $β$ in the next step.
    This step also demonstrates that function extensionality alone doesn't
    suffice.} 
  To demonstrate that $\propeq{}{(X,α)}{(Y,β)}$, we invoke the
  characterization of paths in $Σ$-types, \cref{lemma:path-sigma}. It
  remains to show
  \begin{equation*}
    \propeq{(Y\to PY)}{\transport{Z↦ (Z\to PZ)}{\apply{\ua}{q}}{α}}{β}.
  \end{equation*}
  but by \cref{lemma:polynomial-functor-transport}, it suffices to show that
  for all $x:X$,
  \begin{equation*}
    \propeq{}{
      \transport{P}{\apply{\ua}{q}}{\apply{α}{x}}
    }{
      β(\transport{\id_\universe}{p}{x})
    }.
  \end{equation*}
  % Lemma 2 in HoTT/M-types
  First, note that
  \begin{align*}
    \transpor{P}{\apply{\ua}{q}}
    &= P^*\paren{\transpor{\id_{\universe}}{\apply{\ua}{q}}}
    && \text{\Cref{lemma:polynomial-functor-transport}} \\
    &= P^*q
    && \text{\Cref{def:ua}} \\
    &= P^*(\appr{1}{f})
    && \text{\Cref{lemma:algebra-iso-equiv}}
  \end{align*}
  The last step utilizes the idea of \textit{proof-relevant
    mathematics}. Although we define $q$ within a proof, we can (without
  cheating) refer to its definition from another proof. Also note the
  use of \cref{notation:weq-coerce}. Working from the other side of the
  equation, we can use the computational rule of univalence (\cref{def:ua}):
  \begin{align*}
    β(\transport{\id_\universe}{\apply{\ua}{q}}{x})
    = β(\apply{q}{x})
    = β({\appr{1}{f}}{x})
  \end{align*}
  Thus, we now want to demonstrate that
  \begin{align*}
    P^*(\appr{1}{f})(α x) &=
    \transport{P}{\apply{\ua}{q}}{\apply{α}{x}} \\
    &= β(\transport{\id_\universe}{p}{x}) \\
    &= β({\appr{1}{f}}{x})
  \end{align*}
  However, this is exactly the condition that $f$ is a $P$-coalgebra morphism:
  \TODO{reference definition}
  \begin{center}
    \begin{tikzcd}[column sep=large]
      X  \arrow[d, "α"] \arrow[r, "\appr{1}{f}"] & Y \arrow[d, "β"] \\
      FX \arrow[r, "P^*(\appr{1}{f})"] & FY
    \end{tikzcd}
  \end{center}
\end{proof}

\begin{definition}\label[definition]{def:limitt}
	Given a family of types $X:\N\to\universe$ and a family of functions
  $π_n:X_{n+1}\to X_n$, the \define{limit} of $(X,π)$ is the type
  \begin{equation*}
    L(X,π)\defeq \∑{x:\∏{n:\N}{X_n}}{\∏{n:\N}π_nx_{n+1}=x_n}
  \end{equation*}
  We give special names to the projection maps for limits: $p \defeq \pr{1}$ and
  $β \defeq \pr{2}$.
\end{definition}

\begin{lemma}\label[lemma]{lemma:limitt-universal}
	There is an equivalence of types
  \begin{equation*}
    \weq{(A\to L(X,π))}{\∑{f:\∏{n:\N}A\to X_n}{\∏{n:\N}{\propeq{}{π_n∘ f_{n+1}}{f_n}}}}
  \end{equation*}
\end{lemma}
\begin{proof}
	To give an equivalence, it suffices to give functions back and forth that
  compose to the respective identities.\TODO{reference definition}
  First, define
  \begin{gather*}
    ϕ :(A\to L(X,π)) ⟶ \∑{f:\∏{n:\N}A\to X_n}{\∏{n:\N}{\propeq{}{π_n∘ f_{n+1}}{f_n}}} \\
    ϕ(g) \defeq
    \dpair{\λ{n}\λ{a}\appply{p}{(\apply{g}{a})}{n}}
          {\λ{n}\appply{β}{(\apply{g}{a})}{n})}
  \intertext{and}
    ψ : \paren*[\bigg]{\∑{f:\∏{n:\N}A\to X_n}{\∏{n:\N}{π_n∘ f_{n+1}=f_n}}}
          \to A\to L(X,π) \\
    ψ((f; h)) \defeq \λ{a:A} \dpair{\λ{n}\appply{f}{n}{a}}{h}
  \end{gather*}
  Their composites are judgmentally equal to the appropriate identities.
  \TODO{should I include proof?}
\end{proof}

\begin{lemma}\label[lemma]{lemma:cochains}
	If $X:\N\to\universe$ is a family of types, $ρ:\∏{n:\N}{X_n\to X_{n+1}}$ is a
  family of functions, and
  \begin{equation*}
    Z\defeq \∑{x:\∏{n:\N}X_n}{\∏{n:\N}x_{n+1}=ρ_n(x_n)},
  \end{equation*}
  then $\weq{Z}{X_0}$, that is, limits of cochains are entirely determined by
  the first element.
\end{lemma}
\begin{proof}
  Consider again the functor of \cref{ex:nat}, specialized to the case of
  $\bfC\defeq\universe$:
  \begin{gather*}
    G:\universe ⟶ \universe  \\
    G(W) \defeq 1+W
  \end{gather*}
  A $G$-algebra is given by a type $W:\universe$, a point in $W$, and a function
  $W\to W$.
\end{proof}

\section{W and M}
\label{sec:w-and-m}

\section{Internalizing M-types}
\label{sec:internalizing-m-types}

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
\chaptermark{Conclusion}
\markboth{Conclusion}{Conclusion}
\setcounter{chapter}{4}
\setcounter{section}{0}


%If you feel it necessary to include an appendix, it goes here.
\appendix
\chapter{Cross-reference of names}

The following table lists lemmas taken from \cite{book}.
\begin{table}[ht]
  \centering
  \begin{tabular}{c | c}
    This thesis & The \abbreviation{HoTT} book \\ \hline
    \Cref{def:ua} & Axiom 2.10.3
  \end{tabular}
\end{table}

The following table compares the terminology used in this thesis to that in our
\Coq{} formalization (under the column \UniMath{}) and that of the \Agda{} development
of \cite{non-wellfounded} (under the column \MTypes{}).
\begin{table}[ht]
  \centering
  \begin{tabular}{c | c | c }
    This thesis & \UniMath{} & \MTypes{} \\ \hline
  \end{tabular}
\end{table}
% \chapter{The Second Appendix, for Fun}

\backmatter % backmatter makes the index and bibliography appear properly in the TOC

% if you're using bibtex, the next line forces every entry in the bibtex file to be included
% in your bibliography, regardless of whether or not you've cited it in the thesis.
\nocite{*}

%  \bibliographystyle{bsts/mla-good} % there are a variety of styles available; 
%  \bibliographystyle{plainnat}
% replace ``plainnat'' with the style of choice. You can crefer to files in the bsts or APA 
% subfolder, e.g. 
% \bibliographystyle{APA/apa-good}  % or
\bibliography{thesis}
\bibliographystyle{plain}
% Comment the above two lines and uncomment the next line to use biblatex-chicago.
%\printbibliography[heading=bibintoc]

% Finally, an index would go here... but it is also optional.
\index{$\id$|see {Identity}}
\index{$F$-algebra|see {Algebra for a functor}}
\printindex
\end{document}

