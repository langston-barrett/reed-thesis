\documentclass[12pt,twoside]{reedthesis}
\usepackage{graphicx}
\usepackage{booktabs,setspace}
% \usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% While drafting:
\makeatletter
\def\ifdraft{\ifdim\overfullrule>\z@
  \expandafter\@firstoftwo\else\expandafter\@secondoftwo\fi}
\makeatother

\ifdraft{
  \usepackage{showlabels}
  % \usepackage{showidx}
  \let\oldindex\index
  \definecolor{index}{HTML}{0088EE}
  \renewcommand{\index}[1]
               {\oldindex{#1}\marginpar{\footnotesize\color{index}index: #1}}
  \newcommand{\indeX}{\oldindex}
}{
  \newcommand{\indeX}{\index}
}

\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\definecolor{TODO}{HTML}{EE8800}
\newcommand{\TODO}[1]{\marginpar{\footnotesize\color{TODO}todo: #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% My stuff:
\input{../tex-preamble/math.tex}
\input{../tex-preamble/general.tex}
\input{../tex-preamble/amsthm.tex}
\input{../tex-preamble/problems.tex}
\input{../tex-preamble/logic.tex}
\input{../tex-preamble/hott.tex}
\input{../tex-preamble/unicode.tex}
\usepackage{rotating}
\usepackage{makeidx}
\usepackage{prftree}\setlength{\prfinterspace}{1.2em}

\newcommand{\software}[1]{{\textsc{#1}}\indeX{#1}}
\newcommand{\Agda}{\software{Agda}}
\newcommand{\UniMath}{\software{UniMath}}
\newcommand{\Coq}{\software{Coq}}
\newcommand{\MTypes}{\software{HoTT/M-Types}}

\definecolor{accepted}{HTML}{0088EE}
\definecolor{notaccepted}{HTML}{EE8800}
% \newcommand{\coqname}{\texttt}
% \newcommand{\unimathname}[1]{\underline{\texttt{#1}}}
\newcommand{\coqname}[1]{\texttt{\footnotesize\color{notaccepted} #1}}
\newcommand{\unimathname}[1]{\texttt{\footnotesize\color{accepted} #1}}

\usepackage{tikz}
\usetikzlibrary{cd,arrows.meta}
\tikzcdset{
  arrow style=tikz,
  arrows={line width=0.65pt},
  >={stealth}
}
\tikzset{every path/.append style={line width=0.65pt}}
\newcommand{\comma}{,}

% Gather environments with more interline spacing
\usepackage{environ}
\newlength{\oldjot}
\NewEnviron{gatherjot}{%
  \setlength{\oldjot}{\jot}\addtolength{\jot}{1em}
  \begin{gather*}
    \BODY
  \end{gather*}
  \setlength{\jot}{\oldjot}
}

\newcommand{\dual}[2]{
  \begin{itemize}\renewcommand{\labelitemi}{$∘$}
    \itemsep0em
    \item #1
    \item #2
   \end{itemize}
}

\newcommand{\define}[1]{\textbf{#1}} % term being defined
\newtheorem{notation}[theorem]{Notation}
\newtheorem{tt-rule}[theorem]{Rule}

\newcommand{\Algtype}{\ensuremath{\ttfun{Alg}}}
\newcommand{\Fibalgtype}{\ensuremath{\ttfun{FiberedAlg}}}
\newcommand{\Coalgtype}{\ensuremath{\ttfun{Coalg}}}
\newcommand{\Final}{\ensuremath{\ttfun{Final}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% My stuff:

\title{Deriving Coinductive Types in Univalent Type Theory}
\author{Langston Barrett}
\date{May 2018}
\division{Mathematics and Natural Sciences}
\advisor{Safia Chettih}
\department{Mathematics}
\thedivisionof{The Established Interdisciplinary Committee for}

\setlength{\parskip}{0pt}
\begin{document}

% \maketitle
\frontmatter % this stuff will be roman-numbered
\pagestyle{empty} % this removes page numbers from the frontmatter

% Acknowledgements (Acceptable American spelling) are optional
% So are Acknowledgments (proper English spelling)
% \chapter*{Acknowledgments}

% The pcreface is optional
% To remove it, comment it out or delete it.
% \chapter*{Pcreface}
% This thesis delves into highly interdisciplinary territory. I'll

% \chapter*{List of Abbreviations}

% \begin{table}[h]
%   \centering
%   \begin{tabular}{ll}
%     \HoTT  	&  Homotopy type theory \\
%     \UTT{}  	&  Univalent type theory \\
%     \ITT  	&  Intensional type theory \\
%     \FOL  	&  Classical first-order logic \\
%     \IPL{}  	&  Intuitionistic propositional logic \\
%     \LC{}  	  &  Church's untyped λ-calculus \\
%     \STLC{}  	&  Simply-typed λ-calculus \\
%     \TLC{}  	&  Typed λ-calculus
%   \end{tabular}
% \end{table}

% Depth to which to number and print sections in TOC
\setcounter{tocdepth}{4}
% \setcounter{secnumdepth}{2}
\tableofcontents
% if you want a list of tables, optional
% \listoftables
% if you want a list of figures, also optional
% \listoffigures

\chapter*{Abstract}

In this thesis we explain Univalent type theory, a constructive and
computationally meaningful foundational system for mathematics inspired by
recent advances in the semantics of Per Martin-L\"of's intuitionistic type
theory. We first develop the classical propositions/types correspondence between
(the constructive subset of) intuitionistic natural dedication and the
λ-calculus. We proceed to explicate Martin-L\"of's theory of dependent types and
the central modern development in type theory: Vladimir Voevodsky's Univalence
principle. We go on to examine the nature of coinduction within this theory,
presenting a novel formalization of a recent result that M-types can be derived
\textit{internally} in Univalent type theory.
% \chapter*{Dedication}

\mainmatter % here the regular arabic numbering starts
\pagestyle{fancyplain} % turns page numbering back on

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\chaptermark{Introduction}
\markboth{Introduction}{Introduction}

\section*{A short history of discomfort in mathematics}

% \subsection*{Trouble in Cantor's Paradise}

In the early 20\textsuperscript{th} century, mathematicians had a problem. Often
held up as the pinnacle of necessary, undoubtable, \textit{a priori} truth,
their discipline was suffering from an embarrassing lack of certainty. The
intricate arguments of advanced analysis left mathematicians unable to confirm
nor deny each other's proofs. Due to the wave of paradoxes unleashed by
Cantor's na\"ive set theory, the need for a rigorous logical foundation for higher
mathematics was so great that the enterprise of axiomatic set theory was pursued
headlong until some kind of consensus was reached. To make a long, rich story
brutally short, virtually every modern paper in mathematics and computer
science uses a combination of Gentzen's first-order logic (\FOL)
and an axiomatization of sets called
\ZFC.\footnote{This ``consensus'' left out many
  prominent schools of thought, such as the Intuitionists and constructivists, a
  point we'll soon return to.}

Despite the apparent rigor of \FOL+\ZFC,
practitioners of the deductive sciences were still beset by issues of
verifiability. Complexity, specialization and sheer length made modern proofs
difficult to comprehend with the absolute certainty which is supposed to
characterize these disciplines. In the 1970s, two teams of topologists proved
contradictory results, and neither group could find the error in the other's
proof \cite{kolata}. Wiles's famous proof of Fermat's last theorem was utterly
unintelligible to the vast majority of mathematicians \cite{nyt}. The
classification of the simple finite groups, one of the crowning achievements of
modern mathematics, has a combined proof of over ten thousand pages. These
examples illustrate merely a few of the practical epistemological challenges
facing mathematicians; for a historical perspective see \cite{rigor-and-proof},
and for a general overview see \cite{fidelity}.

In 1990s, Fields medalist Vladimir Voevodsky grew concerned with the state
of mathematical knowledge. In 1998, Carlos Simpson released a pre-print arguing
that there was a major mistake in one of Voevodsky's papers. However, it was not
clear whether Voevodsky had errored, or whether there was a flaw in Simpson's
counterexample. In 1999, Pierre Deligne found a crucial mistake in Voevodsky's
``Cohomological Theory of Presheaves with Transfers'', upon which he had based
much of his work in the area of motivic cohomology. As he began to develop more
and more complex arguments, Voevodsky wondered: ``And who would ensure that I
did not forget something and did not make a mistake, if even the mistakes in
much more simple arguments take years to uncover?'' \cite{voevodsky-ias}.

The problems facing Voevodsky and his peers seemed insurmountable.
As the requisite attention span, memory, and capacity for detail required to
understand new developments in higer mathematics reached inhuman proportions,
where was he to turn? He would not find a solution in the realm of pure
mathematics, but rather in one of the finest examples of collaboration between
mathematicians, computer scientists, and philosophers: the modern proof
assistant. \TODO{reword}

\section*{TODO}

While much of the mathematical discipline was simply relieved to have ``solved''
their foundational issues with axiomatic set theory, there remained a vocal
opposition to the newly-adopted methods. Most mathematicians are dimly aware
that there's some controversy about the Axiom of Choice (the ``C'' in
\ZFC) \cite{martin-lof-100-years}, you don't have to look further
than \FOL to find disagreements.

\section*{Our contribution}

(Co)inductive types and UniMath

begin
\begin{notation}
  All of the results of this thesis have been formalized in the \Coq{} proof
  assistant \cite{coq-manual}. The names of the formal proofs
  appear in a monospaced font (e.g.\ \coqname{univalence}). Many results have
  already been reviewed (by the \UniMath{} development team \cite{unimath} and
  Anders Mörtberg) and accepted into the \UniMath{} library, these appear
  with an underline (e.g.\ \unimathname{univalence}).
\end{notation}

\chapter{Propositions and Types}
\label[chapter]{chap:propositions-and-types}

In this chapter, we will present first a logical and then a computational
framework. We will begin with general remarks on discussing and defining formal
systems in \cref{sec:discussing-logic}. \Cref{sec:ipl} begins with an
intuitionistic, proof-relevant version of Gentzen's natural deduction
\cite{gentzen1935untersuchungen} called \IPL{}. As most
mathematicians, computer scientists, and philosophers are familiar with some
version of first-order logic, this section will focus on the introduction of
notation, meta-logical concerns, and the key differences between
\IPL{} and \FOL. In \cref{sec:the-lambda-calculus}, we
will introduce Church's λ-calculus \LC{} (and the simply-typed and
typed versions \STLC{} and \TLC{}). Finally, in
\cref{sec:propositions-and-types}, we will discuss the fundamental and
harmonious relation between these systems known as the Curry-Howard
correspondence.\TODO{Motivate the formality of this chapter}

The vocabulary of \cref{sec:discussing-logic} is due to Martin-L\"of. He
emphasized the importance of the notion of judgment (\cref{def:judgment})
\cite{martin-lof-meanings} and justified the rules of his type theory
by meaning explanations \cite{martin-lof-constructive}, which we will imitate
in \cref{subsec:ipl-intro}. See \cite{modal-judgment} for a similar application
of Martin-L\"of vocabulary to modal logic.

The presentation of \crefrange{sec:ipl}{sec:the-lambda-calculus} is influenced by
Philip Wadler \cite{wadler-propositions}, Frank Pfenning's lecture notes for
``Constructive Logic'', and Robert Harper's lectures on
\HoTT.\TODO{citation}

Two recent Reed graduates wrote theses on the Curry-Howard correspondence, both
taking a different approach in presentation that may complement this work,
\cite{curry-howard-reed-thesis} and \cite{process-calculi-reed-thesis}.

\begin{notation}\label[notation]{notation:parens}
  Throughout this thesis, we will omit parentheses when applying function-like
  constructions wherever this results in no ambiguity. Function application is
  left-associative. For example, for function-like constructions $f$, $g$, $h$,
  and $i$ we have
  \begin{align*}
    \apply{f}{h}=f(h) &&
    f(\apply{g}{h})=f(g(h)) && \appply{f}{(\apply{g}{i})}{h}=f(g(i))(h)
  \end{align*}
  and so on.
\end{notation}

\section{Discussing logic}
\label{sec:discussing-logic}

The observant student of logic may notice a problem in the usual definition of
implication: one usually defines $P→ Q$ as ``if whenever $P$ is true,
$Q$ is true, then $P→ Q$ is true''. It seems that in order to understand
implication, one must first understand implication! More perniciously,
philosophers and mathematicians alike have argued that Skolem's
``paradox''\footnote{(An application of) the L\"owenheim-Skolem theorem states
  that if there is a model of \ZFC, there is a countable one.
  The paradox is that the statement ``there are uncountable sets'' is (famously)
  a theorem of \ZFC, Cantor's.}
has deep and significant consequences for the philosophy of
mathematics \cite{skolem}. These misunderstandings both arise from a common
root: the failure to distinguish between object language and metalanguage. In
this section, we will attempt to mitigate some of the difficulties that arise
when discussing and defining formal systems for logic and computation.

\begin{definition}\label[definition]{def:object-meta-language}
  When discussing or defining a formal language (most commonly, a
  logico-deductive framework), we call that language the
  \define{object language}\index{object language}. Our discussion takes place in
  a \define{metalanguage}.
\end{definition}

\begin{example}
  \
  \begin{itemize}
    \itemsep0em
    \item In the statement ``\FOL is complete'', \FOL
      is the object language, and the metalanguage is English.
    \item G\"odel's second incompleteness theorem is a statement in the
      metalanguage of \FOL+\ZFC, about the object
      language of Peano arithmetic.
  \end{itemize}
\end{example}

\begin{figure}
  \centering
  \includegraphics[scale=1.1]{figures/nested-languages.pdf}
  \caption{\label{fig:nested}Nested languages, formal and informal. Outer boxes
    are meta-languages, inner boxes are object languages.}
\end{figure}

\begin{definition}\label[definition]{def:metavariable}
	A \define{metavariable}\index{Metavariable} is a variable meant to stand for
  any expression of our object language.
\end{definition}

\begin{notation}
  In imitation of various proof assistants, we will prefix metavariables with
  question mark, e.g.\ $\mvar{a}$.
\end{notation}

See \cref{fig:structure}, \cref{ex:judgments}, and \cref{sec:ipl} for examples
of uses of metavariables.

\begin{definition}\label[definition]{def:judgment}
	A \define{judgment}\index{Judgment} is something that could be known. A
  judgment is \define{evident} if one does, in fact, know it.
  A \define{proof} evidence for a judgment.
  A \define{hypothetical judgment}\index{Judgment!Hypothetical} is one that
  holds under the assumption that some other judgments hold.
\end{definition}

\begin{example}\label[example]{ex:judgments}
  Here are some examples of common judgments in logic (where the metavariables
  in these examples are to be understood as ranging over expressions of some
  unspecified object language):
  \begin{itemize}
    \itemsep0em
    \item $\mvar{a}$ is a well-formed formula
    \item $\mvar{a}$ is a proposition
    \item if $\mvar{a}$ and $\mvar{b}$ are propositions, then
      $\mvar{a}\land \mvar{b}$ is a proposition (this judgment is hypothetical)
    \item $\mvar{a}$ will happen in the future
    \item $\mvar{a}$ is a program with type $\mvar{t}$
    \item $\mvar{a}$ is true
    \item the variable $v$ is free (resp.\ bound) in $\mvar{a}$
  \end{itemize}
  If one is working in a formal metalanguage, judgments may be defined
  inductively in it.
\end{example}

\begin{figure}
  \centering
  \begin{equation*}
    \overbrace{\text{I know} \overbrace{\underbrace{\mvar{A}}_{\text{Expression}}\text{ is true}}^{\text{Judgment}}}^{\text{Evident judgment}}
  \end{equation*}
  \caption{\label{fig:structure}The structure of a judgment (transcribed
    from \cite{martin-lof-meanings})}
\end{figure}

% Since this thesis will define several different formal systems, it
% will be useful to establish some uniform notation for such definitions.
% Therefore, the following definitions and notations are part of our
% meta-language, which is plain English.

\begin{notation}\label[notation]{notation:proof-tree}
  The premises and conclusions of proofs or rules of deduction are always
  judgments. If we can deduce a conclusion $K$ from premises $J_1,\ldots,J_n$
  via some rule R (again: $J_1,\ldots,J_n$ and $K$ are \textit{judgments},
  phrased in plain Enlgish, not terms of an object language)\footnote{These
    could be called meta-metavariables.},
  we will write
  \begin{align*}
    \prftree[r]{\footnotesize R}
      {J_1}{J_2}{\ldots}{J_n}
      {K}
    &&\text{or}&&
    \prftree[r]{\footnotesize R}
      {\prftree[r, noline]{}
        {J_1}
        {J_2}}
      {\prftree[r, noline]{}
        {J_3}
        {J_4}}
      {\ldots}{J_n}
      {K}.
  \end{align*}
  Iterating this notation, we can write long derivations as trees with the
  conclusion as their root.
\end{notation}

\begin{notation}\label[notation]{notation:sequent}
  If $K$ is a hypothetical judgment holding under assumptions $J_1,\ldots,J_n$,
  we denote this by $J_1,\ldots,J_n⊢ K$ (``$⊢$'' can be read ``entails'').
  There are three (seemingly) related notions that entailment is distinct from:
  \begin{enumerate}
    \itemsep0em
    \item Gentzen's notion of syntactic entailment, which we won't discuss here,
    \item the deduction of $K$ from premises $J_1,\ldots,J_n$ (shown in
      \cref{notation:proof-tree}), and
    \item implication (see \cref{subsec:ipl-intro} for more discussion of this
      distinction).
  \end{enumerate}
  To denote some arbitrary sequence of hypotheses, we will use capital Greek
  letters $Γ$, $Δ$, and $Θ$.\footnote{Again, these are sorts of
  meta-metavariables.}
\end{notation}

\begin{definition}\label[definition]{def:entailment-rules}
  Our notions of entailment will always allow for \define{reflexivity},
  \define{weakening}, \define{contraction}, \define{substitution}, and
  \IPL{} will allow for \define{exchange}, which are the following
  ``meta-rules'' (where $J$, $K$, and $L$ stand for judgments):
  \begin{gatherjot}
    \prftree[r]{\footnotesize refl}{}
      {Γ,J,Δ ⊢ J}
    \qquad
    \prftree[r]{\footnotesize weak}
      {Γ ⊢ J}
      {Γ,K ⊢ J}
    \qquad
    \prftree[r]{\footnotesize contr}
      {Γ,K,Δ,K,Θ ⊢ J}
      {Γ,K,Δ,Θ ⊢ J} \\
    \prftree[r]{\footnotesize subst}
      {Γ,K,Δ ⊢ J}{Γ ⊢ K}
      {Γ,Δ ⊢ J}
    \qquad
    \prftree[r]{\footnotesize ex}
      {Γ,J,Δ,K,Θ ⊢ L}
      {Γ,K,Δ,J,Θ ⊢ L}
  \end{gatherjot}
\end{definition}

\begin{notation}\label[notation]{notation:substitution}
  We denote the substitution of a term $t$ for all free occurrences variable $v$
  in an expression $e$ by $e[v\coloneqq t]$. We will not deal directly with the
  issues of variable capture, scoping, and substitution here, for a rigorous
  treatment see any thorough textbook on logic.
\end{notation}

\begin{sidewaystable}
  \centering
  \begin{tabular}{c | l | l | l}
    Symbol               & Introduced & Language & Meaning \\ \hline
    $J,K,L$
      & \cref{notation:proof-tree}
      & English
      & Some arbitrary judgment \\
    $Γ,Δ,Θ$
                         & \cref{notation:proof-tree}
      & English                & Some arbitrary ordered sequence of judgments \\
    $Γ ⊢ J$
      & \cref{notation:sequent}
      & English
      & Under the hypotheses $Γ$, $J$ holds \\
    $\mvar{a},\mvar{b}$
      & \cref{def:metavariable}
      & English
      & Metavariable: a term in the object language \\
    $x\≔\mvar{a}$
      & TODO
      & English
      & ``x'' is a shorthand for the expression $\mvar{a}$ \\
    $\mvar{a}[\mvar{b} \≔ \mvar{c}]$
      & \cref{notation:substitution}
      & English
      & Substitute the expression $\mvar{c}$ for $\mvar{b}$ in $\mvar{a}$ \\
    $\prop{\mvar{a}}$
      & \cref{subsec:ipl-intro}
      & English
      & Judgment: $\mvar{a}$ is a proposition \\
    $\mvar{a}:\mvar{p}$
      & \cref{subsec:ipl-intro}, TODO: \STLC{}
      & English
      & Judgment: $\mvar{a}$ is a proof of (or has type) $\mvar{p}$ \\
    $\mvar{a}\≡\mvar{b}:\mvar{p}$
      & \cref{def:jdeq-ipl}
      & English
      & Judgment: $\mvar{a}$ is equal to $\mvar{b}$ proofs/elements of $\mvar{p}$ \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    $\apply{\mvar{a}}{\mvar{b}}$
      & \cref{notation:parens}
      &  All
      & Application of $\mvar{a}$ to $\mvar{b}$ \\
    $v,w,x,y,z$
      &
      & All
      & Variables (free or bound) \\
    % $c_0,c_1,\ldots$
    %   &
    %   & \(S)TLC
    %   & Constants \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    $\λ{x}\mvar{a}$
      &
      & \IPL{}, \formalsystem{((S)T)LC}, \UTT{}
      & A function that takes an input $x$ \\
    $→$
      &
      & \IPL{}, \formalsystem{(S)TLC}, \UTT{}
      & Material implication or function type \\
    $×$
      &
      & \ZFC+\FOL, \TLC{}, \UTT{}
      & Cartesian or categorical product, product type \\
    $+$
      &
      & \TLC{}, \UTT{}
      & Coproduct type, categorical coproduct \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    $(a,b)$
      &
      & \IPL{}, \TLC{}, \UTT{}
      & Constructor of the pair type \\
    $\inl$, $\inr$
      &
      & \IPL{}, \TLC{}, \UTT{}
      & Constructor of the coproduct type \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    $\pr{1}$, $\pr{2}$
      &
      & \IPL{}, \TLC{}, \UTT{}
      & Destructor of the product type \\
    $\case$
      &
      & \IPL{}, \TLC{}
      & Destructor of the coproduct type \\
    $\rec$
      & Various
      & \IPL{}, \TLC{}, \UTT{}
      & Recursion principle or elimination rule \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    $\∑{\mvar{a}:\mvar{p}}{\apply{\mvar{b}}{\mvar{a}}}$
      &
      & \UTT{}
      & Σ or dependent pair type \\
    $\∏{\mvar{a}:\mvar{p}}{\apply{\mvar{b}}{\mvar{a}}}$
      &
      & \UTT{}
      & Π or dependent pair type \\
  \end{tabular}
  \caption{\label{tab:symbols}Symbols and their interpretations}
\end{sidewaystable}

\section{Intuitionistic propositional logic}
\label{sec:ipl}

With these notations, we are prepared to define \IPL{}.
\Crefrange{sec:ipl}{sec:the-lambda-calculus} present these systems in some
degree of formality. Proofs in later chapters are not written in this style, but
it is important to see precisely specified versions of these logics in order to
understand \cref{sec:propositions-and-types}.

\subsection{Formation rules}
\label{subsec:ipl-form}

Since \IPL{} is supposed to be a logic, we better have a notion of a
proposition.\footnote{Why do we judge that terms are propositions, instead of
  well-formed formulae? Propositions are strictly more general. With
  hypothetical judgments, we can have a judgment of the form
  $\true{\mvar{a}}⊢ \prop{\mvar{b}}$, expressing that $\mvar{b}$ is a
  proposition \textit{under the assumption that $\mvar{a}$ is true}. We will
  need such expressive power in \cref{chap:type-theory}. This is also why we
  refrain from specifying our syntax via formal BNF (or similar) grammars.}
Indeed, we denote the judgment that some term $\mvar{a}$ represents a
proposition by $\prop{\mvar{a}}$. The following are the \define{formation
rules}\index{Formation!In \IFOL}:\footnote{For the less
  logically-versed reader: $⊤$ is read ``true'', $⊥$ is read ``false'',
  $\lnot$ is read ``not'', $\lor$ is read ``or'',
  $\land$ is read ``and'', $→$ is read ``implies'', $∀$ is read
  ``for all'', and $∃$ is read ``there exists''.
  \Cref{subsec:ipl-intro} attempts to justify these readings.}
\begin{gatherjot}
  \prftree[r]{}{}{Γ⊢\prop{⊤}}
  \qquad
  \prftree[r]{}{}{Γ⊢\prop{⊥}}
  \qquad
  \prftree[r]{}
    {\prop{Γ⊢\mvar{a}}}
    {\prop{Γ⊢\lnot \mvar{a}}} \\
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ⊢\prop{\mvar{a}→\mvar{b}}}
  \qquad
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ⊢\prop{\mvar{a}\land \mvar{b}}}
  \qquad
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ⊢\prop{\mvar{a}\lor\mvar{b}}} \\
  % \prftree[r]{}
  %    {\mvar{a}\prop}{v∉\FV(\mvar{a})}
  %    {∀ v.\mvar{a}}
  % \qquad
  % \prftree[r]{}
  %    {\mvar{a}\prop}{v∉\FV(\mvar{a})}
  %    {∃ v.\mvar{a}}
\end{gatherjot}
We add parentheses where necessary to disambiguate compound expressions.
As noted in \cref{tab:symbols}, ``$Γ$'', ``$⊢$'', and the horizontal line
``$\frac{\hspace{1em}}{\hspace{1em}}$'' are symbols in our metalanguage
(English), whereas ``$⊤,⊥,¬,→,∧,∨$'' are symbols in the object language
(\IPL{}). See \cref{tab:incoherent} for examples of some valid
expressions we could form with the above rules. Such valid expressions (in the
left column) are things that could be substituted in for our metavariables like
$\mvar{a}$.

\begin{table}[ht]
  \centering
  \begin{tabular}{l | l}
    \IPL{}                      & Incoherent \\ \hline
    $(\mvar{a}∧\mvar{b})∨\mvar{c}$          & $(Γ ⊢ \prop{\mvar{a}})∨\mvar{b}$ \\
    $(\mvar{a}→\mvar{b})→\mvar{c}$          & $∧∨→$ \\
    $⊤∨\mvar{a}_1∨\mvar{a}_2∨⋯∨\mvar{a}_n$  & $Γ⊢Θ$ \\
    $⊤→⊥$                                   & $\prop{\mvar{a}}→\mvar{b}∧\mvar{c}$ \\
  \end{tabular}
  \caption{\label{tab:incoherent}Valid \IPL{} expressions and incoherent
    nonsense.}
\end{table}

\subsection{Introduction rules}
\label{subsec:ipl-intro}

Martin-L\"of said ``The meaning of a proposition is determined by [...] what
counts as a verification of it'' \cite{martin-lof-meanings}. To
understand the meanings of the formation rules, we must define the judgment
``$\mvar{p}:\mvar{a}$'', read ``$\mvar{p}$ is a proof of $\mvar{a}$''. The
ways of proving a proposition are called \define{introduction rules}.

The symbol $⊤$ represents the trivially true proposition. Accordingly, we
give it a trivial proof (with a weird name which will make sense later):
\begin{equation*}
  \prftree[r]{}{}{Γ ⊢ \unitelem:⊤}.
\end{equation*}

How do we know a conjunction? Intuitively, we know (have a proof of) $\mvar{a}$
and $\mvar{b}$ (written $\mvar{a}\land\mvar{b}$) just when we know (have a proof
of) both $\mvar{a}$ and $\mvar{b}$. In symbols,
\begin{equation*}
  \prftree[r]{}
    {Γ ⊢ \prop{\mvar{a}}}{Γ ⊢ \prop{\mvar{b}}}
    {Γ ⊢ \mvar{p}:\mvar{a}}{Γ ⊢ \mvar{q}:\mvar{b}}
    {Γ ⊢ (\mvar{p},\mvar{q}):\mvar{a}\land\mvar{b}}.
\end{equation*}

What about disjunction? There are two ways to know $\mvar{a}$ or
$\mvar{b}$ (written $\mvar{a}\lor\mvar{b}$). We can either know $\mvar{a}$ or
know $\mvar{b}$. Correspondingly, we have two introduction rules:
\begin{align*}
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{a}}
    {Γ ⊢ \apply{\inl}{\mvar{p}}:\mvar{a}\lor\mvar{b}}
  &&\text{and}&&
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{b}}
    {Γ ⊢ \apply{\inr}{\mvar{p}}:\mvar{a}\lor\mvar{b}}.
\end{align*}
The symbol $⊥$ represents falsehood. We say a logic is
\define{consistent}\index{Consistency} if it cannot prove falsehood (a highly
desirable property!). To this end, we have no introduction rule for $⊥$.

What does it mean to know a negation? We'll define
$\lnot\mvar{a}\defeq \mvar{a}→⊥$ and instead worry about when we
know an implication.\footnote{This definition suggests the following ``derived
  introduction rule'' for $⊥$:
  \begin{equation*}
    \prftree[r]{}
      {Γ ⊢ \prop{\mvar{a}}}{Γ ⊢ \mvar{p}:\mvar{a}}{Γ ⊢ \mvar{q}:\lnot\mvar{a}}
      {Γ ⊢ \ttfun{whoops!}(\mvar{p},\mvar{q}):⊥}.
  \end{equation*}
  The derivation of this rule from the elimination rule for $→$
  (\cref{subsec:ipl-elim}) is immediate.}
An implication is true just when we have a hypothetical judgment where the
hypothesis is the antecedent and the conclusion is the consequent. Implication
allows us to \textit{internalize}\index{Internalizing!Hypothetical judgments}
the meta-theoretical notion of hypothetical judgments by turning them into
non-hypothetical ones. To define the introduction rule for $→$, we
assume the presence of a countably-infinite set of \define{variables}
$v,w,\ldots$
\begin{equation*}
  \prftree[r]{}
    {Γ ⊢ \prop{\mvar{a}}}{Γ ⊢ \prop{\mvar{b}}}
    {Γ,\mvar{p}:\mvar{a},Δ⊢ \mvar{q}:\mvar{b}}
    {Γ,Δ⊢\λ{v}{\mvar{q}[\mvar{p}\coloneqq v]}:\mvar{a}→\mvar{b}}.
\end{equation*}

\begin{definition}\label[definition]{def:valid-theorem}
  If know that under no hypotheses $\mvar{a}$ is a proposition and has a proof,
  then we may judge that $\mvar{a}$ is \define{valid}\index{Valid!In
  \IPL{}}:
  \begin{equation*}
    \prftree[r]{}
      {⊢\prop{\mvar{a}}}{⊢\mvar{p}:\mvar{a}}
      {⊢\valid{\mvar{a}}}.
  \end{equation*}
  In this case, we say that $\mvar{a}$ is a \define{theorem}\index{Theorem!Of
  \IPL{}} of \IPL{}.
\end{definition}

\subsection{Elimination rules}
\label{subsec:ipl-elim}

Once we know a judgment, how do we use it in a derivation?
We need \define{elimination rules}. How do we know what they should be?
In a sense to be made precise in \crefrange{subsec:proof-terms}{subsec:ipl-uni},
they should be dual to our introduction rules: we should be able to extract the
same amount of information from a proposition that went into its proof.

For conjunction, we should be able to recover proofs of both conjuncts:
\begin{align*}
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{a}\land\mvar{b}}
    {Γ ⊢ \appr{1}\mvar{p}:\mvar{a}}
  &&\text{and}&&
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{a}\land\mvar{b}}
    {Γ ⊢ \appr{2}\mvar{p}:\mvar{b}}.
\end{align*}
If both disjuncts imply a hypothesis, so does their disjunction:
\begin{align*}
  \prftree[r]{}
    {\prftree[r, noline]{}
      {\prftree[r, noline]{}
        {Γ ⊢ \prop{\mvar{a}}}
        {Γ ⊢ \prop{\mvar{b}}}}
      {Γ ⊢ \prop{\mvar{c}}}}
    {\prftree[r, noline]{}
      {Γ ⊢ \mvar{p}:\mvar{a}→\mvar{c}}
      {Γ ⊢ \mvar{q}:\mvar{b}→\mvar{c}}}
    {Γ ⊢ \mvar{r}:\mvar{a}\lor\mvar{b}}
    {Γ ⊢ \apppply{\case}{\mvar{p}}{\mvar{q}}{\mvar{r}}:\mvar{c}}.
\end{align*}
The elimination rule for implication has the common name
\define{modus ponens}\index{Modus ponens}, and expresses the idea that, if we
had a hypothetical judgment and then come to know all of the hypotheses, we can
deduce the consequence:
\begin{align*}
  \prftree[r]{}
    {Γ ⊢ \prop{\mvar{a}}}{Γ ⊢ \prop{\mvar{b}}}
    {Γ ⊢ \mvar{p}:\mvar{a}→ \mvar{b}}
    {Γ ⊢ \mvar{q}:\mvar{a}}
    {Γ ⊢ \apply{\mvar{p}}{\mvar{q}}:\mvar{b}}.
\end{align*}
The elimination rule for falsehood comes from the principle
\textit{ex falso quod libet}, ``from falsehood anything follows'' (also more
dramatically called the ``principle of explosion''). If we've been
able to prove $⊥$, our whole logic is bankrupt and we can derive anything we
please:
\begin{align*}
  \prftree[r]{}
    {Γ ⊢ \prop{\mvar{a}}}
    {Γ ⊢ \mvar{p}:⊥}
    {Γ ⊢ \apply{\rec_{⊥}}{\mvar{p}}:\mvar{a}}.
\end{align*}

At this point, we'll begin to see some full proofs. Since these quickly become
unmanageably large, we'll implicitly hypothesize that all metavariables involved
represent propositions.

\begin{example}\label[example]{ex:ipl-and-comm}
  The following proves the classical tautology that $\land$ is commutative:
  $\mvar{a}\land\mvar{b}⊢ \mvar{b}\land\mvar{a}$.
  \begin{equation*}
    \prftree[r]{}
      {\prftree[r]{}
        {Γ ⊢ \mvar{p}:\mvar{a}\land\mvar{b}}
        {Γ ⊢ \appr{2}{\mvar{p}}:\mvar{b}}}
      {\prftree[r]{}
        {Γ ⊢ \mvar{p}:\mvar{a}\land\mvar{b}}
        {Γ ⊢ \appr{1}{\mvar{p}}:\mvar{a}}}
      {Γ ⊢ (\appr{2}{\mvar{p}},\appr{1}{\mvar{p}}):\mvar{b}\land\mvar{a}}
  \end{equation*}
  This derivation serves as a small-scale verification that the introduction and
  elimination rules for conjunction complement one another well.
\end{example}

\begin{definition}\label[definition]{def:lem-dne}
  The \define{law of the excluded middle}\index{Law of excluded middle} (from
  the syllogistic principle \textit{tertium non datur}, ``no third is given'')
  is the following rule of deduction:
  \begin{equation*}
    \prftree[r]{\footnotesize LEM}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \apply{\ttfun{lem}}{\mvar{a}}:\true{\mvar{a}\lor\lnot\mvar{a}}}.
  \end{equation*}
  It is inter-derivable with the rule of \define{double negation elimination}:
  \index{Double negation elimination}
  \begin{equation*}
    \prftree[r]{\footnotesize DNE}
      {Γ ⊢ \prop{\mvar{a}}}{\mvar{p}:\lnot\lnot\mvar{a}}
      {Γ ⊢ \apply{\ttfun{dne}}{\mvar{p}}:\mvar{a}}.
  \end{equation*}
  that is, using the rule LEM, you can construct a proof of DNE and vice versa.
\end{definition}

\begin{example}\label[example]{ex:ipl-not-not-lem}
  A crucial consequence of the definition of \IPL{} is that the law
  of excluded middle isn't a theorem. However, we can demonstrate that
  its negation isn't either. While reading this proof, keep the following in
  mind:
  \begin{itemize}
    \itemsep0em
    \item for brevity, we abbreviate our hypothesis
      $\mvar{h}:\lnot(\mvar{a}\lor\lnot \mvar{a})$ as $\mvar{h}:H$
    \item $\lnot\mvar{a}\equiv \mvar{a}→⊥$
  \end{itemize}
  \begin{center}
  \noindent\makebox[\textwidth]{%
    \prftree[r]{}
      {\prftree[r]{}
        {\prftree[r]{}
          {\prftree[r]{}
            {\prftree[r]{}
              {\prftree[r]{\footnotesize weak}
                {\prftree[r]{}
                  {\prftree[r]{\footnotesize refl}
                    {\mvar{p}:\mvar{a}⊢\mvar{p}:\mvar{a}}}
                  {\mvar{p}:\mvar{a}⊢\apply{\inl}{\mvar{p}}:\mvar{a}\lor\lnot\mvar{a}}}
                {\mvar{p}:\mvar{a},\mvar{h}:H⊢\apply{\inl}{\mvar{p}}:\mvar{a}\lor\lnot\mvar{a}}}
              {\prftree[r]{\footnotesize ex}
                {\prftree[r]{\footnotesize weak}
                  {\prftree[r]{\footnotesize refl}
                    {\mvar{h}:H⊢\mvar{h}:H}}
                  {\mvar{h}:H,\mvar{p}:\mvar{a}⊢\mvar{h}:H}}
                {\mvar{p}:\mvar{a},\mvar{h}:H⊢\mvar{h}:H}}
              {\mvar{p}:\mvar{a},\mvar{h}:H⊢\apply{\mvar{h}}{(\apply{\inl}{\mvar{p}})}:⊥}}
            {\mvar{h}:H⊢\λ{v}\apply{\mvar{h}}{(\apply{\inl}{v})}:\lnot\mvar{a}}}
          {\mvar{h}:H⊢\apply{\inr}{(\λ{v}\apply{\mvar{h}}{(\apply{\inl}{v})})}:\mvar{a}\lor\lnot \mvar{a}}}
        {\prftree[r]{\footnotesize refl}
          {\mvar{h}:H⊢\mvar{h}:H}}
        {\mvar{h}:H⊢ \apply{\mvar{h}}{(\inr(\λ{v}\apply{\mvar{h}}{(\apply{\inl}{v})}))}:⊥}}
      {⊢\λ{w}\apply{w}{(\inr(\λ{v}\apply{w}{(\apply{\inl}{v})}))}:\lnot (\lnot(\mvar{a}\lor\lnot \mvar{a}))}
  }
  \end{center}
  The law of the excluded middle will be discussed further in \cref{subsec:on-lem}.
  \TODO{cite bob harper}
\end{example}

\subsection{Proof terms and harmony}
\label{subsec:proof-terms}

As we can see in \cref{ex:ipl-and-comm}, and \cref{ex:ipl-not-not-lem},
derivations in \IPL{} end with a \define{proof term} which
summarizes the whole proof tree. To demonstrate this, let's attempt to
reconstruct a proof tree based on a judgment including such a term. Suppose your
friend tells you they apprehended the following hypothetical judgment
in a dream, but upon waking, couldn't recall the derivation:
\begin{equation*}
    {Γ ⊢ \apply{\mvar{r}}{(\appr{1}{(\apply{\mvar{p}}{\mvar{q}})})} : \mvar{d}}.
\end{equation*}
Well, it certainly had to end with an application of implication eliimination
to hypotheses of the form $\mvar{b}$ and $\mvar{b}→ \mvar{d}$:
\begin{equation*}
  \prftree[r]{}
    {Γ ⊢ \mvar{r}:\mvar{b}→\mvar{d}}
    {\prftree[r]{}
      {?}
      {Γ ⊢ \appr{1}{(\apply{\mvar{p}}{\mvar{q}})}:\mvar{b}}}
    {Γ ⊢ \apply{\mvar{r}}{(\appr{1}{(\apply{\mvar{p}}{\mvar{q}})})} : \mvar{d}}.
\end{equation*}
and if the term of type $\mvar{b}$ was produced from the application of
$\pr{1}$, the term it was applied to had to be a proof of
$\mvar{b}\land \mvar{c}$ for some $\mvar{c}$:
\begin{equation*}
  \prftree[r]{}
    {Γ ⊢ \mvar{r}:\mvar{b}→\mvar{d}}
    {\prftree[r]{}
      {\prftree[r]{}
        {?}
        {Γ ⊢ \apply{\mvar{p}}{\mvar{q}}:\mvar{b}\land\mvar{c}}}
      {Γ ⊢ \appr{1}{(\apply{\mvar{p}}{\mvar{q}})}:\mvar{b}}}
    {Γ ⊢ \apply{\mvar{r}}{(\appr{1}{(\apply{\mvar{p}}{\mvar{q}})})} : \mvar{d}}.
\end{equation*}
which itself had to be applied to proofs of $\mvar{a}$ and
$\mvar{a}→\mvar{b}\land\mvar{c}$ for some $\mvar{a}$:
\begin{equation*}
  \prftree[r]{}
    {Γ ⊢ \mvar{r}:\mvar{b}→\mvar{d}}
    {\prftree[r]{}
      {\prftree[r]{}
        {Γ ⊢ \mvar{p}:\mvar{a}→(\mvar{b}\land\mvar{c})}
        {Γ ⊢ \mvar{q}:\mvar{a}}
        {Γ ⊢ \apply{\mvar{p}}{\mvar{q}}:\mvar{b}\land\mvar{c}}}
      {Γ ⊢ \appr{1}{(\apply{\mvar{p}}{\mvar{q}})}:\mvar{b}}}
    {Γ ⊢ \apply{\mvar{r}}{(\appr{1}{(\apply{\mvar{p}}{\mvar{q}})})} : \mvar{d}}.
\end{equation*}
At this point, we can (un)deduce no further. The form of the judgment allowed
us no discretion; this is the only derivation (up to\footnote{For the
  non-mathematical reader: When a mathematician says ``X is true up to $R$''
  for some (equivalence) relation $R$, it means that all objects related by $R$
  are considered equivalent. In this case, ``unique up to isomorphism'' means that
  there may be other terminal objects, but all of them are mutually isomorphic.}
renaming of metavariables) that could have produced that conclusion.

The proof term in longer arguments is often cumbersome, and we might wonder if
there are any benefits to internalizing\index{Internalizing!Proofs} proof. The
answer to this question will become clear in
\crefrange{subsec:ipl-compute}{subsec:ipl-uni}, and will play a critical role in
\cref{sec:propositions-and-types}. In short, they will allow us to
verify the \define{harmony}\index{Harmony} of our logic: that the introduction
rules complement the elimination rules with respect to two specific
criteria.\footnote{On the philosophical side, Martin-L\"of made an interesting
  argument that judgments involving proof terms are analytic, whereas
  proof-irrelevant mathematical arguments are synthetic: one must go
  beyond conceptual analysis and view the proof to be convinced of them
  \cite{martin-lof-analytic}.}

\begin{definition}\label[definition]{def:local-soundness-completeness}
  \define{Local soundness}\index{Soundness!Local} is the condition that
  elimination rules can't extract more information than was put into a
  conclusion by introduction rules; they aren't too strong.

  \define{Local completeness}\index{Completeness!Local} is the condition that
  elimination rules can recover all the information that was put into a conclusion
  by introduction rules; they aren't too weak.
\end{definition}

\subsection{Computation rules and judgmental equality}
\label{subsec:ipl-compute}

Some proofs are needlessly roundabout, and can be shortened.
Consider:
\begin{equation*}
  {\prftree[r]{}
    {\prftree[r]{}
      {Γ ⊢ \mvar{p}:\mvar{a}}
      {Γ ⊢ \mvar{q}:\mvar{b}}
      {Γ ⊢ (\mvar{p},\mvar{q}):\mvar{a}\land\mvar{b}}}
    {Γ ⊢ \appr{1}{(\mvar{p},\mvar{q})}:\mvar{a}}}.
\end{equation*}
This derivation could be eliminated entirely: we had a proof of $\mvar{a}$ to
begin with! This is an instance of a general pattern: whenever we have an
elimination rule for a connective just after its introduction, we can avoid the
circumlocution and cut straight to the conclusion.

Do such shortcuts result in \textit{the same} proof? To phrase this
question precisely, one needs a notion of identity for proof terms.
This notion is constructed just so we can answer this question in the
affirmative.

\begin{definition}\label[definition]{def:jdeq-ipl}
	\define{Judgmental equality} of proof terms, denoted
  $\mvar{p}\jdeq\mvar{q}:mvar{a}$, is the least congruence closed under the
  following
  \define{computation rules}\index{Computation rules!In \IPL{}}
  (also called \define{β-rules}):
  \begin{gatherjot}
    {\prftree[r]{\footnotesize β$∧$\textsubscript{l}}
      {Γ ⊢ \mvar{p}:\mvar{a}}{\mvar{q}:\mvar{b}}
      {Γ ⊢ \appr{1}{(\mvar{p},\mvar{q})}\jdeq \mvar{p}:\mvar{a}}}
    \qquad
    {\prftree[r]{\footnotesize β$∧$\textsubscript{r}}
      {Γ ⊢ \mvar{p}:\mvar{a}}{\mvar{q}:\mvar{b}}
      {Γ ⊢ \appr{2}{(\mvar{p},\mvar{q})} \jdeq \mvar{q}:\mvar{b}}} \\
    {\prftree[r]{\footnotesize β$→$}
      {Γ, \mvar{p}:\mvar{a}⊢\mvar{q}:\mvar{b}}
      {Γ ⊢ \mvar{r}:\mvar{a}}
      {Γ ⊢ \apply{(\λ{v}\mvar{q})}{\mvar{r}}\jdeq
        \mvar{q}[\mvar{p}\coloneqq\mvar{r}]:\mvar{b}}} \\
    {\prftree[r]{\footnotesize β$∨$\textsubscript{l}}
      {\prftree[r, noline]{}
        {Γ ⊢ \mvar{p}:\mvar{a}→\mvar{c}}
        {Γ ⊢ \mvar{q}:\mvar{b}→\mvar{c}}}
      {Γ ⊢ \mvar{r}:\mvar{a}}
      {Γ ⊢ \apppply{\case}{\mvar{p}}{\mvar{q}}{(\apply{\inl}{\mvar{r}})}\jdeq
        \apply{\mvar{p}}{\mvar{r}}:\mvar{c}}}
    \qquad
    {\prftree[r]{\footnotesize β$∨$\textsubscript{r}}
      {\prftree[r, noline]{}
        {Γ ⊢ \mvar{p}:\mvar{a}→\mvar{c}}
        {Γ ⊢ \mvar{q}:\mvar{b}→\mvar{c}}}
      {Γ ⊢ \mvar{r}:\mvar{b}}
      {Γ ⊢ \apppply{\case}{\mvar{p}}{\mvar{q}}{(\apply{\inr}{\mvar{r}})}\jdeq
        \apply{\mvar{q}}{\mvar{r}}:\mvar{c}}}.
  \end{gatherjot}
  (Recall \cref{notation:substitution} for the meaning of
  $\mvar{q}[\mvar{p}\coloneqq\mvar{r}]$).\footnote{
    Since the elimination rule for falsehood (\cref{subsec:ipl-elim}) gives back a
    proof of an arbitrary proposition (which is not in general one of the ``inputs''
    to falsehood introduction), there's no corresponding β-rule.}
\end{definition}

Besides allowing us to write shorter proofs, these rules witness local
soundness; they show that the elimination rules never prove any conclusion that
wasn't given to an introduction rule.

\subsection{Unicity rules}
\label{subsec:ipl-uni}

% Just because a logic doesn't give us an easy way to do something unexpected
% doesn't mean it can't be done. The prime example is proving a contradiction.
% History is filled with deductive systems that were thought to be sound, but
% were shown to be useless by some clever argument. Russell's paradox dispensed
% with Frege's \textit{Begriffsschrift}, the Burali-Forti antimony unraveled
% na\"ive set theories, and Girard's paradox revealed a fatal flaw in
% Martin-L\"of's early attempts at dependent type theory.

One worry we might have when designing our logic is that our propositions have
proofs of a non-standard form. We might \textit{think} that every proof of
$\mvar{a}∧\mvar{b}$ has the form $(\mvar{p},\mvar{q})$ for $\mvar{p}:\mvar{a}$ and
$\mvar{q}:\mvar{b}$, but how do we \textit{know}? The answer was is in
following \define{unicity rules}\index{Unicity rules}, (also known as
\define{η-rules} or \define{uniqueness principles}):
\begin{gatherjot}
  {\prftree[r]{\footnotesize η$⊤$}
    {Γ ⊢ \mvar{p}:⊤}
    {Γ ⊢ \mvar{p}\jdeq \unitelem : ⊤}}
  \qquad
  {\prftree[r]{\footnotesize η$∧$}
    {Γ ⊢ \mvar{p}:\mvar{a}∧\mvar{b}}
    {Γ ⊢ (\appr{1}{\mvar{p}},\appr{2}{\mvar{p}})\jdeq
      \mvar{p}:\mvar{a}∧\mvar{b}}} \\
  {\prftree[r]{\footnotesize η$→$}
    {Γ ⊢ \mvar{p}:\mvar{a}→\mvar{b}}
    {Γ ⊢ \λ{v}{(\apply{\mvar{p}}{v})}\jdeq \mvar{p}:\mvar{a}→\mvar{b}}}
  \qquad
  {\prftree[r]{\footnotesize η$∨$}
    {Γ ⊢ \mvar{p}:\mvar{a}∨\mvar{b}}
    {Γ ⊢ \apppply{\case}{\inl}{\inr}{\mvar{p}}\jdeq \mvar{p}:\mvar{a}∨\mvar{b}}}.
\end{gatherjot}
Naturally, these rules witness local completeness; after applying elimination
rules and then introduction rules we can recover our original proofs.

\subsection{Diagrams}
\label{subsec:ipl-diagrams}

One of the ways that we can understand our computation rules is via a clever
sort of diagram. This section foreshadows some connections between
\IPL{} and the content of \cref{chap:category-theory}.

\begin{definition}\label[definition]{def:implication-composition}
  Suppose given terms $\mvar{p}:\mvar{a}→\mvar{b}$ and
  $\mvar{q}:\mvar{b}→\mvar{c}$ under some hypotheses $\Gamma$. We define the
  \define{composition}\index{Composition!Of proof terms}
  $\mvar{q}∘\mvar{p}:\mvar{a}→\mvar{c}$ to be the term in the conclusion of the
  following deduction:
  \begin{equation*}
  {\prftree[r]{}
    {\prftree[r]{}
      {\prftree[r]{}
        {\prftree[r]{\footnotesize refl}
          {}
          {Γ, \mvar{r}:\mvar{a} ⊢ \mvar{r}:\mvar{a}}}
        {\prftree[r]{\footnotesize weak}
          {Γ ⊢ \mvar{p}:\mvar{a}→\mvar{b}}
          {Γ, \mvar{r}:\mvar{a} ⊢ \mvar{p}:\mvar{a}→\mvar{b}}}
        {Γ, \mvar{r}:\mvar{a} ⊢ \apply{\mvar{p}}{\mvar{r}}:\mvar{b}}}
      {\prftree[r]{\footnotesize weak}
        {Γ ⊢ \mvar{q}:\mvar{b}→\mvar{c}}
        {Γ, \mvar{r}:\mvar{a} ⊢ \mvar{q}:\mvar{b}→\mvar{c}}}
      {Γ, \mvar{r}:\mvar{a} ⊢ \apply{\mvar{q}}{(\apply{\mvar{p}}{\mvar{r}})}:\mvar{c}}}
    {Γ ⊢ \λ{v}{\apply{\mvar{q}}{(\apply{\mvar{p}}{v}})}:\mvar{a}→\mvar{c}}}.
  \end{equation*}
  In short, $\mvar{q}∘\mvar{p}\defeq \λ{v}{\apply{\mvar{q}}{(\apply{\mvar{p}}{v}})}$.
\end{definition}

Now suppose we have terms $\mvar{p}:\mvar{a}→\mvar{c}$ and
$\mvar{q}:\mvar{b}→\mvar{c}$. Via the elimination rule for $∨$, there is a
term $\apppply{\case}{\mvar{p}}{\mvar{q}}:\mvar{a}∨\mvar{b}→\mvar{c}$.
We can rewrite the β-rules using the composition we just defined:
\begin{align*}
  \appply{\case}{\mvar{p}}{\mvar{q}}∘\inl &\jdeq \mvar{p} \\
  \appply{\case}{\mvar{p}}{\mvar{q}}∘\inr &\jdeq \mvar{q}
\end{align*}
Similarly, given $\mvar{p}:\mvar{c}→\mvar{a}$ and $\mvar{q}:\mvar{c}→\mvar{b}$,
the introduction rule for $∧$ guarantees a function
$\mvar{f}\defeq\lam{v}{(\apply{\mvar{p}}{v}, \apply{\mvar{q}}{v})}:\mvar{c}→\mvar{a}∧\mvar{b}$.
The computation rules give us the following equalities:
\begin{align*}
  \pr{1}∘f &\jdeq \mvar{p} \\
  \pr{2}∘f &\jdeq \mvar{q}
\end{align*}
We can express these equalities using the following \define{commutative diagram}
(skip ahead to \cref{def:commutative-diagram} for details).

% Now suppose we have another term $\mvar{r}:\mvar{a}∨\mvar{b}→\mvar{c}$ which
% obeys the same equalities,
% \begin{align*}
%   \mvar{r}∘\inl &\jdeq \mvar{p} \\
%   \mvar{r}∘\inr &\jdeq \mvar{q}
% \end{align*}

% \begin{lemma}
% 	For any $\mvar{f}:\mvar{c}→\mvar{d}$,
%   \begin{equation*}
%     \appply{\case}{(f∘\inl)}{(f∘\inr)}
%     \jdeq f∘\appply{\case}{\inl}{\inr}
%   \end{equation*}
% \end{lemma}
% \begin{proof}
% 	\begin{align*}
%     \appply{\case}{(f∘\mvar{p})}{(f∘\mvar{q})}
%     &= \λ{v}{\apply{(\appply{\case}{(f∘\mvar{p})}{(f∘\mvar{q})})}{v}}
%   \end{align*}
% \end{proof}

\begin{center}
  \begin{minipage}[b]{0.47\linewidth}
    \centering
      \begin{tikzcd}[sep=large]
        A \arrow[dr, "\inl"{name=I1}]\arrow[ddr, swap, "\mvar{p}"{name=F}, bend right] &
        {} & B\arrow[dl, "\inr"{name=I2},swap] \arrow[ddl,"\mvar{q}"{name=G}, bend left] \\
        {} & A+B \arrow[d,dashed, "\appply{\case}{\mvar{p}}{\mvar{q}}"] & {} \\
        {} & C & {}
        % filling in the cells
        \arrow[phantom, shift left=0.3em, from=I1, to=F, "(\beta)"]
        \arrow[phantom, shift right=0.3em, from=I2, to=G, "(\beta)"]
        % \arrow[phantom, from=FG1, to=FG2, "(\eta)"]
      \end{tikzcd}
  \end{minipage}
  \begin{minipage}[b]{0.47\linewidth}
    \centering
      \begin{tikzcd}[sep=large]
        {} & C\arrow[ddl, bend right, swap, "\mvar{p}"{name=F}]\arrow[ddr, bend left, "\mvar{q}"{name=G}]
              % \arrow[d,dashed, bend right, swap, "\mathsf{rec}"{name=FG1}]
              \arrow[d,dashed, "\langle \mvar{p}\comma \mvar{q} \rangle"{name=FG2}] & {} \\
        {} & A\times B \arrow[dl, "\pr{1}"{name=P1}]
                      \arrow[dr,swap, "\pr{2}"{name=P2}] & {} \\
        A & {} & B
        % filling in the cells
        \arrow[phantom, shift right=0.3em, from=P1, to=F, "(\beta)"]
        \arrow[phantom, shift left=0.3em, from=P2, to=G, "(\beta)"]
        % \arrow[phantom, from=FG1, to=FG2, "(\eta)"]
      \end{tikzcd}
  \end{minipage}
\end{center}
In short, each of the two respective computation rules ensure that, whichever
(directed) path you take around the perimeters of the two triangles, composing
as you go, you get judgmentally equal results.

\section{The λ-calculus}
\label{sec:the-lambda-calculus}

In the 1930s, mathematicians and computer scientists (though they were not
called ``computer scientists'' then) sought a rigorous foundation for terms like
``effectively computable'' and ``systematic''. They sought to lay on a rigorous
logical foundation an intuitive idea of what a(n idealized) human being could
accomplish with specific\footnote{(finite)} instructions and arbitrarily much
paper, pen, and time. Many particular mathematical models were proposed,
including the λ-calculus (\LC{}), Turing machines, Post systems,
register machines, combinatory definability, and more \cite{sep-church-turing}.
The \textit{Church-Turing thesis} proposes that the proper definition is (any
model equivalent to) the λ-calculus.\footnote{Perhaps the strongest reason to
  accept such a thesis is the theorem that all models just mentioned are
  equivalent.}
Computer scientists widely adopt this thesis, and the λ-calculus and its
variants are of central importance to the study of computation in
general.

A comprehensive study of \LC{} is beyond the scope
of this thesis. For a tutorial on \LC{} and \STLC{} to
complement the presentation here, see \cite{lambda-lecture}. For a standard
reference on \LC{} (with an appendix on \STLC{}), see
\cite{barendregt}.

The type theory considered in \cref{chap:type-theory} is an extension of the
typed λ-calculus (\TLC{}) of \cref{subsec:more-complex-types}, which
itself an extension of the simply-typed λ-calculus (\STLC{}) of
\cref{subsec:the-simply-typed-lambda-calculus}.

\subsection{The untyped λ-calculus}
\label{subsec:the-untyped-lambda-calculus}

The formation\index{Formation!In the λ-calculus} (really, all) rules of the
λ-calculus are brilliantly simple.\footnote{Again, ignoring issues of variable
binding.} We will again assume the presence of a countably infinite set of
variables $v,w,\ldots$:
\begin{align*}
  {\prftree[r]{}
    {}
    {Γ ⊢ \term{v}}}
  &&
  {\prftree[r]{}
    {Γ ⊢ \term{\mvar{a}}}
    {Γ ⊢ \term{\mvar{b}}}
    {Γ ⊢ \term{\apply{\mvar{a}}{\mvar{b}}}}}
  &&
  {\prftree[r]{}
    {Γ ⊢ \mvar{a}}
    {Γ ⊢ \term{\lam{v}{\mvar{a}}}}}
\end{align*}
In short, we have variables, application of one term to another, and
\define{function abstraction}. The λ-calculus is the inspiration for our
notation of function application by juxtaposition (\cref{notation:parens}).
Accordingly, function application is left-associative.

There are no elimination rules. Again, we define judgmental equality λ-terms to
be the least congruence closed under the following computation
rule\index{Computation rules!In \LC{}} and subject to the following
unicity rule:
\begin{align*}
  {\prftree[r]{\footnotesize β}
    {Γ ⊢ \term{\mvar{a}}}
    {Γ ⊢ \term{\mvar{b}}}
    {Γ ⊢ \apply{(\λ{v}\mvar{a})}{\mvar{b}}\jdeq
      \mvar{a}[v\coloneqq\mvar{b}]}}
  &&
  {\prftree[r]{\footnotesize η}
    {Γ ⊢ \term{\mvar{a}}}
    {Γ ⊢ \λ{v}{(\apply{\mvar{a}}{v})}\jdeq \mvar{a}}}.
\end{align*}

\begin{notation}\label[notation]{notation:beta}
  If we apply the computation rule to $\mvar{a}$ and get some term $\mvar{a}'$,
  we write $\mvar{a} \betato \mvar{a}'$. If we apply it an arbitrary but finite
  (possibly zero) amount of times to get $\mvar{a}''$, we write $\mvar{a}
  \betatoo \mvar{a}''$.
\end{notation}

\begin{example*}\label[example]{ex:calculations-in-lc}
  Under the computation rule, we have
  \begin{align*}
    &\appply
      {(\λ{u}{\λ{v}{\λ{w}{\appply{\mvar{x}}{v}{w}}}})}
      {(\λ{y}{y})}
      {(\λ{z}{\apply{z}{\mvar{a}}})} \\
    \≡ &\apply
      {(\λ{v}{\λ{w}{\appply{\mvar{x}}{v}{w}}})[u\≔ (\λ{y}{y})]}
      {(\λ{z}{\apply{z}{\mvar{a}}})}
    &&\quad\text{(β)} \\
    \≡ &\apply
      {(\λ{v}{\λ{w}{\appply{\mvar{x}}{v}{w}}})}
      {(\λ{z}{\apply{z}{\mvar{a}}})}
    &&\quad\text{(substitution)} \\
    \≡ & (\λ{w}{\appply{\mvar{x}}{v}{w}})[v\≔ \λ{z}{\apply{z}{\mvar{a}}}]
    &&\quad\text{(β)} \\
    \≡ & (\λ{w}{\appply{\mvar{x}}{(\λ{z}{\apply{z}{\mvar{a}}})}{w}})
    &&\quad\text{(substitution)}.
  \end{align*}
  (Recall the meta-theoretical notation $\mvar{a}[\mvar{b} \≔ \mvar{c}]$,
  \cref{notation:substitution}.)
  Since $\mvar{x}$ is some arbitrary term, we can reduce this expression no
  further.
\end{example*}

\begin{example*}\label[example]{ex:fixed-point}
  A \define{fixed point} of a function $f$ (in \LC{},
  \FOL+\ZFC, or any other formal system or programming language)
  is an input $x$ such that $\apply{f}{x}=x$. Every term of \LC{}
  has a fixed point (up to judgmental equality $\≡$). To see this, define
  $θ \defeq (\λ{x}{\λ{y}{\apply{y}{(\appply{x}{x}{y})}}})$ and let
  $𝚯\defeq \apply{θ}{θ}$. Let $\mvar{f}$ be any term of \LC{}. Then
  \begin{align*}
    \apply{𝚯}{f}
    &\≡ \appply{θ}{θ}{f} \\
    &\≡ \appply{(\λ{x}{\λ{y}{\apply{y}{(\appply{x}{x}{y})}}})}{θ}{f} \\
    &\≡ \apply{f}{(\appply{θ}{θ}{f})} \\
    &\≡ \apply{f}{(\apply{𝚯}{f})}
  \end{align*}
  The term $𝚯$ is called the \define{Turing fixed-point combinator}, and it can
  be used to define recursive functions in \LC{} and its derivatives
  \cite{lambda-lecture}.
\end{example*}

We can make choices about what part of an expression to apply the computation
rule to. Consider the following computations:
\begin{center}
  \begin{tikzpicture}[node distance=1.5cm]
    \node[] (A) at (0, 0) {$
      \apply{(\apply{(\λ{x}{\apply{x}{x}})}
                    {(\λ{y}{\mvar{a}})})}
            {(\apply{(\λ{z}{\apply{z}{\mvar{b}}})}
                      {(\λ{w}{\mvar{c}})})}
    $};
    %%%%%%%%%%%%%%%%%%%%%%% LEFT
    \node[below of=A, xshift=-8em] (B) {$
      \apply{(\apply{(\λ{y}{\mvar{a}})}{(\λ{y}{\mvar{a}})})}
            {(\apply{(\λ{z}{\apply{z}{\mvar{b}}})}
                      {(\λ{w}{\mvar{c}})})}
    $};
    \node[below of=B] (C) {$
      \apply{(\apply{(\λ{y}{\mvar{a}})}{\mvar{a})})}
            {(\apply{(\λ{z}{\apply{z}{\mvar{b}}})}
                      {(\λ{w}{\mvar{c}})})}
    $};
    \node[below of=C] (D) {$
      \apply{\apply{\mvar{a}}{\mvar{a}}}
            {(\apply{(\λ{z}{\apply{z}{\mvar{b}}})}
                      {(\λ{w}{\mvar{c}})})}
    $};
    \node[below of=D] (E) {$
      \apply{\apply{\mvar{a}}{\mvar{a}}}
            {(\apply{(\λ{w}{\mvar{c}})}{\mvar{b}})}
    $};
    %%%%%%%%%%%%%%%%%%%%%%% RIGHT
    \node[below of=A, xshift=8em] (F) {$
      \apply{(\apply{(\λ{x}{\apply{x}{x}})}
                    {(\λ{y}{\mvar{a}})})}
            {(\apply{(\λ{w}{\mvar{c}})}{\mvar{b}})}
    $};
    \node[below of=F] (G) {$
      \apply{(\apply{(\λ{x}{\apply{x}{x}})}
                    {(\λ{y}{\mvar{a}})})}
            {(\apply{\mvar{b}}{\mvar{c}})}
    $};
    \node[below of=G] (H) {$
      \apply{(\apply{(\λ{y}{\mvar{a}})}{(\λ{y}{\mvar{a}})})}
            {(\apply{\mvar{b}}{\mvar{c}})}
    $};
    %%%%%%%%%%%%%%%%%%%%%%% LAST
    \node[below of=A, yshift=-13em] (Z) {$
      \appply{\mvar{a}}{\mvar{a}}{(\apply{\mvar{b}}{\mvar{c}})}
    $};
    %%%%%%%%%%%%%%%%%%%%%%% ARROWS
    %%%%% LEFT
    \draw (A) edge [->] node[above] {\scriptsize β} (B);
    \draw (B) edge [->] node[left] {\scriptsize β} (C);
    \draw (C) edge [->] node[left] {\scriptsize β} (D);
    \draw (D) edge [->] node[left] {\scriptsize β} (E);
    \draw (E) edge [->] node[below] {\scriptsize β} (Z);
    %%%%% RIGHT
    \draw (A) edge [->] node[above] {\scriptsize β} (F);
    \draw (F) edge [->] node[right] {\scriptsize β} (G);
    \draw (G) edge [->] node[right] {\scriptsize β} (H);
    \draw (H) edge [->] node[below] {\scriptsize β} (Z);
  \end{tikzpicture}
\end{center}
In the left column, we reduce the leftmost expression first, whereas in the
right column, we reduce the rightmost expression first. In this example, they
result in the same term, but there is \textit{a priori} no reason to think this
is so in general.

\begin{theorem}[Church-Rosser]\label[theorem]{thm:church-rosser}
  The order in which we apply computation rules to a term of \LC{}
  doesn't matter. More specifically, if $\mvar{a}\betatoo\mvar{b}$ and
  $\mvar{a}\betatoo\mvar{c}$, then there is a term $\mvar{d}$ such that
  $\mvar{b}\betatoo\mvar{d}$ and $\mvar{c}\betatoo \mvar{d}$.
  Diagrammatically,
  \begin{center}
    \begin{tikzcd}[sep=small, every arrow/.append style={two heads, "β"}]
      {} &
      \mvar{a}
        \arrow[dl, swap]
        \arrow[dr] &
      {} \\
      \mvar{b}
        \arrow[dr, dashed, swap] &
      {} &
      \mvar{c}
        \arrow[dl, dashed] \\
      {} & \mvar{d} & {}
    \end{tikzcd}
  \end{center}
\end{theorem}

Note that the Church-Rosser Theorem doesn't claim judgmental equality
$\≡$,\footnote{If it did, it would be trivial; the β-rules preserve judgmental
equality.} but actual \textit{syntactic identity}, they compute to the exact
same term.

\begin{definition}\label[definition]{def:normal-form}
	If $\mvar{a}$ is a term of \LC{} such that there does not exist
  a term $\mvar{a}'$ with $\mvar{a}\betato \mvar{a}'$, then we say that
  $\mvar{a}$ is in \define{(β-)normal form}\index{Normal form}.
  If $\mvar{b}\betatoo\mvar{b}'$ and $\mvar{b}'$ is in normal form, we say
  $\mvar{b}'$ is the normal form of $\mvar{b}$.
\end{definition}

The use of the definite article in \cref{def:normal-form} is justified by
Church-Rosser: every term has at most one β-normal form. However, not every
term has a β-normal form.

\begin{example*}\label[example]{ex:non-terminating}
  Consider the term $⊥\≔ \λ{v}{\appply{v}{v}{v}}$. Using the computation rule,
  \begin{align*}
    \apply{⊥}{⊥}
    &\≡ \apply{(\λ{v}{\appply{v}{v}{v}})}{⊥}
    &&\quad\text{(Definition)} \\
    &\≡ (\appply{v}{v}{v})[v \defeq ⊥]
    &&\quad\text{(β)} \\
    &\≡ \appply{⊥}{⊥}{⊥}
    &&\quad\text{(Substitution)} \\
    &\≡ \apppply{⊥}{⊥}{⊥}{⊥}
    &&\quad\text{(…)} \\
    &\≡ \appppply{⊥}{⊥}{⊥}{⊥}{⊥} \\
    &\≡ ⋯
  \end{align*}
  Not only can we build terms that don't really ``progress'' when we try to
  ``compute'', but we can build terms that actually \textit{expand} under rules
  meant to \textit{reduce} them, that are judgmentally equal to infinitely many
  other terms!
\end{example*}

While an ability to loop indefinitely is essential to Turing-Completeness (the
property of being equivalent to a Turing machine model of computation), we
might ask if there is another system in which we could express many of the same
ideas and computations, but with a computation rule that always ``terminates''.

\subsection{The simply-typed λ-calculus}
\label{subsec:the-simply-typed-lambda-calculus}

In order to fix the problem raised in \cref{ex:non-terminating}, we can add
\define{types}\index{Types!Simply-typed λ-calculus} to \LC{} to get
\STLC{}, the simply-typed λ-calculus. We consider two new judgments
of the form ``$\type{\mvar{t}}$'' and ``$\mvar{a}:\mvar{t}$'', read
``\mvar{t} is a type'' and ``\mvar{a} has type \mvar{t}'', respectively (recall
that ``$:$'' is a symbol of the meta-language, it is just shorthand for writing
a judgment). In writing these judgments, we implicitly assume that all
expressions are well-formed terms. As a first approximation, a type can be
thought of as a description of a program's behavior.

This language begins with additional formation rules
\begin{align*}
  {\prftree[r]{}
    {}
    {Γ ⊢ \type{\groundtype_i}}}
  &&
  {\prftree[r]{}
    {Γ ⊢ \type{\mvar{s}}}
    {Γ ⊢ \type{\mvar{t}}}
    {Γ ⊢ \type{\mvar{s}→\mvar{t}}}}
\end{align*}
The $\groundtype_i$ are called \define{ground types}, \define{base types}, or
\define{atomic types}.\footnote{There can be as many ground types as necessary
  or only one, it doesn't fundamentally change the character of the resulting
  theory.}
The term $\mvar{a}→\mvar{b}$ is the type of functions from $\mvar{a}$ to
$\mvar{b}$. The introduction
rule\index{Introduction rules!In \STLC{}} tells us how to form
elements of these types:
\begin{align*}
  {\prftree[r]{}
    {Γ, \mvar{a}:\mvar{s} ⊢ \mvar{b}:\mvar{t}}
    {Γ ⊢ \λ{v}{\mvar{b}}:\mvar{s}→\mvar{t}}}
    % {Γ ⊢ \λ{v}{\mvar{b}}[\mvar{a}\defeq v]:\mvar{s}→\mvar{t}}}
\end{align*}
Introduction rules for some ground type(s) are reasonable additional axioms, but
they are not necessary for the development of the theory.\footnote{This might
look like
\begin{align*}
  {\prftree[r]{}
    {}
    {Γ ⊢ z:\groundtype_0}}.
\end{align*}
which would assert the existence of an element $z$ of the 0\textsuperscript{th}
ground type $\groundtype_0$.
} An elimination rule balances this introduction:
\begin{align*}
  {\prftree[r]{}
    {Γ ⊢ \mvar{a}:\mvar{s}→\mvar{t}}
    {Γ ⊢ \mvar{b}:\mvar{s}}
    {Γ ⊢ \apply{\mvar{a}}{\mvar{b}}:\mvar{t}}}
\end{align*}
The computation rules\index{Computation rules!In \STLC{}} and
unicity rules remain much the same, only now with some restrictions on the types
of the expressions involved:
\begin{align*}
  {\prftree[r]{\footnotesize β}
    {Γ, \mvar{a}:\mvar{s} ⊢ \mvar{b}:\mvar{t}}
    {Γ ⊢ \mvar{c}:\mvar{s}}
    {Γ ⊢ \apply{(\λ{v}\mvar{b})}{\mvar{c}}\jdeq
      \mvar{b}[\mvar{a}\coloneqq\mvar{c}]}}
  &&
  {\prftree[r]{\footnotesize η}
    {Γ ⊢ \term{\mvar{a}}}
    {Γ ⊢ \λ{v}{(\apply{\mvar{a}}{v})}\jdeq \mvar{a}}}.
\end{align*}
Variables may stand in for terms of arbitrary type.

\begin{theorem}\label[theorem]{thm:strong-normalization}
  \STLC{} is \define{strongly normalizing}; every term has exactly
  one normal form.
\end{theorem}

\begin{example}
	Recall the problematic term of \cref{ex:non-terminating}:
  $⊥\≔ \λ{v}{\apply{v}{v}}$. Can the introduction rules assign this term a type?
  To determine the overall type of $⊥$, we must determine the type of its
  argument. Its argument gets applied to a term, so it must have a function type
  $\mvar{a} → \mvar{b}$. However, since it is applied to
  itself, it must have type $(\mvar{a} → \mvar{b}) → \mvar{b}$. But now it still
  can't be applied to itself, so it must have type
  $((\mvar{a} → \mvar{b}) → \mvar{b}) → \mvar{b}$. Of course, this problem
  persists no matter how many layers are added, and leads us to the following
  conclusion: no well-typed term may be applied to itself.
\end{example}

\subsection{More complex types}
\label{subsec:more-complex-types}

The ontology of \STLC{} is severely limited: it can only express
ground type(s) and functions between them. This section develops
various \define{type formers}, which build up more complex types
from simpler ones (e.g.\ $→$ from \STLC{}). In
\cref{sec:propositions-and-types}, we will see that these types conform to
intuitions from \cref{sec:ipl}.

We will specifically consider four additional types (plus $→$), formed via the
following rules\index{Formation rules!In \TLC{}}:
\begin{gatherjot}
  \prftree[r]{}{}{Γ⊢\type{⊤}}
  \qquad
  \prftree[r]{}{}{Γ⊢\type{⊥}}
  \qquad
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \type{\mvar{a}}}
      {Γ ⊢ \type{\mvar{b}}}}
    {Γ⊢\type{\mvar{a}×\mvar{b}}}
  \qquad
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \type{\mvar{a}}}
      {Γ ⊢ \type{\mvar{b}}}}
    {Γ⊢\type{\mvar{a}+\mvar{b}}}
\end{gatherjot}

Perhaps unsurprisingly, the type $⊥$ is empty, it has no elements.
The type $⊤$ has one element:\footnote{This rule only really says we have one
  known way to construct an element of $⊤$, but the η-rules for this type
  justify the claim that this is the only such element.}
\begin{equation*}
  \prftree[r]{}{}{Γ ⊢ \unitelem:⊤}.
\end{equation*}
The type $\mvar{a}×\mvar{b}$ contains (ordered) pairs, with one element from
$\mvar{a}$ and one from $\mvar{b}$:
\begin{equation*}
  \prftree[r]{}
    {Γ ⊢ \prop{\mvar{a}}}{Γ ⊢ \prop{\mvar{b}}}
    {Γ ⊢ \mvar{p}:\mvar{a}}{Γ ⊢ \mvar{q}:\mvar{b}}
    {Γ ⊢ (\mvar{p},\mvar{q}):\mvar{a}×\mvar{b}}.
\end{equation*}
The type $\mvar{a}+\mvar{b}$ is sometimes called ``option'' or ``either'': it
contains tagged elements of $\mvar{a}$ or $\mvar{b}$:
\begin{align*}
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{a}}
    {Γ ⊢ \apply{\inl}{\mvar{p}}:\mvar{a}+\mvar{b}},
  &&
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{b}}
    {Γ ⊢ \apply{\inr}{\mvar{p}}:\mvar{a}+\mvar{b}}.
\end{align*}

The elimination rules\index{Elimination rules!In \TLC{}}
act as one might expect, given the reuse of the notation from \cref{sec:ipl} for
the introduction rules:
\begin{gatherjot}
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{a}×\mvar{b}}
    {Γ ⊢ \appr{1}\mvar{p}:\mvar{a}}
  \qquad
  \prftree[r]{}
    {\prftree[r, noline]{}
      {Γ ⊢ \prop{\mvar{a}}}
      {Γ ⊢ \prop{\mvar{b}}}}
    {Γ ⊢ \mvar{p}:\mvar{a}×\mvar{b}}
    {Γ ⊢ \appr{2}\mvar{p}:\mvar{b}} \\
  \prftree[r]{}
    {\prftree[r, noline]{}
      {\prftree[r, noline]{}
        {Γ ⊢ \prop{\mvar{a}}}
        {Γ ⊢ \prop{\mvar{b}}}}
      {Γ ⊢ \prop{\mvar{c}}}}
    {\prftree[r, noline]{}
      {Γ ⊢ \mvar{p}:\mvar{a}→\mvar{c}}
      {Γ ⊢ \mvar{q}:\mvar{b}→\mvar{c}}}
    {Γ ⊢ \mvar{r}:\mvar{a}+\mvar{b}}
    {Γ ⊢ \apppply{\rec_{+}}{\mvar{p}}{\mvar{q}}{\mvar{r}}:\mvar{c}}
  \qquad
  \prftree[r]{}
    {Γ ⊢ \prop{\mvar{a}}}
    {Γ ⊢ \mvar{p}:⊥}
    {Γ ⊢ \apply{\rec_{⊥}}{\mvar{p}}:\mvar{a}}.
\end{gatherjot}
Just as with the formation, introduction, and elimination rules,
the computation and unicity rules are identical to those in \IPL{}
(modulo notational differences), and so are omitted here.

Examples of (somewhat) practical programs that can be written in an extension
of this theory abound in \cref{chap:type-theory}.

\subsection{Propositions and Types}
\label{sec:propositions-and-types}

What just happened? \Cref{sec:ipl} defined a logico-deductive system, capable of
expressing many familiar proofs of sentential logic (with some differences
stemming from its constructivist nature). \Cref{sec:the-lambda-calculus} took a
sharp left turn, describing a formalism for capturing a notion of algorithmic
computability. However, the system obtained by adding a few reasonable features
for describing more complicated data to \STLC{} looked not merely
analogous, but \textit{identical to} \IPL{} (again, modulo simple
notational changes, see \cref{tab:curry-howard}).

While perhaps less surprising due to the notational conveniences of hindsight,
this observation has startling and deep consequences (and a rich intellectual
history). It is known as the
Curry-Howard(-Lambek)\footnote{More on Lambek's part in \cref{rmk:lambek}}
correspondence, or the propositions-as-types interpretation. First investigated
by Curry \cite{curry-howard}, who noticed that the types of the ``combinators''
of his computational system\footnote{Equivalent to the λ-calculus, of course}
could be interpreted as valid propositions in \IPL{}. This
observation squared nicely with what's called the Brouwer-Heyting-Kolmogorov
interpretation of \IPL{}, which asserts that the semantics thereof
are to be interpreted as referring not exactly to propositions and their proofs,
but more general ``problems'' and their ``solutions'' \cite{kolmogorov}.

So what does this correspondence really say? It can be understood in many
ways.\footnote{Read in a particular light, the univalence principle
  \cref{sec:univalence} might suggest that this ``isomorphism'' be read
  as an \textit{identity}, an expression of the fundamental sameness of logic
  and computation.}
It says that \TLC{} can be used to provide semantics for programming
languages \cite{landin}. It says that proofs of \IPL{} can be
encoded into \TLC{}. This translation has the nice property that
type-checking (the process of determining whether or not has a given type in a
given context $Γ$) is decidable: it can be performed mechanically, preferably by
a computer. This gives such proofs a strong epistemological footing.

\begin{sidewaystable}
  \centering
  \begin{tabular}{l | l | l | l | l}
    \ZFC & \IPL & \UTT & Category theory & Homotopy theory \\ \hline
    Set
      & Proposition
      & Type
      & Object
      & Space \\
    Element
      & Proof
      & Term/program
      & ``Element'' ($⊤→X$)
      & Point \\
    Product ($A×B$)
      & Conjunction ($a∧b$)
      & Pair type (${a}×{b}$)
      & Product (${a}×{b}$)
      & Product space (${A}×{B}$) \\
    Disjoint union (${A}\amalg {B}$)
      & Disjunction (${a}∨{b}$)
      & Sum type ($a+b$)
      & Coprod.\ type ($A+B$)
      & Coprod.\ space ($A+B$) \\
    Empty set ($\emptyset$)
      & Falsehood
      & Empty type ($\emptytype$)
      & Initial object
      & Empty space \\
    Singleton set
      & Truth
      & Unit type ($\unittype$)
      & Final object
      & Singeton space \\
    Function ($f:A→B$)
      & Impl.\ ($p:a→b$)
      & Function ($f:A→B$)
      & Morphism ($f:A→B$)
      & Continuous function \\
    Equality ($A=B$)
      & Judg.\ eq.\ ($A\≡ B$)
      & Judg.\ eq.\ ($A\≡ B$)
      & Equality ($A=B$)
      & Equality ($A=B$) \\
    Path $f:[0,1]→X$
      &
      & Prop. eq.\ ($\propeq{A}{a}{b}$)
      & Equality ($A=B$)
      & Path \\
    {}
      & {}
      & Weak equiv.\ $\weq{A}{B}$
      & Isomorphism $A≅B$
      & Equivalence $A\simeq B$ \\
    Universal q.\ ($∀x.Px$)
      & {}
      & Π-type ($\prod_{x:A}\apply{P}{x}$)
      & {}
      & Section \\
    Existential q.\ ($∃x.Px$)
      & {}
      & Σ-type ($\sum_{x:A}\apply{P}{x}$)
      & {}
      & Total space
  \end{tabular}
  \caption{\label{tab:curry-howard}A table of metaphors, including but not
    limited to the Curry-Howard-Lambek-Voevodsky correspondence.}
\end{sidewaystable}

\chapter{Type theory}
\label{chap:type-theory}

Whatever the meaning of the Curry-Howard correspondence, its utility is
circumscribed by the fact that \IPL{}/\TLC{} as
presented do not match the expressive power of classical \FOL:
there are no types expressing quantification over some domain, nor propositions
or relations that may ``apply'' to terms and variables. Certainly, they are
missing the infinite structures of classical set theory.

The system described in this section enriches \TLC{} with
\textit{dependent types} (\cref{sec:dependent-types}) and a few more base types,
including our first examples of \textit{inductive types}
(\cref{subsec:natural-numbers-and-lists}). Additionally, we define and include
Voevodsky's univalence axiom (\cref{sec:univalence}). The
resulting system (univalent type theory, or \UTT{}) has similar
expressive power to \ZFC+\FOL.\footnote{Indeed, the
  ``set types'' (\cref{def:sets}) behave very much like sets in classical
  mathematics, especially when a few classically-minded axioms are assumed. See
  \cite{book} for more commentary.}

\UTT{} mirrors the type theory $\caH'$ of \cite{homotopy-limits}, which itself
is the system $\caH$ in \cite{inductive} and \cite{homotopy-initial} with the
addition of the natural numbers and a universe. The results of
\cref{chap:coinductive-types-in-univalent-type-theory} don't
require the full power of homotopy type theory (\HoTT), specifically we leave
out the axioms asserting the existence of higher inductive types.

\section{Dependent types}
\label{sec:dependent-types}

One of the slogans of dependently typed programming is that \textit{types are
terms}. Propositions, like proof terms, can now be subsumed as objects of study
in their own right. However, all terms must have a type. What, if anything, is
the type of types?

\begin{definition}\label[definition]{def:universes}
  All the base types we have considered up to this point ($⊤$, $⊥$) are members
  of a \define{universe} \universei{0}, formed via the following rule:
  \begin{equation*}
    \prftree[r]{}
      {}
      {Γ⊢\type{\universei{0}}}
  \end{equation*}
  The type formers $×$, $+$, and $→$
  preserve membership of this universe, i.e.\
  \begin{gatherjot}
    \prftree[r]{}{}{Γ⊢⊤:\universei{0}}
    \qquad
    \prftree[r]{}{}{Γ⊢⊥:\universei{0}}
    \qquad
    \prftree[r]{}
      {\prftree[r, noline]{}
        {Γ ⊢ \mvar{a}:\universei{0}}
        {Γ ⊢ \mvar{b}:\universei{0}}}
      {Γ⊢\mvar{a}×\mvar{b}:\universei{0}}
    \qquad
    \prftree[r]{}
      {\prftree[r, noline]{}
        {Γ ⊢ \mvar{a}:\universei{0}}
        {Γ ⊢ \mvar{b}:\universei{0}}}
      {Γ⊢\mvar{a}+\mvar{b}:\universei{0}}
  \end{gatherjot}
  Conversely, if something is a member of the universe, we may conclude that
  it is a type:
  \begin{equation*}
    \prftree[r]{}
      {Γ ⊢ \mvar{a}:\universei{0}}
      {Γ⊢\type{\mvar{a}}}
  \end{equation*}
\end{definition}

Dependent types blur (well, destroy) the distinction between what goes
on the left- and right-hand side of the typing judgment (``$:$'').
Additional commentary on the universe follows in \cref{rmk:universe-hierarchy}.

\subsection{Π-types}
\label{subsec:pi-types}

We now have the power to write an \textit{single} identity function that works
for \textit{all} types.

\begin{definition}\label[definition]{def:id-polymorphic}
  The identity function is
	\begin{align*}
    \id :\∏{A:\universe}{A\to A} &&
    \id \defeq \λ{A}{\λ{a}{a}}
  \end{align*}
  We will write its first argument as a subscript, as in $\id_{A}$.
\end{definition}


\begin{remark}\label[remark]{rmk:universe-hierarchy}

\end{remark}

\subsection{Σ-types}
\label{subsec:sigma-types}

\begin{lemma}[Paths in $Σ$-types]\label[lemma]{lemma:path-sigma}
	If $x,y:\∑{a:A}{B(a)}$, then
  \begin{equation*}
    \weq{
      (\propeq{}{x}{y})
    }{
      \∑{
        p:\propeq{}{\appr{1}{x}}{\appr{1}{y}}
      }{
        \propeq{}{
          \transport{}{p}{\appr{2}{x}}
        }{
          \appr{2}{y}
        }
      }
    }.
  \end{equation*}
  \TODO{cite with page number}
\end{lemma}

\subsection{Natural numbers and lists}
\label{subsec:natural-numbers-and-lists}

\subsubsection{On the law of the excluded middle}
\label{subsec:on-lem}

\TODO{introduce the topic based on previous sections}

Consider functions with the following types:
\begin{itemize}
  \itemsep0em
  \item $\ttfun{dne}:\∏{A:\universe}{((A\to⊥)\to⊥)\to A}$
  \item $\ttfun{lem}:\∏{A:\universe}{A+(A\to⊥)}$
\end{itemize}
What can we tell about these functions from their type signatures?
The term $\ttfun{dne}$ takes as argument a term $x:(A\to⊥)\to⊥$,
that is, a term showing that $A$ is not the empty type\footnote{More precisely,
  $x$ demonstrates that $A$ is not \textit{isomorphic} or \textit{equivalent} to
  $⊥$, in the sense of \cref{subsec:weak-equivalences}. If $A$ were isomorphic
  to $⊥$, then that isomorphism would be an inhabitant of $A\to⊥$.}, and
produces some element of $A$. This seems like a very tricky function to write:
how can we give a term of type $A$ just by knowing $A$ has terms? We don't
know what form data of type $A$ have! The function $\ttfun{lem}$ seems
similarly quagmired. Given a type $A$, this function either has to produce an
element of it, or demonstrate that it is uninhabited.

As you may have already guessed, under the propositions/types correspondence,
these functions correspond to the rule of double negation elimination
($\ttfun{dne}$)\index{Double negation elimination}
and the law of excluded middle ($\ttfun{lem}$)\index{Law of excluded middle}
\cref{def:lem-dne}.

To demonstrate with finality that these are not definable terms in any
consistent type theory, consider the application of $\ttfun{lem}$ to a type
$\ttfun{pnp}$ that corresponds to the $\textsc{P}\neq\textsc{NP}$ conjecture
under the types-as-propositions interpretation. If $\ttfun{lem}$ existed, we
could trivially solve this problem and make a million dollars! We need no
recourse to philosophy to justify the constructive/intuitionistic logic of proof
assistants: nothing else computes.

\section{The identity type}
\label{sec:the-identity-type}

\begin{tt-rule}\label[rule]{rule:id-elim}
  The rule of \define{identity elimination} (in the \HoTT
  community, known as \define{path induction}) is as follows:\TODO{define id-elim}
\end{tt-rule}

\begin{lemma}[$\ttfun{ap}$]\label[lemma]{lemma:ap}
	\TODO{define ap}
\end{lemma}

\begin{lemma}[Transport]\label[lemma]{lemma:transport}
	\TODO{define transport}
\end{lemma}

\begin{notation}\label[notation]{notation:transport}
  We will often curry\index{Curry} \transportname. If we have a family
  $P:A\to\universe$ and a path $p:\propeq{A}{x}{y}$, we write $\transpor{P}{p}$
  for $\λ{x}\transport{P}{p}{x}$.
\end{notation}

\begin{lemma}\label[lemma]{lemma:transport-compose}
	For $f:A\to B$, $P:B\to\universe$, and $p:\propeq{A}{x}{y}$,
  \begin{equation*}
    \propeq{}{
      \transpor{P∘ f}{p}
    }{
      \transpor{P}{\ap{f}{p}}
    }
  \end{equation*}
\end{lemma}


\section{h-levels and truncation}
\label{sec:h-levels and truncation}

\begin{definition}\label[definition]{def:mere-proposition}
  A type $X:\universe$ is a \define{mere proposition}, a \define{proposition},
  or an \define{hprop} if all of its elements are propositionally equal,
  \begin{equation*}
    \apply{\isProp}{X} \defeq \pit{x,y:X}x=y
  \end{equation*}
\end{definition}

\begin{definition}\label[definition]{def:contr}
  A type $X:\universe$ is \define{contractible} if it is a mere proposition and
  is inhabited, that is
  \begin{equation*}
    \apply{\isContr}{X} \defeq \sigmat{x:X}{\pit{y:X}x=y}
  \end{equation*}
  The first projection is the \define{center} of $X$.
\end{definition}

\begin{definition}\label[definition]{def:sets}

\end{definition}

\begin{figure}[ht]
  \centering
  \caption{\label{fig:awodey} Awodey's two axes: size (universe level)
    and complexity (h-level).}
\end{figure}

\section{Univalence}
\label{sec:univalence}

\subsection{Weak equivalences}
\label{subsec:weak-equivalences}

\begin{notation}\label[notation]{notation:weq-coerce}
  We may treat a weak equivalence as if it were a function and apply it to an
  argument. This amounts to implicitly appling $\pr{1}$.
\end{notation}

\begin{lemma}\label[lemma]{lemma:contr-weq-unit}
	Any contractible type is weakly equivlant to $\unittype$.
\end{lemma}
\begin{proof}
	Suppose $X:\universe$ is contractible with center $x$. Define $f:X→\unittype$
  as $\λ{x}{\unitelem}$ and $g:\unittype→X$ as $\λ{u}{x}$. Then by hypothesis on
  $X$, the induction principle for $\unittype$\TODO{reference}, and function
  extensionality (\cref{thm:funext}), the composites are the appropriate
  identities.
\end{proof}


\subsection{The univalence axiom}
\label{subsec:the-univalence-axiom}

\begin{definition}\label[definition]{def:ua}
  \TODO{definintion}
  It has the following computation rule:

  Note that this rule holds up to \textit{propositional equality}. This is
  because in \UTT{}, univalence is indeed an \textit{axiom}. It has
  no computational meaning. \TODO{mention CTT}
\end{definition}

\begin{theorem}[Function extensionality]\label[theorem]{thm:funext}
	Under the hypothesis of the univalence axiom, function extensionality holds,
  i.e.\ there is a term
  \begin{equation*}\label{eq:funext}
    \funext:\∏{A,B\:\universe}{(\homot{f}{g})\to \propeq{}{f}{g}}
  \end{equation*}
\end{theorem}

\chapter{Category theory}
\label{chap:category-theory}

Category theory presents a challange to set-theoretic mathematicians: the
canonical example of a category is $\Set$, the collection of all sets. In
\ZFC+\FOL, this category is undefinable\footnote{In
  particular, its formation is prevented by the axiom of regularity
  \cite{vonneumann}, which was included in \ZFC to avoid the
  paradoxes of Burali-Forti and Russell. The discovery of said paradoxes
  motivated Bertrand Russell to invent something he called the ``theory of
  types'' \cite{russell}.}.
\Crefrange{sec:basics}{sec:functors-and-their-algebras},
are willfully imprecise, we work with an abstract and undefined notion of
``collection'', and adopt set-theoretic notation. See any textbook on category
theory for information on how problems of size are dealt with set-theoretically.
We will examine category theory within \UTT{} in
\cref{sec:type-theoretic-category-theory}.

\section{Basics}
\label{sec:basics}


\TODO{Type theory and category theory are intimately connected via the
      discipline of \textit{topos theory}}%\index{Topos theory}.}

\begin{definition}
	A \define{category}\index{Category} $\bfC$ consists of the following data:
  \begin{itemize}
    \itemsep-0.2em
    \item a collection of \define{objects}, denoted $\Obj \bfC$,
    \item for each pair of objects $A,B\in\Obj \bfC$, a collection of
      \define{arrows} (or \define{morphisms}) between them, denoted
        $\Hom_{\bfC}(A,B)$,
    \item for each object $A\in\Obj \bfC$, a distinguished arrow
      $\id_A\in\Hom_{\bfC}(A,A)$ called the
      \define{identity}\index{Identity!Morphism}, and
    \item for each triple of objects $A,B,C\in\Obj\bfC$, an operation \\
      $∘:\Hom_{\bfC}(B,C)\times\Hom_{\bfC}(A,B)\to\Hom_{\bfC}(A,C)$ called
      \define{composition}\index{Composition!In a category}.
  \end{itemize}
  These data are subject to the following axioms:
  \begin{enumerate}%[label=\Alph*.]
    \itemsep-0.2em
    \item composition is associative, and
    \item the identity acts as a unit for composition.
  \end{enumerate}
  When the category in question is clear from context, one writes $f:A\to B$ for
  $f\in\Hom_{\bfC}(A,B)$.
\end{definition}

\begin{definition}\label[definition]{def:domain-and-codomain}
  If $f\in\Hom_{\bfC}(A,B)$, then $A$ is the \define{domain}\index{Domain} or
  \define{source}\index{Source} of $f$ and $B$ is the
  \define{codomain}\index{Codomain} or \define{target}\index{Target} of $f$.
\end{definition}

\begin{example}
  A student of mathematics will be familiar with the following categories:
  \begin{itemize}
    \itemsep-0.2em
    \item $\Set$: The category with sets as objects, functions as morphisms,
      the usual composition of functions, and identity functions.
    \item $\Grp$: The category of groups\index{Group} with group homomorphisms
      as morphisms. Note that the identity function of sets is the required
      identity morphism and that for any homomorphsisms $ϕ:G\to H$ and
      $ψ:H\to I$, the usual composition of functions defines a homomorphism
      $ψ∘ ϕ:G\to I$.
    \item $\AbGrp$: The category of abelian groups (this can be considered a
      \define{subcategory} of $\Grp$).
    \item $F\dVect$: The category of vector spaces\index{Vector space} over a
      field $F$ with linear transformations as morphisms.
    \item For any group $(G,∘,e)$, there is a corresponding category
      $\underline{G}$ with a single object (denoted $∗$) and
      $\Hom_{\underline{G}}(∗,∗)\coloneqq G$. Composition is given by the
      group operation.
    \item For any set or type $A$, the \define{discrete category on $A$} has as
      objects the members or elements as $A$, and no other arrows except the
      requisite identities.
    \item The category $\ω$ has as objects the natural numbers, and arrows
      $n→\apply{\suc}{n}$ (plus the requisite composites $n→m$ where $n<m$).
  \end{itemize}
\end{example}

\begin{definition}\label[definition]{def:isomorphism}
	An \define{isomorphism}\index{Isomorphism!In a category} in $\bfC$ is an arrow
  $f\in\Hom_{\bfC}(A,B)$ such that there exists an arrow $g\in\Hom_{\bfC}(B,A)$
  such that $g∘ f=\id_A$ and $f∘ g=\id_B$. We call such a $g$ the
  \define{inverse}\index{Inverse!In a category} of $f$.
\end{definition}

Just as with functions, inverses (should they exist) are unique.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[scale=2]
    \node (A) [] {$∗$};

    \foreach \x in {1,2,3}
      \draw (A) edge [in={50*\x},out={40+50*\x},loop] node[above] {$g_\x$} (A);

    \draw (A) edge [in=200,out=240,loop] node[below] {$\cdots$} (A);
    \draw (A) edge [in=-60,out=-20,loop] node[below] {$e$} (A);
  \end{tikzpicture}
  \caption{
    \label{fig:grp} A schematic of the category $\underline{G}$ for a
    group $G$ with identity $e$ and elements $g_1,g_2,\ldots$. Compositions and
    inverses not shown.
  }
\end{figure}

\begin{definition}\label[definition]{def:commutative-diagram}
  A commutative diagram is a way to visualize equations between arrows
  involving composition. Technically a diagram in $\bfC$ is a directed graph
  with vertices labeled by $\Obj\bfC$. If $e$ is an edge from $A$ to $B$, then it
  is labeled by an arrow in $\Hom_\bfC(A,B)$. A diagram \define{commutes}, or is
  commutative, if the composition of the arrows labeling the edges of any two
  directed paths with the same endpoints are equal.
  \TODO{better wording}
\end{definition}

\begin{example}\label[example]{ex:commutative-diagram}
	If $f\in\Hom_{\bfC}(A,B)$, $g\in\Hom_{\bfC}(B,C)$, $h\in\Hom_{\bfC}(A,C)$,
  and $g∘ f=h$, then we may draw the following
  \define{commutative triangle}:
  \begin{center}
    \begin{tikzcd}[column sep=large]
      A \arrow[r, "f"] \arrow[dr, "h"] & B \arrow[d, "g"] \\
      {}                               & C
    \end{tikzcd}
  \end{center}
	If $f\in\Hom_{\bfC}(A,B)$, $g\in\Hom_{\bfC}(B,D)$, $h\in\Hom_{\bfC}(A,C)$,
  $i\in\Hom_{\bfC}(C,D)$, and $g∘ f=i∘ h$, then we may draw the
  following \define{commutative square}:
  \begin{center}
    \begin{tikzcd}[column sep=large]
      A \arrow[r, "f"]  \arrow[d, "h"] & B \arrow[d, "g"] \\
      C \arrow[r, "i"]                 & D
    \end{tikzcd}
  \end{center}
\end{example}

\section{Duality}
\label{sec:duality}

From now on, dual concepts and statements will be introduced in pairs, and
typeset like so:

\dual{
  Concept
}{
  Co-concept
}

\section{Limits and colimits}
\label{sec:limits-and-colimits}

For this section, fix an arbitrary category $\bfC$.\TODO{elsewhere?}

\begin{definition}\label[definition]{def:terminal-and-initial}
  \
  \vspace{-0.3em}\dual{
    A \define{terminal object}\index{Terminal object} is an
    object $⊤\in\Obj\bfC$ such that for all $A\in\Obj\bfC$,
    there is exactly one arrow $f:A\to⊤$.
  }{
    An \define{initial object}\index{Initial object} is an
    object $⊥\in\Obj\bfC$ such that for all $A\in\Obj\bfC$,
    there is exactly one arrow $f:⊥\to A$.
  }
\end{definition}

\begin{example} \
  \begin{itemize}
    \itemsep-0.2em
    \item $\Set$: 
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item \vspace{-0.6em} Any singleton set $\braces{∗}$ is terminal.
        \item The empty set $\emptyset$ is initial.\footnote{This is a little
            odd: in set-theoretic foundations, functions are actually defined
            as certain kinds of relations (``functional'' ones). Accordingly,
            the unique function out of them empty set is the unique subset
            of $\emptyset×X=\emptyset$, namely $\emptyset$ itself.}
      \end{itemize}
    \item $\universe$: 
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item{} \vspace{-0.6em}
          [\unimathname{CategoryTheory.categories.Types.TerminalType}]
          Any contractible type is terminal (for the
          similarity to the case of sets, note that any contractible type is
          equivalent to the canonical type with one element, $\unittype$
          (\cref{lemma:contr-weq-unit})).
        \item{} [\unimathname{CategoryTheory.categories.Types.InitialType}]
          The elimination rule for the empty type gives an arrow
          $\emptytype→X$ for any $X$. How do we know it's unique? Apply function
          extensionality (\cref{thm:funext}), then it suffices to show that it
          has the same output as any competitor $f:\emptytype\to X$ on some
          $e:\emptytype$. But using the elimination rule for $\emptytype$ on
          $e$, we can conclude that it does.
      \end{itemize}
    \item $\Cat$: 
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item \vspace{-0.6em} The category with one object and one arrow (its
          identity) is terminal.
        \item The empty category is initial, for the same reasons as in $\Set$
          or $\universe$ (depending on which foundational system one is working
          in).
      \end{itemize}
    \item $\Grp$, $\AbGrp$: The group with one element (``trivial group'') is
      initial \textit{and} terminal. Such an object is, in general, called a
      \define{zero object}.
    \item $F\dVect$: Similarly, the trivial vector space is a zero object (its
      underlying additive abelian group of vectors is the trivial group!)
  \end{itemize}
\end{example}

\begin{remark}\label[remark]{remark:terminal-object-id-unique}
	In particular, for a terminal object $⊤$, $\id_⊤$ is the only arrow
  $⊤\to⊤$.\footnote{As noted in \cref{sec:duality}, this statement
    holds for initial objects as well. From this point on, we will leave it
    to the reader to construct the dual of a statement and infer its truth.}
\end{remark}

\begin{lemma}\label[lemma]{lemma:terminal-unique}
  Terminal objects are unique up to a specified isomorphism.
\end{lemma}
\begin{proof}
	Suppose $A$ and $B$ are terminal objects.
  There are unique arrows $f:A\to B$ and $g:B\to A$.
  Then $g∘ f:A\to A$ and $f∘ g:B\to B$, but as per
  \cref{remark:terminal-object-id-unique}\TODO{how to make this ``remark''?},
  $g∘ f=\id_A$ and $f∘ g=\id_B$.
\end{proof}

\begin{definition}\label[definition]{def:product-and-coproduct}
  Given two objects $A,B\in\Obj\bfC$,
  \vspace{-0.3em}\dual{
    a \define{product}\index{Product!In a category} of $A$ and $B$
    consists of an object $C\in\Obj\bfC$ together with arrows $p_1:C\to A$ and
    $p_2:C\to B$ satisfying the following universal property:

    For any other ``candidate product'' $D\in\Obj\bfC$ with arrows
    $q_1:D\to A$ and $q_2:D\to B$, there is a unique arrow $u:D\to C$ making the
    following diagram commute:
    \begin{center}
      \begin{tikzcd}[sep=large,ampersand replacement=\&]
        {} \& D\arrow[dl,swap,"q_1"]\arrow[dr,"q_2"]
              \arrow[d,dashed,"u"] \& {} \\
        A \& C \arrow[l, "p_1"]\arrow[r,swap, "p_2"] \& B
      \end{tikzcd}
    \end{center}
    If $C$ is a product of $A$ and $B$, we will denote it by $A\times B$, and
    the unque arrow $u$ as $\langle f,g \rangle$.
  }{
    a \define{coproduct}\index{Coproduct!In a category} of $A$ and $B$
    consists of an object $C\in\Obj\bfC$ together with arrows $i_1:A\to C$ and
    $i_2:B\to C$ satisfying the following universal property:

    For any other ``candidate coproduct'' $D\in\Obj\bfC$ with arrows
    $j_1:A\to D$ and $j_2:B\to D$, there is a unique arrow $u:C\to D$ making the
    following diagram commute:
    \begin{center}
      \begin{tikzcd}[sep=large,ampersand replacement=\&]
        A \arrow[r, "i_1"{name=I1}]\arrow[dr, swap, "j_1"{name=F}]
        \& C \arrow[d,dashed,"u"]
        \& B\arrow[l, "i_2"{name=I2},swap]\arrow[dl,"j_2"{name=G}] \\
        {} \& D \& {}
      \end{tikzcd}
    \end{center}
    If $C$ is a coproduct of $A$ and $B$, we will denote it by $A+B$, and
    the unque arrow $u$ as $[f,g]$.
  }
\end{definition}

\begin{remark}\label[remark]{rmk:prod-coprod-universal-adj}
	The universal properties of the product and coproduct define a bijections
  (there is one and only one function that fills in the diagram)
  \begin{align*}
    \Hom(A,C)×\Hom(B,C)&≅ \Hom(A+B,C)\text{ and} \\
    \Hom(C,A)×\Hom(C,B)&≅ \Hom(C,A×B).
  \end{align*}
  With one additional criterion (called ``naturality''), this can be taken as an
  alternative definition.
\end{remark}

\begin{example} \
  \begin{itemize}
    \itemsep-0.2em
    \item $\Set$: Let $A$ and $B$ be sets.
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item \vspace{-0.6em} Their Cartesian product
          $A×B\coloneqq\braces{(a,b):a∈A,b∈B}$ together with projections
          $\apply{p_1}{(a,b)}\coloneqq a$ and $\apply{p_2}{(a,b)}\coloneqq b$
          is a product.
        \item Their disjoint union
          $A\amalg B\coloneqq\braces{(0,a):a∈A}∪\braces{(1,b):b∈B}$ together
          with inclusions $\apply{i_1}{a}\coloneqq (0,a)$
          and $\apply{i_2}{b}\coloneqq (1,b)$ is a coproduct.\footnote{Of
            course, the identities of the ``labels'' $0$ and $1$ are
            unimportant, so long as they are distinct. One could equally well
            use ``$\inl$'' and ``$\inr$'' so long as $\inl≠\inr$.}
      \end{itemize}
    \item $\universe$: Let $A,B:\universe$.
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item{} \vspace{-0.6em}
          [\unimathname{CategoryTheory.categories.Types.BinProductsType}]
          The product type $A×B$ with its projections is a product.
          The universal property is guaranteed by $\rec_{A×B}$, and can be
          expressed concisely as an equivalence $\weq{(C→A)×(C→B)}{C→A×B}$
          (see \cref{rmk:prod-coprod-universal-adj}).
        \item{} [\unimathname{CategoryTheory.categories.Types.BinCoproductsType}]
          The coproduct type $A+B$ with its injections is a coproduct.
          The universal property is guaranteed by $\rec_{A+B}$, and can be 
          expressed concisely as an equivalence $\weq{(A→C)×(B→C)}{A+B→C}$.
      \end{itemize}
    \item $\Cat$: 
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item \vspace{-0.6em} See \cref{ex:diagonal}
        \item The coproduct of categories $\bfA$ and $\bfB$ is the category
          $\bfA+\bfB$ with objects
          $\Obj (\bfA+\bfB) \coloneqq (\Obj\bfA)+(\Obj\bfB)$
          and arrows
          \begin{align*}
            \Hom_{\bfA+\bfB}(\apply{\inl}{A}, \apply{\inl}{A'}) &= \Hom_{\bfA}(A,A') \\
            \Hom_{\bfA+\bfB}(\apply{\inr}{B}, \apply{\inr}{B'}) &= \Hom_{\bfB}(B,B') \\
            \Hom_{\bfA+\bfB}(\apply{\inl}{A}, \apply{\inr}{B}) &= \emptytype \\
            \Hom_{\bfA+\bfB}(\apply{\inr}{A}, \apply{\inl}{B}) &= \emptytype.
          \end{align*}
          (In set-theoretic foundations, one merely replaces $\apply{\inl}{A}$,
          $\apply{\inr}{B}$, and $\emptytype$ with $(0,A)$, $(1,B)$, and
          $\emptyset$ respectively.)
      \end{itemize}
    \item $\Grp$: Let $G,H$ be groups.
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item \vspace{-0.6em} The direct product group $G×H$ is a product.
        \item The free product $G∗H$ is a coproduct.
      \end{itemize}
    \item $\AbGrp$, $R\Mod$, $F\dVect$: The direct sum $⊕$ is both a product and
      coproduct.\footnote{In fact, has a stronger property of being a
      \define{biproduct}. These three categories share many significant
      features; one fruitful lens through which to view them is as
      \define{abelian categories}.} 
  \end{itemize}
\end{example}

\begin{lemma}
	(Co)products are unique up to a specified isomorphism.
\end{lemma}
\begin{proof}
  Note that for any coproduct $C$ of $A$ and $B$, the identity makes the
  following diagram ($⋆$) commute, and so is necessarily unique in doing so:
  \begin{center}
    \begin{tikzcd}[sep=large,ampersand replacement=\&]
      A \arrow[r, "i_1"{name=I1}]\arrow[dr, swap, "i_1"{name=F}]
      \& C \arrow[d,"\id_C"]
      \& B\arrow[l, "i_2"{name=I2},swap]\arrow[dl,"i_2"{name=G}] \\
      {} \& C \& {}
    \end{tikzcd}
  \end{center}
	Suppose $C$ and $D$ are coproducts of $A$ and $B$. The universal
  property of the coproduct induces arrows $u$ and $u'$ making the following
  diagram commute:
  \begin{center}
    \begin{tikzcd}[column sep=large,ampersand replacement=\&]
      {} \& C \arrow[dd, dashed, "u", bend left] \& {} \\
      A \arrow[ur, "i_1"{name=I1}]\arrow[dr, swap, "i_1"{name=F}]
      \& {}
      \& B\arrow[ul, "i_2"{name=I2},swap]\arrow[dl,"i_2"{name=G}] \\
      {} \& D \arrow[uu, dashed, "u'", bend left] \& {}
    \end{tikzcd}
  \end{center}
  However by this commutativity, the compositions $u∘u'$ and $u'∘u$ fit into the
  diagram ($⋆$), and so must be the respective identities, making $C$ and $D$
  isomorphic.
\end{proof}

\begin{definition}\label[definition]{def:equalizer-and-coequalizer}
  Given objects $A,B∈\Obj\bfC$ and arrows $f,g:A→B$,
  \vspace{-0.3em}\dual{
    an \define{equalizer}\index{Equalizer} of $f$ and $g$
    consists of an object $E$ and an arrow $e:E→A$ satisfying the following
    universal property:

    For any other ``candidate equalizer'' $C$ with arrow $c:C→A$, there is a
    unique arrow $u:C→E$ making the following diagram commute:
    \begin{center}
      \begin{tikzcd}[column sep=large, ampersand replacement=\&]
        E \arrow[r, "e"]
        \& A \arrow[r, yshift=2.5,"f"]\arrow[r, yshift=-2.5, swap,"g"]
        \& B \\
        C \arrow[ur, swap, "c"]
          \arrow[u, dashed, "u"] \& {} \& {}
      \end{tikzcd}
    \end{center}
  }{
    a \define{coequalizer}\index{Coequalizer} of $f,g$
    consists of an object $Q$ and an arrow $q:B→Q$ satisfying the following
    universal property:

    For any other ``candidate coequalizer'' $C$ with arrow $c:B→C$, there is a
    unique arrow $u:Q→C$ making the following diagram commute:
    \begin{center}
      \begin{tikzcd}[column sep=large, ampersand replacement=\&]
        A \arrow[r, yshift=2.5,"f"]\arrow[r, yshift=-2.5, swap,"g"]
        \& B \arrow[r, "q"] \arrow[dr, "c"]
        \& Q \arrow[d, dashed, "u"] \\
        {} \& {} \& 
        C
      \end{tikzcd}
    \end{center}
  }
\end{definition}

\begin{example}\label[example]{ex:equalizer-and-coequalizer}
  \
  \begin{itemize}
    \itemsep-0.2em
    \item $\Set$: Let $A,B$ be sets and $f,g:A→B$.
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item \vspace{-0.6em} The equalizer of $f$ and $g$
          is the set $E\coloneqq\braces{a∈A:\apply{f}{a}=\apply{g}{a}}$.
          The map $E→A$ is the standard inclusion of a subset (which sends each
          element of $E$ to itself in $A$).
        \item The functions $f$ and $g$ induce an equivalence relation $∼$ on
          $B$ which is the closure of $\apply{f}{b}∼\apply{g}{b}$ for all $b∈B$.
          The quotient set $B/∼$ together with the projection $p:B→B/∼$ which
          sends each element of $b$ to its equivalence class is the coequalizer
          of $f$ and $g$.
      \end{itemize}
    \item $\universe$: 
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item{} \vspace{-0.6em} \TODO{}
        \item{} \TODO{}
      \end{itemize}
    \item $F\dVect$: A coequalizer for a linear map $f:A→B$ is the quotient
      space $B/\im f$.
  \end{itemize}
\end{example}

\begin{definition}\label[definition]{def:pullback-and-pushout}
  \
  \vspace{-0.3em}\dual{
    Given objects $A,B,C∈\Obj\bfC$ with arrows $f:A→C$ and $g:B→C$,
    a \define{pullback}\index{Pullback} of $f$ and $g$
    consists of an object $P$ together with arrows $p_1:P→A$ and $p_2:P→B$
    satisfying the following universal property:

    For any other ``candidate pullback'' $D$ with arrows $q_1:D→A$ and
    $q_2:D→B$, there is a unique arrow $u:D→P$ making the following
    diagram commute:
    \begin{center}
      \begin{tikzcd}[sep=large, ampersand replacement=\&]
        D \arrow[dr, dashed, "u"]
          \arrow[ddr, "q_1", swap, bend right=20]
          \arrow[drr, "q_2", bend left=20] \& {} \& {} \\
        {}
        \& P \arrow[r, "p_2"] \arrow[d, swap, "p_1"]
        \& B \arrow[d, "g"] \\
        {} \& A \arrow[r, swap, "f"] \& C
      \end{tikzcd}
    \end{center}
    The object $P$ is denoted $A×_C B$, or better 
    $A×_{\langle f,g\rangle}B$.\footnote{The notation $A×_C B$
      draws an important connection to the product, but omits some the
      key information: \textit{which} arrows are being pulled back matters, 
      and there might be many choices for the same objects $A$, $B$, and $C$.}
  }{
    Given objects $A,B,C∈\Obj\bfC$ with arrows $f:C→A$ and $g:C→B$,
    a \define{pushout}\index{Pushout} of $f$ and $g$
    consists of an object $P$ with arrows $i_1:A→P$ and $i_2:B→P$ satisfying the
    following universal property:

    For any other ``candidate pushout'' $D$, there is a unique arrow $u:P→D$
    making the following diagram commute:
    \begin{center}
      \begin{tikzcd}[sep=large, ampersand replacement=\&]
        C\arrow[r, "g"] \arrow[d, "f", swap]
        \& B \arrow[d, "i_2"] \arrow[ddr, "j_2", bend left=20] \& {} \\
        A \arrow[r, swap, "i_1"]
          \arrow[drr, "j_1", swap, bend right=20]
        \& D \arrow[dr, dashed, "u"] \& {} \\
        {} \& {} \& P
      \end{tikzcd}
    \end{center}
    The object $P$ is denoted $A+_C B$, or better $A+_{[f,g]}B$.
  }
\end{definition}

\begin{example}\label[example]{ex:pullback-and-pushout}
  \
  \begin{itemize}
    \itemsep-0.2em
    \item $\Set$: 
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item \vspace{-0.6em}
          Let $A,B,C$ be sets $f:A→C$, and $g:B→C$.
          The pullback of $f$ and $g$ is the set
          $P\coloneqq$; the maps $P→A$ TODO
        \item 
          Let $A,B,C$ be sets $f:C→A$, and $g:C→B$.
          The pushout of $f$ and $g$ is the disjoint union quotiented by the
          smallest equivalence relation containing 
          $(0,\apply{f}{c})∼(1,\apply{g}{c})$; the maps $p_1$ and $p_2$ are the
          projections to equivalence classes.
      \end{itemize}
    \item $\universe$: 
      \begin{itemize}\renewcommand{\labelitemi}{$∘$}
        \itemsep-0.2em
        \item{} \vspace{-0.6em} \TODO{}
        \item{} \TODO{}
      \end{itemize}
  \end{itemize}
\end{example}
\begin{example}\label[example]{ex:equalizer-and-coequalizer}
\end{example}

\begin{example}\label[example]{ex:pullback-special-cases-top}
	The pullback and pushout are complex; it is less immediately clear where they
  arise in standard mathematics than, say, the product and coproduct.
  Two special cases of the pullback in $\Set$ help motivate these constructions.
  TODO
\end{example}

\begin{example}\label[example]{ex:pullback-special-cases-top}
  In $\Top$, the wedge sum $A∨B$ is an instance of a pushout.
  Suppose $a∈A$ and $b∈B$ are the basepoints to be ``glued''.
  Define (necessarily continuous) functions
  \begin{align*}
    \begin{split}
      f:\braces{∗}&\longrightarrow A \\
      ∗&\longmapsto a
    \end{split}
    \begin{split}
      g:\braces{∗}&\longrightarrow B \\
      ∗&\longmapsto b
    \end{split}
  \end{align*}
  The equivalence relation $∼$ only relates $\apply{f}{∗}$ and 
  $\apply{g}{∗}$, that is $a∼b$. Thus,
  \begin{equation*}
    A+_{[f,g]}B 
    = A+B/(\apply{f}{∗}∼\apply{g}{∗})
    = A+B/(a∼b)
    = A∨B
  \end{equation*}
\end{example}

\Cref{def:terminal-and-initial}, \cref{def:product-and-coproduct},
\cref{equalizer-and-coequalizer}, and \cref{def:pullback-and-pushout}
are all instances of a general construction of \textit{limits} and
\textit{colimits}. Some preliminary constructions are necessary before these
definitions; these will help make sense of the terms ``candidate \_'' and
``candidate co\_'' used above.

These concepts can be defined in greater generality using
functors (\cref{def:functor}); here we consider only limits over graphs.

\begin{definition}\label[definition]{def:graph-diagram}
	A \define{graph}\index{Graph} $G$ consists of a (not necessarily finite)
  collection $G_0$ of vertices, and a map $G_1:G_0×G_0→\universe$ which assigns
  to each ordered pair of vertices a collection of directed edges between
  them.\footnote{This definition may seem obtuse to a classically-trained
    mathematician, but it translates more elegantly into the type-theoretic
    setting.}
\end{definition}

\begin{definition}
  A \define{diagram}\index{Diagram} $D$ for a graph $G$ is a collection
  of objects and arrows ``of shape $G$''. Specifically, $D$ consists of a map
  $D_0:G_0→\Obj\bfC$ and for each pair $(u,v)$ of vertices in $G_0$,
  a map $D_{1}^{(u,v)}:G_1(u,v)→\Hom(\apply{D_0}{u},\apply{D}_0{v})$.
  When convenient, we drop the sub- and super-scripts.
\end{definition}

\begin{definition}\label[definition]{def:cone-cocone}
  Let $D$ be a diagram in $\bfC$ for a graph $G$.
  \vspace{-0.3em}\dual{
    A \define{cone}\index{Cone} for $D$ consists of an object $C\in\Obj\bfC$
    together with \define{projections}
    $\braces{p_{v}:C→\apply{D}{v}}_{v∈G_0}$ such that the following triangle
    commutes for all $u,v∈G_0$ and $e∈G_1(u,v)$:
    \begin{center}
      \begin{tikzcd}[row sep=large, column sep=small, ampersand replacement=\&]
        {} \& C \arrow[dl, bend right=20, swap, "p_u"] \arrow[dr, bend left=20, "p_v"] \& {} \\
        \apply{D}{u}\arrow[rr, "\apply{D}{e}"] \& {} \& \apply{D}{v}
      \end{tikzcd}
    \end{center}
  }{
    A \define{cocone}\index{Cocone} for $D$ consists of an object $C\in\Obj\bfC$
    together with \define{injections} $\braces{i_{v}:\apply{D}{v}→C}_{v∈G_0}$
    such that the following triangle commutes for all $u,v∈G_0$ and
    $e∈G_1(u,v)$:
    \begin{center}
      \begin{tikzcd}[row sep=large, column sep=small, ampersand replacement=\&]
        \apply{D}{u}\arrow[dr, bend right=20, swap, "i_u"] \arrow[rr, "\apply{D}{e}"]\& {} \&
        \apply{D}{v}\arrow[dl, bend left=20, "i_v"] \\
        {} \& C \& {}
      \end{tikzcd}
    \end{center}
  }
\end{definition}

\begin{example}
  A cone for a diagram of shape $·\;·$
  picks out a ``candidate product'' in $\bfC$. Similarly, we can draw the
  following correspondences:
  \begin{itemize}\renewcommand{\labelitemi}{$∘$}
    \itemsep-0.2em
    \item \vspace{-0.3em} Terminal/Initial: [empty]
    \item (Co)product: $·\;·$
    \item (Co)equalizer:
      \begin{tikzcd}[cramped, sep=small]
        · \arrow[r, yshift=2.5]\arrow[r, yshift=-2.5] & · 
      \end{tikzcd}
    \item Pullback/Pushout:
      \begin{tikzcd}[cramped, sep=small] 
        · & · \arrow[l]\arrow[r] & · 
      \end{tikzcd}
  \end{itemize}
\end{example}

\begin{definition}\label[definition]{def:limit-and-colimit}
  Given a diagram $D$ for a graph $G$ in $\bfC$,
  \vspace{-0.3em}\dual{
    a \define{limit}\index{Limit} of $D$ consists of a cone $(L,\braces{p_v})$
    satisfying the following universal property:

    For any other cone $(C,\braces{q_v})$, there is a unique arrow $u:C→L$ making
    the following diagram commute for all $u,v∈G_0$ and $e∈G_1(u,v)$:
    \begin{center}
      \begin{tikzcd}[row sep=large, column sep=small, ampersand replacement=\&]
        {} \& C \arrow[ddl, bend right=25, swap, "q_u"]
                \arrow[ddr, bend left=25, "q_v"]
                \arrow[d, dashed, "u"]
           \& {} \\
        {} \& L
                \arrow[dl, bend right=15, "p_u"]
                \arrow[dr, bend left=15, swap, "p_v"]
           \& {} \\
        \apply{D}{u}\arrow[rr, swap, "\apply{D}{e}"] \& {} \& \apply{D}{v}
      \end{tikzcd}
    \end{center}
  }{
    a \define{colimit}\index{Colimit} of $D$
    consists of a cocone $(L,\braces{i_v})$ satisfying the following universal
    property:

    For any other cone $(C,\braces{j_v})$, there is a unique arrow
    $u:L→C$ making the following diagram commute:
    \begin{center}
      \begin{tikzcd}[row sep=large, column sep=small, ampersand replacement=\&]
        \apply{D}{u}
        \arrow[rr, "\apply{D}{e}"]
        \arrow[dr, bend right=15, "j_u"]
        \arrow[ddr, bend right=25, swap, "i_u"]
        \& {} \&
        \apply{D}{v}
        \arrow[dl, swap, bend left=15, "j_v"]
        \arrow[ddl, bend left=25, "i_v"] \\
        {} \& L \arrow[d, dashed, "u"] \& {} \\
        {} \& C \& {}
      \end{tikzcd}
    \end{center}
  }
\end{definition}

\begin{definition}\label[definition]{def:complete-and-cocomplete}
  A category $\bfC$ \define{has (co)limits of shape $G$} any diagram of shape
  $G$ has a (co)limit in $\bfC$. A category is \define{(co)complete} or
  \define{has small (co)limits} when it has (co)limits of every shape.
\end{definition}

\section{CCCs and the λ-calculus}
\label{sec:cccs-and-lc}

This section isn't necessary for understanding later sections and chapters, but
provides an interesting categorical framework for understanding the λ-calculus.
\TODO{subsection}

\begin{lemma}[\unimathname{CategoryTheory.categories.Types.ExponentialsType}]
	Any fixed universe $\universe$ of types has exponentials.
\end{lemma}
\begin{proof}
	
\end{proof}

\begin{definition}
	A \define{monoidal category} consists of a category $\bfC$ together with
  \begin{itemize}
    \itemsep-0.2em
    \item a bifunctor $\bfC×\bfC→\bfC$ called the \define{tensor product} or
      \define{monoidal product},
    \item an object $I∈\Obj\bfC$ called the \define{unit},
    \item for all objects $A,B,C∈\Obj\bfC$, an isomorphism
      $α_{A,B,C}:(A⊗B)⊗C≅A⊗(B⊗C)$, and
    \item for all objects $A∈\Obj\bfC$, isomorphisms $λ_A:I⊗A≅A$ and $ρ_A:A⊗I≅A$.
  \end{itemize}
  These isomorphisms are subject to additional ``coherence conditions'', which
  won't play a crucial role here. A monoidal category is
  \begin{itemize}
    \itemsep-0.2em
    \item \define{symmetric} if there are additional isomorphisms
      $s_{A,B}:A⊗B→B⊗A$ satisfying other coherence conditions; and is
    \item \define{cartesian} if the monoidal product $⊗$ coincides with the
      categorical product $×$.
  \end{itemize}
\end{definition}

Only cartesian monoidal categories play a crucial role later in this section.

\begin{remark}
	For any categorical product there is a specified isomorphism $A×B≅B×A$, so any
  cartesian monoidal category is also symmetric monoidal.
\end{remark}

\begin{example}
  \
  \begin{itemize}
    \itemsep-0.2em
    \item \vspace{-0.3em} $\Set$, $\universe$, $\Cat$, $\Grp$, and others are 
      cartesian (so necessarily symmetric) monoidal under their categorical
      products. 
    \item $F\dVect$ is symmetric monoidal under $⊗$; the trivial vector space is
      a unit.
  \end{itemize}
\end{example}

\begin{definition}\label[definition]{def:exponentials}
  An \define{internal hom} for a monoidal category $(\bfC,⊗)$
  consists of, for each pair of objects $A,B∈\Obj\bfC$, an object
  $A⇒B$ together an arrow $\eval:(A⇒B)×A→B$ satisfying the following universal
  property: for any object $C∈\Obj\bfC$ and arrow $f:C⊗A→B$, there is an arrow
  $λf:C→(A⇒B)$ making the following diagram commute:
  \begin{center}
    \begin{tikzcd}[sep=large]
      C×A \arrow[dr, "f"] \arrow[d, swap, "λf×\id_A"] & {} \\
      (A⇒B)×A \arrow[r, swap, "\eval"]      & B
    \end{tikzcd}
  \end{center}
  When $\bfC$ is cartesian, $A⇒B$ is called an \define{exponential},
  and is denoted $B^A$.
\end{definition}

\begin{definition}
  A monoidal category $(\bfC,⊗)$ is \define{closed} when it has exponentials.
\end{definition}

\begin{remark}
  (Compare to \cref{rmk:prod-coprod-universal-adj})
  In a closed monoidal category, for all $A,B,C∈\Obj\bfC$,
  $λ$ and $\eval$ define a bijection
  \begin{equation*}
    \Hom(A⊗B,C)≅\Hom(A,B⇒C).
  \end{equation*}
  This bijection can be taken as their definition
  with an additional ``naturalality'' condition.
  This bijection is often called \define{currying}.
\end{remark}

\begin{example*}
  $\Set$ and $\universe$ are closed monoidal. Define $B⇒A\coloneqq \Hom(B,A)$.
  Then this is an element of $\Set$/$\universe$. Mixing set- and type-theoretical
  notation, the bijection is given by the mutually inverse functions
  \begin{align*}
    \Hom(A×B,C) &\longrightarrow \Hom(A,\Hom(B,C)) \\
    f &\longmapsto \λ{a}{\λ{b}{\appply{f}{a}{b}}} \\
    \Hom(A,\Hom(B,C)) &\longrightarrow \Hom(A×B,C) \\
    g &\longmapsto \λ{p}{\appply{g}{(\appr{1}{a})}{(\appr{2}{b})}}
  \end{align*}
\end{example*}

\begin{definition}\label[definition]{def:cartesian-cat}
  A category is \define{Cartesian closed}
  when it is closed, cartesian monoidal, and has a terminal object.
  Equivalently, it is cartesian closed when it has
  \begin{itemize}
    \itemsep-0.2em
    \item \vspace{-0.1em} a terminal object (\cref{def:terminal-and-initial}),
    \item binary products (\cref{def:product-and-coproduct}), and
    \item exponentials (\cref{def:exponentials}).
  \end{itemize}
  ``CCC'' abbreviates ``Cartesian closed category''.
\end{definition}

\begin{remark}\label[remark]{rmk:lambek}
	We now have the vocabulary to (informally) extend the correspondence of
  \cref{sec:propositions-and-types} to statements about CCCs.
  \cite{lambek}
  \TODO{}
\end{remark}

\begin{remark}
  The Curry-Howard-Lambek correspondence gives the barest hint of insight into
  Voevodsky's innovation. He also gave a categorically-based model of a version
  of the typed λ-calculus, but of full Martin-Löf dependent type theory. The
  structure of higher/nested identity types
  ($\propeq{\propeq{\cdots}{a}{b}}{p}{q}$) recalled ideas of homotopy theory,
  where paths are considered identical up to the existence of ``higher
  paths''.\TODO{}

  For the initial development of the connection between locally Cartesian closed
  categories (LCCCs) and type theory, see \cite{seely}.
\end{remark}

\section{Functors and their algebras}
\label{sec:functors-and-their-algebras}

\begin{definition}\label[definition]{def:functor}
	A \define{functor}\index{Functor} $F$ between categories $\bfC$ and $\bfD$
  consists of the following data:
  \begin{itemize}
    \itemsep-0.2em
    \item a map $F_0:\Obj \bfC\to\Obj \bfD$
    \item for each pair of objects $A,B\in\Obj C$, a map \\
      $F_1:\Hom_{\bfC}(A,B) \to\Hom_{\bfD}(F_0(A),F_0(B))$
  \end{itemize}
  These data are subject to the following axioms:
  \begin{enumerate}%[label=\Alph*.]
    \itemsep-0.2em
    \item functors preserve composition
    \item $F(\id_A)=\id_{F(A)}$ for all $A\in\Obj C$.
  \end{enumerate}
\end{definition}

We generally leave off the subscripts and parentheses when possible, denoting
the application by simply $FA$ or $Ff$. A functor $F$ from $\bfC$ to
$\bfD$ may be denoted $F:\bfC\to\bfD$. We may define functors without names
using the following notation:
\begin{align*}
  \bfC &⟶ \bfD \\
  A    &⟼_0 \ldots \\
  f    &⟼_1 \ldots
\end{align*}

\begin{example}\label[example]{ex:identity-functor-cat}
  For any category $\bfC$, there is an \define{identity
  functor}\index{Identity!Functor} $\id_{\bfC}\bfC\to\bfC$ which acts as the
  identity on objects and morphisms. The composition of functors is associative,
  and there is a category $\Cat$ of ``small'' categories (it doesn't include
  itself, for instance).
\end{example}

\begin{example}\label[example]{ex:diagonal}
  There is a product in $\Cat$; the product $\bfC×\bfD$ has as object pairs of
  objects $(A,B)$ for $A∈\bfC$ and $B∈\bfD$ and similarly for arrows.
  For any category, there is a \define{diagonal functor}
  \begin{align*}
    Δ:\bfC &⟶ \bfC×\bfC \\
    A    &⟼_0 (A,A) \\
    f    &⟼_1 (f,f)
  \end{align*}
\end{example}

\begin{example}\label[example]{ex:forget}
  For each category of algebraic objects where the morphisms are the
  corresponding type of homomorphism, there is a \define{forgetful functor},
  generally denoted $U$, which takes sets with some structure to their
  underlying sets and homomorphisms to the corresponding maps of sets. For
  instance, there is a forgetful functor $U:\Grp\to\Set$.
\end{example}

\begin{example}\label[example]{ex:coproduct-functoriality}
	If $\bfC$ has binary coproducts, then
  for any fixed $A,B\in\Obj\bfC$, one can define the following
  functors:
  \begin{align*}
    \begin{split}
      \bfC &⟶ \bfC \\
      X    &⟼_0 A + X \\
      f    &⟼_1 \id_A+f = [i_1, i_2∘ f]
    \end{split}
    \begin{split}
      \bfC &⟶ \bfC \\
      Y    &⟼_0 Y + B \\
      g    &⟼_1 g+\id_B = [i_1∘ g, i_2]
    \end{split}
  \end{align*}
  These two functors interact well, meaning in part that they extend to
  a \define{bifunctor}
  \begin{align*}
    \begin{split}
      \bfC×\bfC &⟶ \bfC \\
      (X,Y)    &⟼_0 X + Y \\
      (f,g)    &⟼_1 f+g
    \end{split}
  \end{align*}
  where $(f+g)∘\inl=f$ and $(f+g)∘\inr=g$.
\end{example}

Since functors preserve sources, targets, and composition, they preserve
commutative diagrams. If $f,g,h$ form a commutative triangle in $\bfC$, then
their images under $F:\bfC\to\bfD$ do in $\bfD$:
\begin{center}
  \begin{tikzcd}[column sep=large]
    A\arrow[dr, "f"]\arrow[dd, "h"]\arrow[rr,mapsto,"F"] & {} & F A
    \arrow[dr, "F f"] \arrow[dd, near start, "F h", crossing over] & {} \\
    {} & B\arrow[dl, "g"]\arrow[rr,near start, mapsto, "F", crossing over] & {}
    & F B \arrow[dl, "F g"]\\
    C \arrow[rr,mapsto,"F"]& {} & F C &  {}
  \end{tikzcd}
\end{center}
Note however that it is possible that $FA=FB=FC$, as in a functor to a category
with a single object. However, the equalties between composites still hold.
One consequence is that functors preserve isomorphism.

\begin{definition}\label[definition]{def:endofunctor}
	An \define{endofunctor}\index{Functor!Endofunctor} is a functor with identical
  domain and codomain.
\end{definition}

\begin{definition}\label[definition]{def:f-coalgebra}
  \
  \vspace{-0.3em}\dual{
    An \define{algebra}\index{(Co)algebra for a functor} for an endofunctor
    $F:\bfC\to\bfC$ (also called an $F$\define{-algebra}) is a pair
    $(A,α)$ of an object $A\in\Obj\bfC$ and an arrow $α:FA\to A$.
  }{
    A \define{coalgebra} for an endofunctor
    $F:\bfC\to\bfC$ (also called an $F$\define{-coalgebra}) is a pair
    $(A,α)$ of an object $A\in\Obj\bfC$ and an arrow $α:A\to FA$
    \cite{category-theory-for-computing-science}.
  }
\end{definition}

For the remainder of this section when a definition or result is presented only
for coalgebras or algebras, it holds dually for the other.

\begin{definition}\label[definition]{def:coalgebra-morphism}
  A \define{coalgebra morphism}\index{(Co)algebra morphism} from
  $(A,α)$ to $(B,β)$ is an arrow $f:A\to B$ such that the following
  diagram commutes:
  \begin{center}
    \begin{tikzcd}
      A  \arrow[d, "α"] \arrow[r, "f"] & B \arrow[d, "β"] \\
      FA \arrow[r, "Ff"] & FB
    \end{tikzcd}
  \end{center}
\end{definition}

Since $F$ is a functor, the composition of coalgebra morphisms is again a
coalgebra morphism. In fact, $F$-coalgebras have all of the structure of a
category, which we will call $F\Coalg$.\TODO{Proof?}

\begin{definition}\label[definition]{def:initial-alg}
  \
  \vspace{-0.3em}\dual{
    An \define{initial $F$-algebra}\index{Initial algebra} is an initial
    object of $F\Alg$.
  }{
    An \define{final $F$-coalgebra}\index{Final coalgebra} is a terminal
    object of $F\Coalg$.
  }
\end{definition}

The following example is crucial to
\cref{chap:coinductive-types-in-univalent-type-theory}.

\begin{example}[\unimathname{CategoryTheory.FunctorAlgebras.Nats}]\label[example]{ex:nat}
  Let $\bfC$ be a category with distinguished binary coproducts and a terminal
  object $1$. Consider the functor
  \begin{align*}
    F:\bfC &⟶ \bfC  \\
    A &⟼_0 1+A \\
    f &⟼_1 \id_1+f
  \end{align*}
  where 1 is a terminal object\index{Terminal object} and $+$ is the coproduct
  bifunctor as in \cref{ex:coproduct-functoriality}. Let's examine
  what it \textit{means} for some $F$-algebra $(N,η)$ to be initial. By
  composing with the coproduct\index{Coproduct!In category theory} injections,
  we can define
  \begin{align*}
    \begin{split}
      z &: 1 ⟶ N \\
      z &= η∘ i_1
    \end{split}
    \begin{split}
      s &: 1 ⟶ N \\
      s &= η∘ i_2
    \end{split}
  \end{align*}
  so that $η=[z,s]$:
  \begin{center}
    \begin{tikzcd}[sep=large,ampersand replacement=\&]
      1 \arrow[r, "i_1"{name=I1}]\arrow[dr, swap, "z"{name=F}]
      \& 1+N \arrow[d,"η"]
      \& N\arrow[l, "i_2"{name=I2},swap]\arrow[dl,"s"{name=G}] \\
      {} \& N \& {}
    \end{tikzcd}
  \end{center}
  Suppose we have another $F$-algebra $(A,α)$, and we define $f,g$ by
  composition as above so that $α=[f,g]$. By initiality of $(N,η)$,
  there is a unique arrow $u$ making the following diagram commute:
  \newcommand{\eeeeeta}{[z,s]}
  \newcommand{\aaaaalpha}{[f,g]}
  \begin{center}
    \begin{tikzcd}[sep=large]
      1+N \arrow[r, dashed, "\id_1+u"]\arrow[d, "\eeeeeta"] & 1+A\arrow[d, "\aaaaalpha"]  \\
      N \arrow[r, dashed, "u"] & A
    \end{tikzcd}
  \end{center}
  By functoriality of the coproduct (\cref{ex:coproduct-functoriality}), we can
  compose along either the left- or right-hand paths in the above diagram.
  The above square states $u∘ [z,s]= [f,g]∘ (\id_1+u)$. Precomposing
  with $i_1:1\to 1+N$ yields
  \begin{align*}
    u∘ z = u∘ [z,s]∘ i_1
    &= [f,g]∘ (\id_1+u) ∘ i_1
    && \text{Above diagram} \\
    &= [f,g]∘ [i_1,i_2∘ u] ∘ i_1
    && \text{Definition of }+ \\
    &= [f, g]∘ i_1 \\
    &= f
  \end{align*}
  By similar reasoning, precomposing with $i_2$ yields
  \begin{equation*}
    u∘ s = u∘ [z,s] ∘ i_2
    = [f,g]∘(\id_1+u)∘ i_2
    = g∘ u.
  \end{equation*}
  Combining the above two equations, we have the following universal property
  for $(N,η)$. For any object $A$ with arrows $f:1\to A$ and $g:A\to A$,
  there is a unique arrow $u:N\to A$ making the following diagram commute:
  \begin{center}
    \begin{tikzcd}[sep=large]
      1
        \arrow[r, "z"]
        \arrow[dr, swap, "f"]
        & N \arrow[r, "s"] \arrow[d, "u", dashed]
        & N \arrow[d, "u", dashed] \\
      {}
      & A \arrow[r, "g"]
      & A
    \end{tikzcd}
  \end{center}
  Well, we've successfully rephrased the property of initiality, but it seems
  just as cryptic now as it was then. Let's see if we can figure out what an
  type with this property would look like in our favorite category \universe, the
  universe of small types in \ITT!

  Let \List{\N} be the type of lists of natural numbers
  as in \cref{subsubsec:lists}. To utilize the above property, we need
  to choose $A$, $f$, and $g$. Pick the following:
  \begin{itemize}
    \itemsep0em
    \item $A:\equiv \List{\N}$
    \item $f:\equiv \λ{x}{\nil}$
    \item $g:\equiv \λ{l}{\cons(5,l)}$
  \end{itemize}
  From the universal property of $(N,[z,s])$, we get a function
  $u:N\to \List{A}$ such that
  \begin{align*}
    u(z) = \nil
    &&\text{and}&&
    u(s(n)) = \cons(5, u(n))
  \end{align*}
  Look familiar yet? Indeed, one type with such a property is $\N$! In that
  case, $u$ is the function that, when given a number $n$, outputs a list of $5$s
  with length $n$.\footnote{In $\Set$, $\N$ would be an initial algebra.
    It makes sense to ask if this functor has an initial algebra in any
    category with a terminal object and chosen binary coproducts, and in
    general, such algebras are called \define{natural number objects} (NNOs)
    \cite{sketches} \cite{lawvere-etcs}.
  }
\end{example}

\begin{example}\label[example]{ex:final-coalgebra}
  A coalgebra for this same functor $A↦A+1$ is a set $A$ with a partial unary
  operation (where the value $1$ represents failure/undefined).
  In $\universe$ the type $\N_∞\jdeq\N+\unittype$ is the carrier of a final
  coalgebra given by a partial operation
  \begin{alignat*}{3}
    &\ttfun{pred} : \N+\unittype &&\to(\N+\unittype)+\unittype \\
    &\ttfun{pred}(\apinl{0})                 &&\jdeq \apinr{(\apinr{\unitelem})}
      &\qquad\quad&\text{(undefined)}\\
    &\ttfun{pred}(\apinl{(\apply{\suc}{n})}) &&\jdeq n
      &\qquad\quad& \\
    &\ttfun{pred}(\apinr{\unitelem})         &&\jdeq \apinr{\unitelem}
      &\qquad\quad&\text{($\apinr{\unitelem}$ is like $∞$)}
  \end{alignat*}
  which acts like the natural numbers with an extra ``infinite element''
  \cite{adamek}.
\end{example}

\begin{remark}\label[remark]{rmk:universal-algebra}
  The form of the functor encodes what a student of universal algebra would call
  a \define{signature}\index{Signature}. By the universal property of
  coproducts, of an algebra for an functor of the form $A↦(A×A)+A+1$ consists
  of:
  \begin{itemize}
    \itemsep0em
    \item an arrow $A×A→A$ (a binary operation on $A$),
    \item an arrow $A→A$ (a unary operation on $A$),
    \item and a distinguished element $1→A$.
  \end{itemize}
  This is exactly the signature of a \define{group}\index{Group} (the arrow
  $A×A→A$ corresponds the group operation, $A→A$ to inversion, and $1→A$ to the
  identity).\footnote{However, an algebra for this functor will not form a group
    in general: the signature doesn't encode the group laws (the operation is
    associative, etc.). To encode these relations, one needs the richer
    structure of a \define{monad}\indeX{Monad}.}
\end{remark}

\begin{example}\label[example]{ex:initial-algebras}
  \
  \begin{itemize}
    \itemsep0em
    \item An initial algebra for the functor $A↦B×A+1$ has the universal property
      of a \define{list object}.
    \item The initial algebra $A↦(A×A)+1$ has a universal property expressing
      the induction principle for binary trees.
  \end{itemize}
\end{example}

\section{Type theoretic category theory}
\label{sec:type-theoretic-category-theory}

Unfortunately, terminology varies between the three predominant sources
on category theory in univalent type theory \cite{book} \cite{unimath}
\cite{hott-lib}.

For a pioneering development of univalent category theory, see \cite{rezk} (the
formalization of which served as a foundation for \UniMath{}'s
\unimathname{CategoryTheory} package).


\begin{theorem}
	The precategory $\universe$ of types is complete
  (\cref{def:complete-and-cocomplete}) \cite{homotopy-limits}.
\end{theorem}
\begin{proof}

\end{proof}

\chapter{Coinductive types in univalent type theory}
\label{chap:coinductive-types-in-univalent-type-theory}

The results in this chapter all appear in the \UniMath{} package
\unimathname{Induction}, so that prefix is left off
(e.g.\ \unimathname{Induction.W.Core} is shortened to
\unimathname{W.Core}).

\section{W and M}
\label{sec:w-and-m}

We now present W-types and M-types. Per Martin-Löf introduced W-types as a
way to study types with a well-ordering. In fact, W-types provide a general and
convenient setting for the study of induction. In extensional type theories,
W-types are equivalent to initial algebras (as in \cref{def:initial-alg}) for
polynomial functors (see \cref{def:polynomial-functor}), so all the examples
we've seen so far of inductive types defined as functor algebras are W-types.

Dually, M-types are non-wellfounded trees, that is, trees with potentially
infinitely long branches.

\begin{definition}\label[definition]{def:container}
  A \define{container}\index{Container} (sometimes called a
  \define{signature}\index{Signature} by analogy, see
  \cref{rmk:universal-algebra}) is a pair $(A,B)$ of a type
  $A:\universe$ and a family $B:A→\universe$.
\end{definition}
Given a container, we can form the associated W- and M-types:
\begin{gatherjot}
  \prftree[r]{W-form}
    {Γ ⊢ A:\universe}{Γ ⊢ B:A\to\universe}
    {Γ ⊢ \type{\W{a}{A}{B}}}
  \\
  \prftree[r]{M-form}
    {Γ ⊢ A:\universe}{Γ ⊢ B:A\to\universe}
    {Γ ⊢ \type{\M{a}{A}{B}}}
\end{gatherjot}
Again, these types are shaped like trees. An element of a W-type is specified by
an element $a:A$ (the ``label'' of this vertex) and a function
$b:B(a)\to \W{x}{A}{B(x)}$ which picks out its subtrees.
\begin{equation*}
  \prftree[r]{W-intro}
    {Γ ⊢ A:\universe}{Γ ⊢ B:A\to\universe}{Γ ⊢ a:A}{Γ ⊢ b:B(a)\to \W{x}{A}{B}}
    {Γ ⊢ \sup{a}{b}:\W{x}{A}{B}}
\end{equation*}
Martin-Löf points out that a ``bottom'' element can be given by taking one of
the $B(a)$ to be $\emptytype$ and the subtree-function to be the one provided by
$\rec_{\emptytype}$ (example in \cref{ex:nat-w})\TODO{reference}.

The elimination rule\index{Elimination!For W-types} for W-types encodes
transfinite induction. If, when a property $C:\W{a}{A}{B}→\universe$ holds for
all subtrees of a given node it also holds for that node, then it holds for any
element in $\W{a}{A}{B}$:
\begin{equation*}
  \prftree[r]{W-elim}
    {Γ ⊢ w:\W{a}{A}{B}}
    {\prftree[r, noline]{}
      {Γ, a:A,\quad f:\apply{B}{a}→\W{a}{A}{B},\quad g:\∏{b:\apply{B}{a}}{\apply{C}{(\apply{f}{b})}}}
      {⊢\apppply{h}{a}{f}{g}: C(\sup{a}{f})}}
    {Γ ⊢ \appply{\recW}{w}{h}:\apply{C}{w}}
\end{equation*}

\begin{example}\label[example]{ex:nat-w}
  The natural numbers are readily encoded as a W-type.
  Define $B\defeq \apppply{\rec_{\booltype}}{\universe}{\emptytype}{\unittype}$
  so that $\apply{B}{\bfalse}\jdeq \emptytype$ and
  $\apply{B}{\btrue}\jdeq\unittype$, and consider $\N'\jdeq \W{b}{A}{B}$.
  Let $!$ be the unique function out of the empty type and further define:
  \begin{align*}
    \begin{split}
      0_{\N'} &: \W{b}{\booltype}{B} \\
      0_{\N'} &\defeq \sup{\bfalse}{!} \\
    \end{split}
    \begin{split}
      1_{\N'} &: \W{b}{\booltype}{B} \\
      1_{\N'} &\defeq \sup{\btrue}{(\λ{x}{0_{\N'}})}
    \end{split}
  \end{align*}
  Following this pattern, we can define
  \begin{align*}
    \suc &: \N' \to \N' \\
    \suc &\defeq \λ{n}{\sup{\btrue}{(\λ{x}{n})}}
  \end{align*}
  so that $1_{\N'}\equiv \apply{\suc}{0_{\N'}}$. What does this tree look like?
  Well, by virtue of construction, $0_{\N'}$ is its least element. Applying
  $\suc$ gives us an element at one level higher in the tree than the one we put
  in.
  \vspace{-2em}\begin{center}
    \begin{tikzpicture}[scale=4]
      \node (A) [scale=2] {$\cdot$};
      \node     [above of=A] {$0_{\N'}$};

      \node (B) [scale=2, right of=A] {$\cdot$};
      \node     [above of=B] {$1_{\N'}$};

      \node (C) [scale=2, right of=B] {$\cdot$};
      \node     [above of=C] {$\apply{\suc}{1_{\N'}}$};

      \node (D) [scale=2, right of=C] {$\cdot$};
      \node     [above of=D] {$\cdots$};

      \node     [right of=D, scale=2.0, color=white] {$\cdot$};
      \node (E) [right of=D] {$\cdots$};
      \node     [scale=2, above of=E, color=white] {$\cdots$};

      \draw[->] (A) edge (B);
      \draw[->] (B) edge (C);
      \draw[->] (C) edge (D);
      \draw[->] (D) edge (E);
      % \draw (A) edge [in=-60,out=-20,loop] node[below] {$e$} (A);
    \end{tikzpicture}
  \end{center}

  We can derive from the induction principle of $\booltype$ that it
  only has two elements, $\bfalse$ and $\btrue$ \TODO{reference}.
  Using this (and ignoring uniquely specified terms like $!$),
  the W-elimination rule for $\N'$ can be reshaped and simplified:
  \begin{equation*}
    \prftree[r]{\N'-elim}
      {Γ ⊢ n:\N'}
      {\prftree[r, noline]{}
        {Γ⊢h_1: C(0_{\N'})}}
      {\prftree[r, noline]{}
        {Γ, f:\unittype→\N',g:\∏{\unitelem:\unittype}\apply{C}{(\apply{f}{\unitelem)}}}
        {⊢\appply{h_2}{f}{g}: C(\sup{\btrue}{f})}}
      {Γ ⊢ \apppply{\rec_{\N'}}{n}{h_1}{h_2}:\apply{C}{n}}
  \end{equation*}
  But since a function $\unittype→\N'$ is equivalent to picking out a single
  element of $\N'$ and $\sup{\btrue}{f}\equiv\apply{\suc}{f}$,
  this is just the elimination rule for the natural numbers (\TODO{ref})
  in disguise.
\end{example}

Other examples include lists, binary trees, ordinals of the second number
class, and more.\TODO{reference} In the next few sections, we will develop a
correspondence between W-types (resp.\ M-types) and initial algebras (resp.\
final coalgebras) for a general class of functor on $\universe$, which will
include all examples in \cref{sec:functors-and-their-algebras}.

\section{Preliminaries}
\label{sec:preliminaries}

\begin{definition}[\unimathname{PolynomialFunctors.polynomial\_functor}]
  \label[definition]{def:polynomial-functor}
  Given a container $S\defeq (A,B)$, its associated \define{polynomial functor}
  is the function
  \begin{gather*}
    P:\universe\to \universe \\
    \apply{P_{A,B}}{X}\defeq\∑{a:A}{B(a)\to X}
  \end{gather*}
  We will regularly leave off the subscript for $P$.
  The \define{action} of $P$ on functions is
  \begin{gather*}
    P^* : (X\to Y)\to P_{A,B}(X)\to P_{A,B}(Y) \\
    \apply{P^*f}{(a,g)}\defeq (a,g∘f).
  \end{gather*}
\end{definition}

\begin{remark}
	Technically, polynomial functors aren't functors, since $\universe$ isn't
  a category (the whole point of the univalent perspective is that it behaves
  more like a weak higher category). It has functorial properties only up to
  \textit{propositional} equality, which will suffice for the following proofs:
  \begin{align*}
    \apply{P^*}{g∘f} = \apply{P^*}{g}∘\apply{P^*}{f}
    &&
    \apply{P^*}{\id_X} = \apply{P^*}{\id_X}
  \end{align*}
\end{remark}

\begin{definition}\label[definition]{def:algebra-coalgebra-in-utt}
  Exactly as in \cref{def:f-coalgebra}, we define the type of (co)algebras for a
  polynomial functor $P$ associated to a container $(A,B)$ as follows:
  \begin{align*}
    \Algtype_P\≔\∑{A:\universe}{PA→A} && \Coalgtype_P\≔\∑{A:\universe}{A→PA}
  \end{align*}
  and coalgebra morphisms as
  \begin{align*}
    \ttfun{CoalgMor}((X,α),(Y,β))
    &\≔ (X,α)⇒(Y,β) \\
    &\≔ \∑{f:X→Y}{(\apply{P}{f})∘α = β∘f}
  \end{align*}
  with algebra morphisms defined dually \cite{homotopy-initial}.
\end{definition}

See \cref{ex:nat-functor-utt} for the polynomial functor for which $\ℕ$ is
initial.

\section{Internalizing M-types}
\label{sec:internalizing-m-types}

Fix a container $S\defeq (A,B)$ and let $P$ be the associated polynomial functor.
We will prove a few auxiliary lemmas on the way to the following result.
This proof appeared in the \Agda{} formalization of \cite{non-wellfounded}, this
is its first appearance ``de-formalized''.

\subsection{Uniqueness}
\label{subsec:uniqueness}

\begin{lemma}[\unimathname{M.Uniqueness.M\_coalg\_eq},
              \unimathname{M.Uniqueness.isaprop\_M}]
  \label[lemma]{lemma:final-colagebra-unique}
  Any two final $P$-coalgebras are equal. In other words, the following type is
  a proposition:
  \begin{equation*}
    \Final(S)\defeq
    \∑{(X,α):\Coalgtype_{S}}{
      \∏{(Y,β):\Coalgtype_{S}}{\isContr((Y,β)⇒ (X,α))}
    }
  \end{equation*}
\end{lemma}

First, we'll show that their carriers are equivalent:

\begin{lemma}[\unimathname{M.Uniqueness.M\_carriers\_iso}]
  \label[lemma]{lemma:algebra-iso-equiv}
	If $(X,α)$ and $(Y,β)$ are final $P$-coalgebras,
  then the first projections of the unique coalgebra morphisms
  $f:X⇒ Y$ and $g:Y⇒ X$ induce
  an equivalence of types $\weq{X}{Y}$.
\end{lemma}
\begin{proof}
  This proof is a standard categorical technique, much reminiscent of
  \cref{lemma:terminal-unique}.
	\TODO{proof}
\end{proof}

We can then invoke the characterization of paths in $Σ$-types,
\cref{lemma:path-sigma}. We'll need to demonstrate that the coalgebra map
$α:X\to FX$ is equal to $β:Y\to FY$ when transported along the path
constructed in \cref{lemma:algebra-iso-equiv}.

\begin{lemma}\label[lemma]{lemma:polynomial-functor-transport}
  For all $X,Y:\universe$, $F:\universe\to\universe$,
  $f:X\to FX$, $g:Y\to FY$, and $p:\propeq{}{X}{Y}$,
  if for all $x:X$ we have
  \begin{equation*}
    \propeq{}{
      \transport{F}{p}{\apply{f}{x}}
    }{
      g({\transport{\id_{\universe}}{p}{x}})
    }
  \end{equation*}
  then $\propeq{}{\transport{Z↦ (Z\to FZ)}{p}{f}}{g}$.
  That is, $f$ is equal to $g$ after being transported just when
  applying $f$ and transporting the result is the same as transporting the
  input and applying $g$.
\end{lemma}
\begin{proof}
  Using identity elimination (\cref{rule:id-elim}), it suffices to assume
  $X\jdeq Y$ and $p\jdeq\refl{X}$. Then by the definition of transport
  (\cref{lemma:transport}), our hypothesis becomes
  \begin{align*}
    &\propeq{}{
      \transport{F}{p}{\apply{f}{x}}
    }{
      g({\transport{\id_{\universe}}{p}{x}})
    } \\
    &\implies
    \propeq{}{
      \transport{F}{\refl{X}}{\apply{f}{x}}
    }{
      g({\transport{\id_{\universe}}{\refl{X}}{x}})
    } \\
    &\implies
    \propeq{}{\apply{f}{x}}{\apply{g}{x}}
  \end{align*}
  so by function extensionality $f=g$. Again by definition of transport,
  $\transport{Z↦ (Z\to FZ)}{\refl{X}}{f} \jdeq f=g$.
\end{proof}

To complete the proof that the transported coalgebra maps are equal, we'll need
the following auxiliary result:

\begin{lemma}\label[lemma]{lemma:polynomial-functor-transport}
  For all $X,Y:\universe$ and $p:\propeq{}{X}{Y}$,
  \begin{equation*}
    \propeq{}{
      P^*\paren{\transpor{\id_{\universe}}{p}}
    }{
      \transpor{P}{p}
    }
  \end{equation*}
  Note that $P^*$ is applied to the function $\transpor{\id_{\universe}}{p}$
  before it gets applied to its second argument.
\end{lemma}
\begin{proof}
  Note that we're using \cref{notation:transport}. By the elimination rule for
  the identity type, (\cref{rule:id-elim}), it suffices to assume that $X\jdeq
  Y$ and that $p\jdeq\refl{X}$. Then
  \begin{align*}
    P^*\paren*{\transpor{\id_{\universe}}{p}}
    &\jdeq P^*\paren*{\transpor{\id_{\universe}}{\refl{X}}}
    && \text{Identity elim.} \\
    &\jdeq P^*\paren{\id_X}
    && \text{\Cref{lemma:transport}} \\
    &\jdeq \λ{(a,f)}{(a,f∘ \id_X)}
    && \text{\Cref{def:polynomial-functor}} \\
    &\jdeq \λ{(a,f)}{(a,f)} \\
    &\jdeq \id_{PX}
    && \text{\Cref{def:id-polymorphic}} \\
    &\jdeq \transpor{\id_{\universe}}{\refl{PX}}
    && \text{\Cref{lemma:transport}} \\
    &\jdeq \transpor{\id_{\universe}}{\ap{P}{\refl{X}}}
    && \text{\Cref{lemma:ap}} \\
    &\jdeq \transpor{P}{\refl{X}}
    && \text{\Cref{lemma:transport-compose}} \\
    &\jdeq \transpor{P}{p}
    && \text{Identity elim.}
  \end{align*}
\end{proof}

\begin{proof}[Proof of \cref{lemma:final-colagebra-unique}]
  Let:
  \begin{itemize}
    \itemsep0em
    \item $(X,α)$, $(Y,β)$ be final $P$-coalgebras,
    \item $f:X⇒ Y$ and $g:Y⇒ X$ be the unique $P$-coalgebra
      morphisms between them,
    \item $q:\weq{X}{Y}$ the equivalence of types induced by $f$ and $g$
      (\cref{lemma:algebra-iso-equiv}).
  \end{itemize}
  By univalence, there is a path
    $\apply{\ua}{q}:\propeq{\universe}{X}{Y}$.\footnote{This
    is our first (but not nearly our last) crucial use of univalence. Without an
    equality, we couldn't \transportname{} $α$ to $β$ in the next step.
    This step also demonstrates that function extensionality alone doesn't
    suffice.}
  To demonstrate that $\propeq{}{(X,α)}{(Y,β)}$, we invoke the
  characterization of paths in $Σ$-types, \cref{lemma:path-sigma}. It
  remains to show
  \begin{equation*}
    \propeq{(Y\to PY)}{\transport{Z↦ (Z\to PZ)}{\apply{\ua}{q}}{α}}{β}.
  \end{equation*}
  but by \cref{lemma:polynomial-functor-transport}, it suffices to show that
  for all $x:X$,
  \begin{equation*}
    \propeq{}{
      \transport{P}{\apply{\ua}{q}}{\apply{α}{x}}
    }{
      β(\transport{\id_\universe}{p}{x})
    }.
  \end{equation*}
  % Lemma 2 in HoTT/M-types
  First, note that
  \begin{align*}
    \transpor{P}{\apply{\ua}{q}}
    &= P^*\paren{\transpor{\id_{\universe}}{\apply{\ua}{q}}}
    && \text{\Cref{lemma:polynomial-functor-transport}} \\
    &= P^*q
    && \text{\Cref{def:ua}} \\
    &= P^*(\appr{1}{f})
    && \text{\Cref{lemma:algebra-iso-equiv}}
  \end{align*}
  The last step utilizes the idea of \textit{proof-relevant
    mathematics}. Although we define $q$ within a proof, we can (without
  cheating) refer to its definition from another proof. Also note the
  use of \cref{notation:weq-coerce}. Working from the other side of the
  equation, we can use the computational rule of univalence (\cref{def:ua}):
  \begin{align*}
    β(\transport{\id_\universe}{\apply{\ua}{q}}{x})
    = β(\apply{q}{x})
    = β({\appr{1}{f}}{x})
  \end{align*}
  Thus, we now want to demonstrate that
  \begin{align*}
    P^*(\appr{1}{f})(α x) &=
    \transport{P}{\apply{\ua}{q}}{\apply{α}{x}} \\
    &= β(\transport{\id_\universe}{p}{x}) \\
    &= β({\appr{1}{f}}{x})
  \end{align*}
  However, this is exactly the condition that $f$ is a $P$-coalgebra morphism:
  \TODO{reference definition}
  \begin{center}
    \begin{tikzcd}[column sep=large]
      X  \arrow[d, "α"] \arrow[r, "\appr{1}{f}"] & Y \arrow[d, "β"] \\
      FX \arrow[r, "P^*(\appr{1}{f})"] & FY
    \end{tikzcd}
  \end{center}
\end{proof}

\begin{lemma}[\unimathname{W.Uniqueness.W\_alg\_eq},
              \unimathname{W.Uniqueness.isaprop\_W}]
  Any two initial $P$-algebras are equal.
\end{lemma}
\begin{proof}
	The above proof works with only minor modifications, see the \Coq{}
  development for details.
\end{proof}

\subsection{Fibered algebras and the natural numbers}
\label{subsec:fibered-algebras}

A functor algebra $\dpair{X}{α}$ has as its first component a type
$X:\universe$. What would it mean for an algebra to be ``fibered'' over this
one, with a first component $Y:X → \universe$? When can a \textit{dependent}
type (over an algebra) can be given the structure of a functor algebra?
Such fibered algebras generalize regular algebras for endofunctors.

The fiber-initial property of $\ℕ$
(see \cref{def:fiber-initiality} and \cref{lemma:fiber-initiality-nat})
is essential to proving a property about limits
(\cref{lemma:cochains}) that plays a pivotal role in the proof of the existence
of M-types.

\begin{definition}[\unimathname{W.Fibered.fibered\_alg},
                   \unimathname{W.Fibered.algebra\_section}]
  \label[definition]{def:algebra-fibration}
  Inspired by the homotopical semantics for \UTT{},
  if $(X,α)$ is an algebra for the functor associated to $(A,B)$,
  the type of \define{fibered $P$-algebras} over $X$ is
  \begin{equation*}
    \Fibalgtype(X)\≔
    \∑{E:X→\universe}
    {\∏{\dpair{x}{h}:P_{A,B}X}{
      \paren{\∏{b:\apply{B}{x}}{\apply{E}{(\apply{h}{b})}}}
      → \apply{E}{(\apply{\alpha}{\dpair{x}{h}})}
    }}
  \end{equation*}
  an \define{algebra fibration}.
  % ((∑ (f : forall x, E x), forall a, f (c a) = e a (f ∘ (pr2 a)))).
  An \define{algebra section} for an algebra fibration $(E,θ)$ is a pair
  \begin{equation*}
    \∑{f:\∏{x:X}{\apply{E}{x}}}
    \∏{x:P_{A,B}X}
    {\propeq
      {}
      % {\∏{x:X}{\apply{E}{(\apply{α}{x})}}}
      {\apply{f}{(\apply{α}{x})}}
      {\appply{θ}{x}{(f∘\appr{2}{x})}}}
  \end{equation*}
\end{definition}

\begin{example}[\unimathname{W.Fibered.alg2fibered\_alg},
                \unimathname{W.Fibered.fibered\_alg2alg}]
  Any $P$-algebra can be made into a fibered $P$-algebra over $(X,α)$ by using a
  constant family:
  \begin{align*}
    \Algtype_P &\longrightarrow \Fibalgtype(X) \\
    \dpair{Y}{β} &\longmapsto
                \dpair{\λ{x}{Y}} {\λ{x}{\λ{f}{\apply{β}{\dpair{\appr{1}{x}}{f}}}}}
  \end{align*}
  Since the $x$ in the argument to the second coordinate is of type
  $P_{A,B}X\jdeq \∑{a:A}\apply{B}{a}→X$ and $f:\apply{B}{(\appr{1}{x})}→Y$, the pair
  $\dpair{x}{f}$ has type $P_{A,B}Y\jdeq \∑{a:A}\apply{B}{a}→Y$, which can then be
  sent via $β$ to $Y$. This matches our expectation that a non-dependent
  $\ttfun{foo}$ should be a generalization of a dependent $\ttfun{foo}$.
  In turn, any fibered algebra with family $E:X→\universe$ can be turned into a
  non-fibered algebra over the ``total space'' $\∑{x:X}\apply{E}{x}$.
\end{example}

\begin{definition}[\unimathname{W.Fibered.is\_preinitial\_sec}]
  \label[definition]{def:fiber-preinitiality}
  An algebra $\dpair{X}{α}$ is \define{fiber-preinitial}
  if for any fibered algebra $\dpair{E}{θ}$ over
  $X$ the type of algebra sections from $X$ to $E$ is
  inhabited.\footnote{\cite{homotopy-initial} call this an ``inductive
  algebra''.\label{fn:inductive-algebra}}
\end{definition}

\begin{definition}[\unimathname{W.Fibered.fibered\_uniqueness}]
  \label[definition]{def:fiber-uniqueness}
  An algebra $\dpair{X}{α}$ has the \define{fiber uniqueness property}
  if for any fibered algebra $\dpair{E}{θ}$ over
  $X$ the type of algebra sections from $X$ to $E$ is a
  proposition.\footnote{\cite{homotopy-initial} call this the η-rule for
    inductive algebras (see \cref{fn:inductive-algebra}).}
\end{definition}

\begin{definition}\label[definition]{def:fiber-initiality}
  Combining \crefrange{def:fiber-preinitiality}{def:fiber-uniqueness}, an
  algebra $\dpair{X}{α}$ is \define{fiber-initial} if the type of algebra
  sections from $X$ to any fibered algebra $\dpair{E}{θ}$ over $X$ is
  contractible.
\end{definition}

\Cref{subsubsec:fibered-natural-numbers} explores the applications of these
functions to $\ℕ$.

\subsubsection{The natural numbers}
\label{subsubsec:fibered-natural-numbers}

We now prove a few facts about the natural numbers and the functor for which
they are initial, some of which will be familiar from
\cref{sec:functors-and-their-algebras}.\footnote{These arguments are my own,
  though some of these theorems are proved in the \Agda development of
  \cite{non-wellfounded}.}

\begin{example}[\coqname{W.Naturals.nat\_functor}]
  \label[example]{ex:nat-functor-utt}
	Consider the polynomial functor associated to $A\defeq \𝔹$ and
  $B\defeq \apppply{\rec_{\𝔹}}{\universe}{\𝟘}{\𝟙}$:
  \begin{equation*}
    PX \defeq \∑{b:\𝔹}{\appppply{\rec_{\𝔹}}{\universe}{\𝟘}{\𝟙}{b}} → X.
  \end{equation*}
  An algebra for this functor is given by a type $X:\universe$ and a function
  \begin{equation*}
    PX → X\≡ \paren{\∑{b:\𝔹}{\appppply{\rec_{\𝔹}}{\universe}{\𝟘}{\𝟙}{b}} → X} → X
  \end{equation*}
  Define:
  \begin{align*}
    &η : Pℕ → ℕ &&\\
    &\apply{η}{\dpair{\bfalse}{g}} \defeq 0
    &&\quad(\text{In this case,  }g:\𝟘 → X) \\
    &\apply{η}{\dpair{\btrue}{g}}  \defeq \apply{\suc}{(\apply{g}{\unitelem})}
    &&\quad(\text{In this case,  }g:\𝟙 → X) \\
  \end{align*}
\end{example}

While the above polynomial functor $P$ bears some relation to the definition of
$ℕ$ as an W-type (\cref{ex:nat-w}), it is not immediately clear that this is the
correct definition, nor how we can apply our intuition about $\ℕ$ to this
situation. The next lemma provides more evidence that $P$ is an analogue
of $X ↦ 1 + X$.

\begin{lemma}[\coqname{W.Naturals.nat\_functor\_equiv}]
  \label[lemma]{lemma:nat-alg-simpl}
  A $P$-algebra structure on a type $X$ is given by a point $x_0:X$ and a
  function $\suc_x:X→X$. More precisely, there is an equivalence:
  \begin{equation*}
    w:\weq{\Algtype_P}{\∑{X:\universe}{X × (X → X)}}
  \end{equation*}
\end{lemma}
\begin{proof}
	Suppose given a $P$-algebra $(X,α)$. Then define
  \begin{align*}
    \begin{split}
      x_0 &: X \\
      x_0 &\defeq \apply{α}{\dpair{\bfalse}{\apply{\rec_{\𝟘}}{X}}} \\
    \end{split}
    \begin{split}
      \suc_X  &: X → X \\
      \suc_X  &\defeq \λ{x}{\apply{α}{\dpair{\btrue}{\appply{\rec_{\𝟙}}{X}{x}}}}
    \end{split}
  \end{align*}
  Now $\dpair{X}{(x_0,\suc_X)}$ has type $\∑{X:\universe}{X × (X → X)}$ as
  desired.

  In the other direction, suppose we have some type $X$, a point $x_0:X$, and a
  function $\suc_X:X → X$. Define
  \begin{align*}
    &α:\paren{\∑{b:\𝔹}{\appppply{\rec_{\𝔹}}{\universe}{\𝟘}{\𝟙}{b}} → X} → X \\
    &\apply{α}{\dpair{\bfalse}{h}} \defeq x_0 \\
    &\apply{α}{\dpair{\btrue}{h}}  \defeq \apply{\suc_X}{(\apply{h}{\unitelem})}
  \end{align*}
  While it's not immediately that these functions are inverses, it follows from
  initiality of $\emptytype$ in $\universe$ and the contractibility of
  $\unittype$, both of which will be used extensively in the next few proofs.
\end{proof}

Applying this equivalence, we could more readily define the $P$-algebra over
$\ℕ$ as $(\appply{w}{0}{\suc})$. In fact, this gives \textit{judgmentally equal}
result. We can give a similar lemma for morphisms:

\begin{lemma}[\coqname{W.Naturals.mk\_nat\_functor\_algebra\_mor}]
  \label[lemma]{lemma:mk-nat-alg-mor}
  Let $\dpair{X}{α}$ and $\dpair{Y}{β}$ be $P$-algebras. Any function $f:X → Y$
  which sends $x_0$ to $y_0$ and intertwines $\suc_X$ with $\suc_Y$ is an
  algebra morphism.
\end{lemma}
\begin{proof}
	To show that $f$ is an algebra morphism (\cref{def:algebra-coalgebra-in-utt}),
  we must demonstrate that the following diagram commutes:
  \begin{center}
    \begin{tikzcd}
      PX  \arrow[d, "α"] \arrow[r, "P^*f"] & PY \arrow[d, "β"] \\
      X \arrow[r, "f"] & Y
    \end{tikzcd}
  \end{center}
  where $\apply{P^*f}{\dpair{b}{h}}\defeq \dpair{b}{f ∘ h}$
  (\cref{def:polynomial-functor}).
  By function extensionality (\cref{thm:funext}), it suffices to show that the
  two composites are equal on all inputs. Let $\dpair{b}{h}:PX$, and proceed by
  cases on $b:\𝔹$.

  If $b\≡\bfalse$, then $h:\𝟘 → X$. Recall that $\emptytype$ is terminal in
  $\universe$; the type of functions $\𝟘 → Z$ is contractible for any
  $Z:\universe$. Thus,
  \begin{align*}
    \apply{(β∘(P^*f))}{\dpair{\bfalse}{h}}
    &\jdeq \apply{β}{\dpair{\bfalse}{f ∘ h}} \\
    &= \apply{β}{\dpair{\bfalse}{\apply{\rec_{\𝟘}}{Y}}}
    &&\quad\text{Terminality of }\𝟘\\
    &\jdeq y_0
    &&\quad\text{Definition} \\
    &= \apply{f}{x_0}
    &&\quad\text{Hypothesis} \\
    &\jdeq \apply{(f∘α)}{\dpair{\bfalse}{\apply{\rec_{\𝟘}}{X}}} \\
    &= \apply{(f∘α)}{\dpair{\bfalse}{h}}
    &&\quad\text{Terminality of }\𝟘
  \end{align*}

  If $b\≡\btrue$, then $h:\𝟙 → X$. By the induction principle for
  $\𝟙$\TODO{reference}, for any type $Z:\universe$, any function $k:\𝟙 → Z$, and
  any $x:\𝟙$, $\apply{k}{x}=\apply{k}{\unitelem}$. In short,
  $k=\appply{\rec_{\𝟙}}{Z}{(\apply{k}{\unitelem})}$. In this case,
  \begin{align*}
    \apply{(β∘(P^*f))}{\dpair{\btrue}{h}}
    &\jdeq \apply{β}{\dpair{\btrue}{f ∘ h}} \\
    &\= \apply{β}{\dpair{\btrue}{f ∘ (\appply{\rec_{\𝟙}}{Y}{(\apply{h}{\unitelem})})}} \\
    &\= \apply{β}{\dpair{\btrue}{\appply{\rec_{\𝟙}}{Y}{(\apply{f}{(\apply{h}{\unitelem})})}}}
    &&\quad\text{Function ext.} \\
    &\jdeq \apply{\suc_Y}{(\apply{f}{(\apply{h}{\unitelem})})}
    &&\quad\text{Definition of }\suc_Y \\
    &\= \apply{f}{(\apply{\suc_X}{(\apply{h}{\unitelem})})}
    &&\quad\text{Hypothesis} \\
    &\jdeq \apply{f}{(\apply{α}{\dpair{\btrue}{\fromunit{X}{(\apply{h}{\unitelem})}}})}
    &&\quad\text{Definition of }\suc_X \\
    &\jdeq \apply{(f∘α)}{\dpair{\btrue}{\fromunit{X}{(\apply{h}{\unitelem})}}}
    &&\quad\text{Definition} \\
    &\= \apply{(f∘α)}{\dpair{\btrue}{h}}
    &&\quad\text{Function ext.}
  \end{align*}
\end{proof}

\begin{lemma}[\coqname{W.Naturals.nat\_alg\_is\_preinitial}]
	For any $P$-algebra $\dpair{X}{α}$, there is an algebra morphism
  $\dpair{ℕ}{η} ⇒ \dpair{X}{α}$.
\end{lemma}
\begin{proof}
	Define $x_0$ and $\suc_X$ as in the above proof.
  Now define a function $g$ recursively:
  \begin{alignat*}{2}
    &g : ℕ → X \\
    &\apply{g}{0}                 &&\defeq x_0 \\
    &\apply{g}{(\apply{\suc}{n})} &&\defeq \suc_X(g(n))
  \end{alignat*}
  By definition $g$ sends the distinguished point $x_0$ to $0$ and intertwines
  $\suc$ with $\suc_X$; by \cref{lemma:nat-alg-mor}, it's an algebra morphism.
\end{proof}

\begin{lemma}[\coqname{W.Naturals.nat\_alg\_func\_is\_unique}]
  \label[lemma]{lemma:nat-alg-mor-fun-unique}
  The underlying function of the above morphism is unique. That is,
	for any algebra morphism $\dpair{τ}{i}:\dpair{ℕ}{η} ⇒ \dpair{X}{α}$,
  $\propeq{}{τ}{g}$, with $g$ as in the previous proof.
\end{lemma}
\begin{proof}
  By function extensionality (\cref{thm:funext}), it suffices to show that they
  are equal on all inputs. We know that the following diagrams commute:
  \begin{center}
    \begin{minipage}[b]{0.48\linewidth}
      \centering
      \begin{tikzcd}
        P\ℕ  \arrow[d, "η"] \arrow[r, "P^*g"] & PX \arrow[d, "α"] \\
        \ℕ \arrow[r, "g"] & X
      \end{tikzcd}
    \end{minipage}
    \begin{minipage}[b]{0.48\linewidth}
      \centering
      \begin{tikzcd}
        P\ℕ  \arrow[d, "η"] \arrow[r, "P^*τ"] & PX \arrow[d, "α"] \\
        \ℕ \arrow[r, "τ"] & X
      \end{tikzcd}
    \end{minipage}
  \end{center}
  Using function extensionality, proceed
  by induction on the argument.
  % by cases on the constructor of the argument.

  % mor 0 =
  % (x ∘ polynomial_functor_arr bool (bool_rect (λ _ : bool, UU) ∅ unit) mor)
  %   (true,, fromempty)
  (Base case) In the zero case, we have
  $\dpair{\bfalse}{\apply{\rec_{\𝟘}}{\ℕ}}:P\ℕ$. Then
  % By definition of $η$ and the
  % commuting condition for $τ$ (in the diagram above),
  \begin{align*}
    \apply{τ}{0}
    &= \apply{τ}{(\apply{η}{\dpair{\bfalse}{\apply{\rec_{\𝟘}}{\ℕ}}})}
    &&\quad\text{Definition of }η \\
    &\jdeq \apply{(τ∘η)}{\dpair{\bfalse}{\apply{\rec_{\𝟘}}{\ℕ}}} \\
    &= \apply{(α∘P^*τ)}{\dpair{\bfalse}{\apply{\rec_{\𝟘}}{\ℕ}}}
    &&\quad\text{Commuting condition} \\
    &\jdeq \apply{α}{\dpair{\bfalse}{τ∘\apply{\rec_{\𝟘}}{\ℕ}}}
    &&\quad\text{Definition of }P^* \\
    &= \apply{α}{\dpair{\bfalse}{\apply{\rec_{\𝟘}}{X}}}
    &&\quad\text{Contractibility of }\𝟘 → X\\
    &\jdeq x_0 \\
    &\jdeq \apply{g}{0}.
  \end{align*}

  % (false,, (λ _ : bool_rect (λ _ : bool, UU) ∅ unit false, n0))
  (Inductive step) Suppose the hypothesis holds for $n:\ℕ$. Then
  $\dpair{\btrue}{\apply{\rec_{\𝟙}}{\ℕ}{n}}:P\ℕ$.
  Recall the definition of $f$ given in the previous proof. Similarly to the
  above reasoning,
  \begin{align*}
    \apply{τ}{(\apply{\suc}{n})}
    &= \apply{τ}{(\apply{η}{\dpair{\btrue}{\appply{\rec_{\𝟙}}{\ℕ}{n}}})}
    &&\quad\text{Definition of }η \\
    &\jdeq \apply{(τ∘η)}{\dpair{\btrue}{\appply{\rec_{\𝟙}}{\ℕ}{n}}} \\
    &= \apply{(α∘P^*τ)}{\dpair{\btrue}{\appply{\rec_{\𝟙}}{\ℕ}{n}}}
    &&\quad\text{Commuting condition} \\
    &\jdeq \apply{α}{\dpair{\btrue}{τ∘\appply{\rec_{\𝟙}}{\ℕ}{n}}}
    &&\quad\text{Definition of }P^* \\
    &= \apply{α}{\dpair{\btrue}{\appply{\rec_{\𝟙}}{X}{(\apply{τ}{n}})}} \\
    &\jdeq \apply{\suc_X}{(\apply{τ}{n})} \\
    &= \apply{\suc_X}{(\apply{g}{n})}
    &&\quad\text{Inductive hypothesis} \\
    &\jdeq \apply{g}{(\apply{\suc}{n})}.
    &&\quad\text{Definition of }g
  \end{align*}
\end{proof}

Similar to the result of \cref{lemma:nat-alg-simpl}, fibered algebras over $\ℕ$
also have a simple structure understood by an equivalence.

\begin{lemma}[\coqname{W.Naturals.fibered\_algebra\_nat}]
  \label[lemma]{lemma:nat-fib-alg-simpl}
  A fibered algebra structure (over $\ℕ$) on a family
  $E:\ℕ\to\universe$ is given by a point $e_0:E_0$
  and a family of functions $f^E:\pit{n:\ℕ}{E_n\to E_{n+1}}$. Specifically,
  there is an equivalence
  \begin{equation*}
    \weq{\Fibalgtype(\dpair{ℕ}{\suc})}
        {\∑{E:\universe}E_0×\pit{n:\ℕ}{E_n\to E_{n+1}}}
  \end{equation*}
\end{lemma}
\begin{proof}
	\TODO{proof?}
\end{proof}

And like \cref{lemma:mk-nat-alg-mor}, we can also simplify our understanding of
algebra sections.

\begin{lemma}[\coqname{W.Naturals.mk\_nat\_alg\_sec}]
  \label[lemma]{lemma:mk-nat-alg-sec}
  % (p1 : x 0 = pr1 (pr2 FA'))
  % (p2 : (∏ n, pr2 (pr2 FA') n (x n) = x (S n))),
  Let $\dpair{E}{θ}$ be a fibered $P$-algebra over $\dpair{\ℕ}{\suc}$.
  If $x:\∏{n:\ℕ}{X}$ sends $0$ to $e_0$ and for all $n$,
  $\propeq{}{\apply{f^E}{n}}{\apply{x}{(\apply{\suc}{n})}}$,
  then there is a corresponding algebra section with $x$ as the underlying
  function.
\end{lemma}
\begin{proof}
	\TODO{proof?}
\end{proof}

\begin{lemma}\label[lemma]{lemma:nat-alg-sec-equiv}
  The above function is an equivalence.
\end{lemma}

This result is proved in the \Agda{} formalization accompanying
\cite{non-wellfounded}. Its proof is outside of the scope

\begin{lemma}[\coqname{W.Naturals.nat\_alg\_is\_preinitial\_sec}]
  \label[lemma]{lemma:fiber-preinitiality-nat}
	The natural numbers are fiber-preinitial (\cref{def:fiber-preinitiality}).
\end{lemma}
\begin{proof}
	Suppose $\dpair{E}{θ}$ is a fibered algebra over $\ℕ$
  (\cref{def:algebra-fibration}) so that $E:\ℕ→\universe$ and
  \begin{equation*}
    θ:\∏{\dpair{n}{h}:P\ℕ}{\paren{\∏{b:\appppply{\rec_{\𝟚}}{\universe}{\𝟘}{\𝟙}{n}}{\apply{E}{(\apply{h}{b})}}} → \apply{E}{(\apply{η}{\dpair{n}{h}})}}.
  \end{equation*}
  The first part of an algebra section is a function $f:\∏{n:\ℕ}{\apply{E}{n}}$.
  Define $f$ by induction:
  \begin{alignat*}{2}
    &f : \∏{n:\ℕ}{\apply{E}{n}}   &&\\
    &\apply{f}{0}                 &&\defeq
    \appply{θ}{\dpair{\bfalse}{\fromempty{ℕ}}}
              {(\fromempty{(E∘\fromempty{\ℕ})})} \\
    &\apply{f}{(\apply{\suc}{n})} &&\defeq
    \appply{θ}{\dpair{\btrue}{\appply{\rec_{\𝟙}}{\ℕ}{n}}}
              {(\fromunit{(\apply{E}{n})}{(\apply{f}{n})})}
  \end{alignat*}
  Let $\dpair{b}{h}:P_{A,B}\ℕ$. It remains to show that this function is indeed
  a section, namely that
  \begin{equation*}
    \propeq
      {}
      {\apply{f}{(\apply{η}{\dpair{b}{h}})}}
      {\appply{θ}{\dpair{b}{h}}{(f∘h)}}
  \end{equation*}
  Proceed by cases on $b$.

  If $b\≡\bfalse$, then $h:\𝟘 → ℕ$. In this case,
  \begin{align*}
    \apply{f}{(\apply{η}{\dpair{\bfalse}{h}})}
    &\≡ \apply{f}{0}
    &&\quad\text{Definition of }η \\
    &\≡ \appply{θ}{\dpair{\bfalse}{\fromempty{ℕ}}}
                  {(\fromempty{(E∘\fromempty{\ℕ})})}
    &&\quad\text{Definition of }f \\
    &\= \appply{θ}{\dpair{\bfalse}{h}}{(f∘h)}
  \end{align*}
  by the contractibility of types $\𝟘 → \ℕ$ and
  $(E∘\fromempty{\ℕ}) → \ℕ$.\TODO{reference}

  If $b\≡\btrue$, then $h:\𝟙 → ℕ$.
  \begin{align*}
    \apply{f}{(\apply{η}{\dpair{\btrue}{h}})}
    &\≡ \apply{f}{(\apply{\suc}{(\apply{h}{\unitelem})})} \\
    &\≡ \appply{θ}{\dpair{\btrue}{\appply{\rec_{\𝟙}}{\ℕ}{(\apply{h}{\unitelem})}}}
                  {\Big(\fromunit{(\apply{E}{(\apply{h}{\unitelem})})}
                             {(\apply{f}{(\apply{h}{\unitelem})})}\Big)} \\
    &\= \appply{θ}{\dpair{\btrue}{h}}{(f∘h)}
  \end{align*}
  where the last equality is by two applications of function extensionality
  (\cref{thm:funext}) and the contractibility for $\𝟙$.
\end{proof}

\begin{lemma}\label[lemma]{lemma:fiber-initiality-nat}
	The natural numbers are fiber-initial (\cref{def:fiber-initiality}).
\end{lemma}

\begin{lemma}\label[lemma]{lemma:fiber-initiality-nat}
	The natural numbers are fiber-initial (\cref{def:fiber-initiality}).
\end{lemma}

% \begin{lemma}\label[lemma]{lemma:algebra-sections}
%   Let $X$ be a $P$-algebra with $α:PX→X$, and let $C:X→\universe$. The standard
%   projection map $π_1:\∑{x:X}{Cx}→X$ has a section $s:\∏{x:X}{Cx}$
%   if $\∑{x:X}{Cx}$ also has a $P$-algebra structure, as in
%   \begin{center}
%     \begin{tikzcd}
%       {} & \∑{x:X}{Cx} \arrow[d, "π_1"] \\
%       X \arrow[ur, "s"] \arrow[r, equal] & X
%     \end{tikzcd}
%   \end{center}
%   In this case, $s$ is a $P$-algebra morphism \cite{inductive}.
% \end{lemma}

\subsection{Limits}
\label{subsec:limits}

In extensional type theories, W-types can be derived as colimits of
\define{chains}, which are given by a family $X:\ℕ → \universe$ and a family of
functions $π_n:X_{n+1}\to X_n$.\TODO{do they use cochains?} Dually,
M-types can be constructed as limits of such chains. As noted in
\cite{non-wellfounded}, these types are instances
of the general homotopy limits of \cite{homotopy-limits}.

\begin{definition}\label[definition]{def:limitt}
	Given chain $X:\N\to\universe$ and $π_n:X_{n+1}\to X_n$, the \define{limit} of
  $(X,π)$ is the type
  \begin{equation*}
    L(X,π)\defeq \∑{x:\∏{n:\N}{X_n}}{\∏{n:\N}π_nx_{n+1}=x_n}
  \end{equation*}
  We give special names to the projection maps for limits: $p \defeq \pr{1}$ and
  $β \defeq \pr{2}$.
\end{definition}

\begin{lemma}\label[lemma]{lemma:limitt-universal}
	There is an equivalence of types
  \begin{equation*}
    \weq{(A\to L(X,π))}{\∑{f:\∏{n:\N}A\to X_n}{\∏{n:\N}{\propeq{}{π_n∘ f_{n+1}}{f_n}}}}
  \end{equation*}
\end{lemma}
\begin{proof}
	To give an equivalence, it suffices to give functions back and forth that
  compose to the respective identities.\TODO{reference definition}
  First, define
  \begin{gather*}
    ϕ :(A\to L(X,π)) ⟶ \∑{f:\∏{n:\N}A\to X_n}{\∏{n:\N}{\propeq{}{π_n∘ f_{n+1}}{f_n}}} \\
    ϕ(g) \defeq
    \dpair{\λ{n}\λ{a}\appply{p}{(\apply{g}{a})}{n}}
          {\λ{n}\appply{β}{(\apply{g}{a})}{n})}
  \intertext{and}
    ψ : \paren*[\bigg]{\∑{f:\∏{n:\N}A\to X_n}{\∏{n:\N}{π_n∘ f_{n+1}=f_n}}}
          \to A\to L(X,π) \\
    ψ((f; h)) \defeq \λ{a:A} \dpair{\λ{n}\appply{f}{n}{a}}{h}
  \end{gather*}
  Their composites are judgmentally equal to the appropriate identities.
  \TODO{should I include proof?}
\end{proof}

\begin{lemma}\label[lemma]{lemma:cochains}
	If $X:\N\to\universe$ is a family of types, $ρ:\∏{n:\N}{X_n\to X_{n+1}}$ is a
  family of functions, and
  \begin{equation*}
    Z\defeq \∑{x:\∏{n:\N}X_n}{\∏{n:\N}x_{n+1}=ρ_n(x_n)},
  \end{equation*}
  then $\weq{Z}{X_0}$, that is, limits of cochains are entirely determined by
  the first element.
\end{lemma}
\begin{proof}
  Consider again the functor of \cref{ex:nat}, specialized to the case of
  $\bfC\≔\universe$:
  \begin{gather*}
    G:\universe ⟶ \universe  \\
    G(W) \defeq 1+W
  \end{gather*}
  A $G$-algebra is given by a type $W:\universe$, a point in $W$, and a function
  $W\to W$. Fix a point $z:Z$. Then $\appr{1}{z}$ gives a point in
  $\∏{n:\ℕ}{\apply{X}{n}}$.
\end{proof}


% \chapter*{Conclusion}
% \addcontentsline{toc}{chapter}{Conclusion}
% \chaptermark{Conclusion}
% \markboth{Conclusion}{Conclusion}
% \setcounter{chapter}{4}
% \setcounter{section}{0}

%If you feel it necessary to include an appendix, it goes here.
\appendix
\chapter{Cross-reference of names}

The following table lists lemmas taken from \cite{book}.
\begin{table}[ht]
  \centering
  \begin{tabular}{c | c}
    This thesis & The \HoTT book \\ \hline
    \Cref{def:ua} & Axiom 2.10.3
  \end{tabular}
\end{table}

The following table compares the terminology used in this thesis to that in our
\Coq{} formalization (under the column \UniMath{}) and that of the \Agda{} development
of \cite{non-wellfounded} (under the column \MTypes{}).
\begin{table}[ht]
  \centering
  \begin{tabular}{c | c | c }
    This thesis & \UniMath{} & \MTypes{} \\ \hline
  \end{tabular}
\end{table}
% \chapter{The Second Appendix, for Fun}

\backmatter % backmatter makes the index and bibliography appear properly in the TOC

%  \bibliographystyle{bsts/mla-good} % there are a variety of styles available;
%  \bibliographystyle{plainnat}
% replace ``plainnat'' with the style of choice. You can crefer to files in the bsts or APA
% subfolder, e.g.
% \bibliographystyle{APA/apa-good}  % or

% uncomment me!
\nocite{*}
\bibliographystyle{apalike}
\bibliography{thesis}

% Comment the above two lines and uncomment the next line to use biblatex-chicago.
%\printbibliography[heading=bibintoc]

% Finally, an index would go here... but it is also optional.
\index{$\id$|see {Identity}}
\index{$F$-algebra|see {Algebra for a functor}}
\printindex
\end{document}
